{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“š Task 2: Text Classification (Without Fine-Tuning)\n",
    "\n",
    "## Model: `distilbert-base-uncased-finetuned-sst-2-english`\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Goals\n",
    "| Goal | Description |\n",
    "|------|-------------|\n",
    "| âœ… Feed custom sentences | Pass diverse inputs to the pipeline |\n",
    "| âœ… Observe predictions | See POSITIVE / NEGATIVE labels |\n",
    "| âœ… Analyse confidence scores | Understand per-label probabilities |\n",
    "| âœ… Understand inference pipeline | End-to-end walkthrough |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§  Background\n",
    "\n",
    "**DistilBERT** is a *distilled* (compressed) version of BERT, trained using knowledge distillation.\n",
    "It preserves **97% of BERT's accuracy** while being **60% smaller** and **2Ã— faster**.\n",
    "\n",
    "The checkpoint `distilbert-base-uncased-finetuned-sst-2-english` was **fine-tuned on the SST-2 dataset**\n",
    "(Stanford Sentiment Treebank), a binary sentiment classification task (POSITIVE / NEGATIVE).\n",
    "\n",
    "> ğŸ’¡ **No Fine-Tuning Required** â€” We are using the model exactly as it was released.\n",
    "   This demonstrates *zero-shot domain transfer* for in-distribution sentiment tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš™ï¸ Step 1 â€” Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), '..', 'src'))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "from transformers import pipeline\n",
    "\n",
    "print(f'PyTorch version : {torch.__version__}')\n",
    "print(f'CUDA available  : {torch.cuda.is_available()}')\n",
    "print(f'Device          : {\"GPU\" if torch.cuda.is_available() else \"CPU\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ Step 2 â€” Load the Inference Pipeline\n",
    "\n",
    "HuggingFace's `pipeline()` abstracts the full inference stack:\n",
    "\n",
    "```\n",
    "raw text\n",
    "   â†“  Tokenizer      (WordPiece tokenisation + input IDs)\n",
    "   â†“  DistilBERT     (contextual embeddings via 6 Transformer layers)\n",
    "   â†“  Classifier Head (linear layer â†’ logits)\n",
    "   â†“  Softmax        (logits â†’ probabilities)\n",
    "   â†“  Label Decode   (argmax â†’ POSITIVE / NEGATIVE)\n",
    "predicted label + confidence score\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "\n",
    "print(f'Loading model: {MODEL_NAME} ...')\n",
    "\n",
    "classifier = pipeline(\n",
    "    task='text-classification',\n",
    "    model=MODEL_NAME,\n",
    "    device=-1,              # -1 = CPU; set to 0 for GPU\n",
    "    return_all_scores=True  # return probabilities for ALL labels\n",
    ")\n",
    "\n",
    "print('âœ… Model loaded successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¬ Step 3 â€” Define Custom Sentences\n",
    "\n",
    "We design sentences across **5 categories** to probe different model behaviours:\n",
    "\n",
    "| Category | Expected Behaviour |\n",
    "|---|---|\n",
    "| Clear positive | Very high confidence POSITIVE |\n",
    "| Clear negative | Very high confidence NEGATIVE |\n",
    "| Ambiguous | Low confidence, uncertain prediction |\n",
    "| Negation traps | Tests contextual understanding |\n",
    "| Mixed sentiment | Model must pick dominant signal |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_groups = {\n",
    "    'Clear Positive': [\n",
    "        'This movie is absolutely fantastic â€” I loved every second of it!',\n",
    "        'The customer service was outstanding and the staff were incredibly kind.',\n",
    "        'I have never tasted such an amazing meal in my entire life.',\n",
    "    ],\n",
    "    'Clear Negative': [\n",
    "        'This product is a complete waste of money and I deeply regret buying it.',\n",
    "        'Terrible experience. The room was dirty, cold, and the staff were rude.',\n",
    "        'I hated this book. The plot was boring and the characters were flat.',\n",
    "    ],\n",
    "    'Ambiguous / Borderline': [\n",
    "        'The film was okay â€” not great, but not terrible either.',\n",
    "        'It is what it is. Some parts were decent, others felt forced.',\n",
    "    ],\n",
    "    'Negation Traps': [\n",
    "        'Not bad at all! Actually quite impressive for the price.',\n",
    "        'I would not say this was a bad experience, but it was not memorable.',\n",
    "    ],\n",
    "    'Mixed Sentiment': [\n",
    "        'The food was delicious but the service was appallingly slow.',\n",
    "        'Beautiful location, however the hotel rooms left much to be desired.',\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Flatten for batch inference\n",
    "all_sentences = [s for sentences in sentence_groups.values() for s in sentences]\n",
    "all_groups    = [g for g, sentences in sentence_groups.items() for _ in sentences]\n",
    "\n",
    "print(f'Total sentences: {len(all_sentences)}')\n",
    "for group, sentences in sentence_groups.items():\n",
    "    print(f'  {group}: {len(sentences)} sentences')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ Step 4 â€” Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_outputs = classifier(all_sentences)\n",
    "\n",
    "# Structure results into a clean list of dicts\n",
    "results = []\n",
    "for sentence, group, label_scores in zip(all_sentences, all_groups, raw_outputs):\n",
    "    scores_dict = {ls['label']: ls['score'] for ls in label_scores}\n",
    "    top = max(label_scores, key=lambda x: x['score'])\n",
    "    results.append({\n",
    "        'Group'     : group,\n",
    "        'Sentence'  : sentence,\n",
    "        'Label'     : top['label'],\n",
    "        'Confidence': round(top['score'] * 100, 2),\n",
    "        'POSITIVE'  : round(scores_dict.get('POSITIVE', 0) * 100, 2),\n",
    "        'NEGATIVE'  : round(scores_dict.get('NEGATIVE', 0) * 100, 2),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(f'Inference complete for {len(df)} sentences.')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Step 5 â€” Visualise Confidence Scores\n",
    "\n",
    "### 5a. Per-sentence horizontal bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(13, 7))\n",
    "\n",
    "colours = ['#2ecc71' if lbl == 'POSITIVE' else '#e74c3c' for lbl in df['Label']]\n",
    "short_labels = [s[:55] + '...' if len(s) > 55 else s for s in df['Sentence']]\n",
    "\n",
    "bars = ax.barh(short_labels, df['Confidence'], color=colours, edgecolor='white', height=0.65)\n",
    "\n",
    "# Annotate bars\n",
    "for bar, conf, lbl in zip(bars, df['Confidence'], df['Label']):\n",
    "    ax.text(\n",
    "        min(conf + 1, 102), bar.get_y() + bar.get_height() / 2,\n",
    "        f'{conf:.1f}%  {lbl}',\n",
    "        va='center', ha='left', fontsize=9, fontweight='bold',\n",
    "        color='#2ecc71' if lbl == 'POSITIVE' else '#e74c3c'\n",
    "    )\n",
    "\n",
    "ax.set_xlim(0, 115)\n",
    "ax.set_xlabel('Confidence (%)', fontsize=12)\n",
    "ax.set_title('Text Classification Confidence Scores\\n(distilbert-base-uncased-finetuned-sst-2-english)',\n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.axvline(80, color='orange', linestyle='--', alpha=0.7, label='80% threshold')\n",
    "ax.axvline(99, color='gray',   linestyle='--', alpha=0.7, label='99% threshold')\n",
    "\n",
    "pos_patch = mpatches.Patch(color='#2ecc71', label='POSITIVE')\n",
    "neg_patch = mpatches.Patch(color='#e74c3c', label='NEGATIVE')\n",
    "ax.legend(handles=[pos_patch, neg_patch], loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/task2_confidence_bars.png', dpi=150)\n",
    "plt.show()\n",
    "print('Chart saved â†’ data/task2_confidence_bars.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b. Grouped box plot â€” confidence by sentence group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(11, 5))\n",
    "\n",
    "group_order = list(sentence_groups.keys())\n",
    "palette = {\n",
    "    'Clear Positive'     : '#27ae60',\n",
    "    'Clear Negative'     : '#c0392b',\n",
    "    'Ambiguous / Borderline': '#f39c12',\n",
    "    'Negation Traps'     : '#8e44ad',\n",
    "    'Mixed Sentiment'    : '#2980b9',\n",
    "}\n",
    "\n",
    "sns.stripplot(\n",
    "    data=df, x='Group', y='Confidence', hue='Label',\n",
    "    palette={'POSITIVE': '#2ecc71', 'NEGATIVE': '#e74c3c'},\n",
    "    order=group_order, size=12, jitter=0.15, ax=ax\n",
    ")\n",
    "\n",
    "ax.axhline(80, color='orange', linestyle='--', alpha=0.8, label='80% threshold')\n",
    "ax.axhline(99, color='gray',   linestyle='--', alpha=0.8, label='99% threshold')\n",
    "ax.set_xlabel('Sentence Group', fontsize=11)\n",
    "ax.set_ylabel('Confidence (%)', fontsize=11)\n",
    "ax.set_title('Confidence Scores by Sentence Group', fontsize=13, fontweight='bold')\n",
    "ax.tick_params(axis='x', rotation=15)\n",
    "ax.legend(loc='lower left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/task2_group_confidence.png', dpi=150)\n",
    "plt.show()\n",
    "print('Chart saved â†’ data/task2_group_confidence.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5c. POSITIVE vs NEGATIVE score scatter (diverging plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9, 9))\n",
    "\n",
    "colours_scatter = ['#2ecc71' if lbl == 'POSITIVE' else '#e74c3c' for lbl in df['Label']]\n",
    "\n",
    "ax.scatter(df['POSITIVE'], df['NEGATIVE'], c=colours_scatter, s=120, edgecolors='white', linewidths=1.5, zorder=3)\n",
    "\n",
    "# Diagonal line â€” where POSITIVE = NEGATIVE = 50%\n",
    "ax.plot([0, 100], [100, 0], 'k--', alpha=0.3, label='Decision boundary (50/50)')\n",
    "\n",
    "# Annotate each point\n",
    "for _, row in df.iterrows():\n",
    "    short = row['Sentence'][:30] + '...'\n",
    "    ax.annotate(short, (row['POSITIVE'], row['NEGATIVE']),\n",
    "                fontsize=6.5, ha='left', va='bottom',\n",
    "                xytext=(3, 3), textcoords='offset points', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('POSITIVE score (%)', fontsize=12)\n",
    "ax.set_ylabel('NEGATIVE score (%)', fontsize=12)\n",
    "ax.set_title('POSITIVE vs NEGATIVE Confidence Scores\\n(points near the diagonal = uncertain predictions)',\n",
    "             fontsize=12, fontweight='bold')\n",
    "\n",
    "pos_patch = mpatches.Patch(color='#2ecc71', label='Predicted POSITIVE')\n",
    "neg_patch = mpatches.Patch(color='#e74c3c', label='Predicted NEGATIVE')\n",
    "ax.legend(handles=[pos_patch, neg_patch, plt.Line2D([0], [0], color='k', linestyle='--', alpha=0.3, label='Decision boundary')])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/task2_pos_vs_neg.png', dpi=150)\n",
    "plt.show()\n",
    "print('Chart saved â†’ data/task2_pos_vs_neg.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ Step 6 â€” Analysis Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total     = len(df)\n",
    "n_pos     = (df['Label'] == 'POSITIVE').sum()\n",
    "n_neg     = (df['Label'] == 'NEGATIVE').sum()\n",
    "high_conf = (df['Confidence'] >= 99).sum()\n",
    "low_conf  = (df['Confidence'] <  80).sum()\n",
    "\n",
    "print('=' * 55)\n",
    "print('  ANALYSIS SUMMARY')\n",
    "print('=' * 55)\n",
    "print(f'  Total sentences       : {total}')\n",
    "print(f'  POSITIVE predictions  : {n_pos} ({n_pos/total*100:.1f}%)')\n",
    "print(f'  NEGATIVE predictions  : {n_neg} ({n_neg/total*100:.1f}%)')\n",
    "print(f'  Avg confidence        : {df[\"Confidence\"].mean():.2f}%')\n",
    "print(f'  High-conf (â‰¥ 99%)     : {high_conf} sentences')\n",
    "print(f'  Low-conf  (< 80%)     : {low_conf}  sentences')\n",
    "print('=' * 55)\n",
    "\n",
    "print('\\n  Confidence by group:')\n",
    "group_summary = df.groupby('Group')['Confidence'].agg(['mean', 'min', 'max']).round(2)\n",
    "group_summary.columns = ['Mean %', 'Min %', 'Max %']\n",
    "display(group_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”¬ Step 7 â€” Deep Dive: Tokenisation Walkthrough\n",
    "\n",
    "Let's look at **how the tokenizer converts raw text** before passing it to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "demo_sentences = [\n",
    "    'Not bad at all! Actually quite impressive for the price.',\n",
    "    'The food was delicious but the service was appallingly slow.',\n",
    "]\n",
    "\n",
    "print('=' * 65)\n",
    "print('  TOKENISATION WALKTHROUGH')\n",
    "print('=' * 65)\n",
    "\n",
    "for sent in demo_sentences:\n",
    "    tokens   = tokenizer.tokenize(sent)\n",
    "    token_ids = tokenizer.encode(sent)\n",
    "    print(f'\\n  Input   : {sent}')\n",
    "    print(f'  Tokens  : {tokens}')\n",
    "    print(f'  IDs     : {token_ids}')\n",
    "    print(f'  # tokens: {len(tokens)} (+ 2 special tokens [CLS]/[SEP] = {len(token_ids)})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§ª Step 8 â€” Interactive: Try Your Own Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_and_display(text: str) -> None:\n",
    "    \"\"\"Classify a single sentence and display a formatted result.\"\"\"\n",
    "    output = classifier([text])[0]  # list of label scores\n",
    "    scores = {ls['label']: ls['score'] for ls in output}\n",
    "    top    = max(output, key=lambda x: x['score'])\n",
    "    \n",
    "    bar_pos = 'â–ˆ' * int(scores['POSITIVE'] * 30) + 'â–‘' * (30 - int(scores['POSITIVE'] * 30))\n",
    "    bar_neg = 'â–ˆ' * int(scores['NEGATIVE'] * 30) + 'â–‘' * (30 - int(scores['NEGATIVE'] * 30))\n",
    "    \n",
    "    print(f'\\n  Input     : \"{text}\"')\n",
    "    print(f'  Prediction: {top[\"label\"]}  ({top[\"score\"]*100:.2f}% confident)')\n",
    "    print(f'  POSITIVE  : [{bar_pos}] {scores[\"POSITIVE\"]*100:.2f}%')\n",
    "    print(f'  NEGATIVE  : [{bar_neg}] {scores[\"NEGATIVE\"]*100:.2f}%')\n",
    "\n",
    "\n",
    "# â”€â”€ Try your own sentences here! â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "classify_and_display('I am so excited about this new project!')\n",
    "classify_and_display('The weather is cloudy today.')\n",
    "classify_and_display('This was the worst decision I have ever made in my career.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ Step 9 â€” Key Takeaways\n",
    "\n",
    "| Concept | What We Learned |\n",
    "|---|---|\n",
    "| **Inference Pipeline** | `pipeline()` chains tokenization â†’ model â†’ softmax in one call |\n",
    "| **DistilBERT** | 66M params, 6 Transformer layers, fast and accurate for NLP |\n",
    "| **Confidence Scores** | Softmax probabilities summing to 1.0 across labels |\n",
    "| **High Confidence** | Clear sentiment sentences score â‰¥ 99% |\n",
    "| **Low Confidence** | Ambiguous / mixed sentences score 50â€“80% |\n",
    "| **Negation** | Model handles 'not bad' correctly via contextual attention |\n",
    "| **No Fine-Tuning** | Pre-trained checkpoint works out-of-the-box for reviews/sentiment |\n",
    "| **Tokenisation** | WordPiece splits words; [CLS] and [SEP] special tokens are added |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸš€ Next Steps\n",
    "- **Task 3**: Named Entity Recognition (NER) with `dslim/bert-base-NER`\n",
    "- **Task 4**: Question Answering â€” extractive QA with `deepset/roberta-base-squad2`\n",
    "- **Fine-Tuning**: Adapt the model to your own domain-specific sentiment data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_lab_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
