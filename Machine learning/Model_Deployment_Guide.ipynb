{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Model Deployment - Complete Guide\n",
                "\n",
                "## üìö Learning Objectives\n",
                "- Save and load trained models\n",
                "- Create prediction functions\n",
                "- Build a simple API with Flask\n",
                "- Implement model versioning\n",
                "- Handle production considerations\n",
                "- Monitor model performance\n",
                "\n",
                "## üéØ What is Model Deployment?\n",
                "\n",
                "**Model Deployment** is the process of making your trained machine learning model available for use in a production environment.\n",
                "\n",
                "### Deployment Pipeline:\n",
                "```\n",
                "Training ‚Üí Validation ‚Üí Saving ‚Üí API Creation ‚Üí Deployment ‚Üí Monitoring\n",
                "```\n",
                "\n",
                "### Why is Deployment Important?\n",
                "- A model is only valuable if it can be used\n",
                "- Real-world impact requires production deployment\n",
                "- Enables continuous improvement through feedback\n",
                "\n",
                "### Deployment Options:\n",
                "1. **Batch Predictions**: Scheduled predictions on datasets\n",
                "2. **Real-time API**: On-demand predictions via REST API\n",
                "3. **Embedded**: Model integrated into applications\n",
                "4. **Edge Deployment**: Models on mobile/IoT devices"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import pickle\n",
                "import joblib\n",
                "import json\n",
                "from datetime import datetime\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.ensemble import RandomForestRegressor\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.metrics import mean_squared_error, r2_score\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "print(\"‚úÖ Libraries imported successfully!\")\n",
                "print(f\"Current timestamp: {datetime.now()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 1: Model Training and Saving\n",
                "### 1Ô∏è‚É£ Train a Production-Ready Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load data\n",
                "df = pd.read_csv('supervised Learning/01_Regression/Linear Regression/data/dataset.csv')\n",
                "\n",
                "# Prepare features and target\n",
                "target_col = 'median_house_value'\n",
                "feature_cols = ['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n",
                "                'total_bedrooms', 'population', 'households', 'median_income']\n",
                "\n",
                "X = df[feature_cols].fillna(df[feature_cols].median())\n",
                "y = df[target_col]\n",
                "\n",
                "# Split data\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, test_size=0.2, random_state=42\n",
                ")\n",
                "\n",
                "print(f\"Training set: {X_train.shape}\")\n",
                "print(f\"Test set: {X_test.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a production pipeline\n",
                "production_pipeline = Pipeline([\n",
                "    ('scaler', StandardScaler()),\n",
                "    ('model', RandomForestRegressor(\n",
                "        n_estimators=100,\n",
                "        max_depth=10,\n",
                "        random_state=42,\n",
                "        n_jobs=-1\n",
                "    ))\n",
                "])\n",
                "\n",
                "print(\"Training production model...\")\n",
                "production_pipeline.fit(X_train, y_train)\n",
                "print(\"‚úÖ Model trained successfully!\")\n",
                "\n",
                "# Evaluate\n",
                "y_pred = production_pipeline.predict(X_test)\n",
                "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
                "r2 = r2_score(y_test, y_pred)\n",
                "\n",
                "print(f\"\\nüìä Model Performance:\")\n",
                "print(f\"RMSE: ${rmse:,.2f}\")\n",
                "print(f\"R¬≤: {r2:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2Ô∏è‚É£ Save Model - Multiple Methods"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "# Create models directory\n",
                "os.makedirs('models', exist_ok=True)\n",
                "\n",
                "# Method 1: Pickle (Python's built-in serialization)\n",
                "print(\"üíæ Saving model with Pickle...\")\n",
                "with open('models/model_pickle.pkl', 'wb') as f:\n",
                "    pickle.dump(production_pipeline, f)\n",
                "print(\"‚úÖ Saved: models/model_pickle.pkl\")\n",
                "\n",
                "# Method 2: Joblib (recommended for sklearn, more efficient)\n",
                "print(\"\\nüíæ Saving model with Joblib...\")\n",
                "joblib.dump(production_pipeline, 'models/model_joblib.pkl')\n",
                "print(\"‚úÖ Saved: models/model_joblib.pkl\")\n",
                "\n",
                "# Check file sizes\n",
                "pickle_size = os.path.getsize('models/model_pickle.pkl') / 1024  # KB\n",
                "joblib_size = os.path.getsize('models/model_joblib.pkl') / 1024  # KB\n",
                "\n",
                "print(f\"\\nüì¶ File Sizes:\")\n",
                "print(f\"Pickle: {pickle_size:.2f} KB\")\n",
                "print(f\"Joblib: {joblib_size:.2f} KB\")\n",
                "print(f\"\\nüí° Joblib is typically more efficient for sklearn models\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3Ô∏è‚É£ Save Model Metadata"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create comprehensive metadata\n",
                "metadata = {\n",
                "    'model_info': {\n",
                "        'name': 'Housing Price Predictor',\n",
                "        'version': '1.0.0',\n",
                "        'algorithm': 'Random Forest Regressor',\n",
                "        'created_date': datetime.now().isoformat(),\n",
                "        'author': 'ML Engineer'\n",
                "    },\n",
                "    'training_info': {\n",
                "        'training_samples': len(X_train),\n",
                "        'test_samples': len(X_test),\n",
                "        'features': feature_cols,\n",
                "        'target': target_col\n",
                "    },\n",
                "    'performance': {\n",
                "        'test_rmse': float(rmse),\n",
                "        'test_r2': float(r2),\n",
                "        'train_rmse': float(np.sqrt(mean_squared_error(\n",
                "            y_train, production_pipeline.predict(X_train)\n",
                "        )))\n",
                "    },\n",
                "    'hyperparameters': {\n",
                "        'n_estimators': 100,\n",
                "        'max_depth': 10,\n",
                "        'random_state': 42\n",
                "    },\n",
                "    'preprocessing': {\n",
                "        'scaler': 'StandardScaler',\n",
                "        'missing_value_strategy': 'median_imputation'\n",
                "    }\n",
                "}\n",
                "\n",
                "# Save metadata\n",
                "with open('models/model_metadata.json', 'w') as f:\n",
                "    json.dump(metadata, f, indent=4)\n",
                "\n",
                "print(\"‚úÖ Metadata saved: models/model_metadata.json\")\n",
                "print(\"\\nüìÑ Metadata Preview:\")\n",
                "print(json.dumps(metadata, indent=2))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4Ô∏è‚É£ Load and Verify Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load model\n",
                "print(\"üìÇ Loading model from disk...\")\n",
                "loaded_model = joblib.load('models/model_joblib.pkl')\n",
                "print(\"‚úÖ Model loaded successfully!\")\n",
                "\n",
                "# Verify model works\n",
                "print(\"\\nüîç Verifying model...\")\n",
                "test_prediction = loaded_model.predict(X_test[:5])\n",
                "actual_values = y_test.iloc[:5].values\n",
                "\n",
                "print(\"\\nSample Predictions:\")\n",
                "for i, (pred, actual) in enumerate(zip(test_prediction, actual_values), 1):\n",
                "    print(f\"  Sample {i}: Predicted=${pred:,.0f}, Actual=${actual:,.0f}, \"\n",
                "          f\"Error=${abs(pred-actual):,.0f}\")\n",
                "\n",
                "# Verify performance matches\n",
                "y_pred_loaded = loaded_model.predict(X_test)\n",
                "rmse_loaded = np.sqrt(mean_squared_error(y_test, y_pred_loaded))\n",
                "\n",
                "print(f\"\\n‚úÖ Performance Verification:\")\n",
                "print(f\"Original RMSE: ${rmse:,.2f}\")\n",
                "print(f\"Loaded RMSE: ${rmse_loaded:,.2f}\")\n",
                "print(f\"Match: {np.isclose(rmse, rmse_loaded)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 2: Creating Prediction Functions\n",
                "### 5Ô∏è‚É£ Build Prediction Interface"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class HousePricePredictor:\n",
                "    \"\"\"\n",
                "    Production-ready house price prediction class.\n",
                "    \n",
                "    Handles model loading, validation, and predictions.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, model_path='models/model_joblib.pkl', \n",
                "                 metadata_path='models/model_metadata.json'):\n",
                "        \"\"\"Initialize predictor with model and metadata.\"\"\"\n",
                "        self.model = joblib.load(model_path)\n",
                "        \n",
                "        with open(metadata_path, 'r') as f:\n",
                "            self.metadata = json.load(f)\n",
                "        \n",
                "        self.feature_names = self.metadata['training_info']['features']\n",
                "        print(f\"‚úÖ Loaded model version: {self.metadata['model_info']['version']}\")\n",
                "    \n",
                "    def validate_input(self, input_data):\n",
                "        \"\"\"Validate input data format and values.\"\"\"\n",
                "        # Check if all required features are present\n",
                "        missing_features = set(self.feature_names) - set(input_data.keys())\n",
                "        if missing_features:\n",
                "            raise ValueError(f\"Missing features: {missing_features}\")\n",
                "        \n",
                "        # Check for valid ranges (example)\n",
                "        if input_data.get('median_income', 0) < 0:\n",
                "            raise ValueError(\"median_income must be positive\")\n",
                "        \n",
                "        return True\n",
                "    \n",
                "    def predict_single(self, input_data):\n",
                "        \"\"\"\n",
                "        Make prediction for a single house.\n",
                "        \n",
                "        Args:\n",
                "            input_data (dict): Dictionary with feature values\n",
                "            \n",
                "        Returns:\n",
                "            dict: Prediction result with confidence interval\n",
                "        \"\"\"\n",
                "        # Validate input\n",
                "        self.validate_input(input_data)\n",
                "        \n",
                "        # Convert to DataFrame\n",
                "        df = pd.DataFrame([input_data])[self.feature_names]\n",
                "        \n",
                "        # Make prediction\n",
                "        prediction = self.model.predict(df)[0]\n",
                "        \n",
                "        # Calculate confidence interval (simplified)\n",
                "        rmse = self.metadata['performance']['test_rmse']\n",
                "        confidence_interval = (prediction - 2*rmse, prediction + 2*rmse)\n",
                "        \n",
                "        return {\n",
                "            'predicted_price': float(prediction),\n",
                "            'confidence_interval_95': {\n",
                "                'lower': float(confidence_interval[0]),\n",
                "                'upper': float(confidence_interval[1])\n",
                "            },\n",
                "            'model_version': self.metadata['model_info']['version'],\n",
                "            'timestamp': datetime.now().isoformat()\n",
                "        }\n",
                "    \n",
                "    def predict_batch(self, input_dataframe):\n",
                "        \"\"\"\n",
                "        Make predictions for multiple houses.\n",
                "        \n",
                "        Args:\n",
                "            input_dataframe (pd.DataFrame): DataFrame with features\n",
                "            \n",
                "        Returns:\n",
                "            pd.DataFrame: Original data with predictions\n",
                "        \"\"\"\n",
                "        predictions = self.model.predict(input_dataframe[self.feature_names])\n",
                "        result = input_dataframe.copy()\n",
                "        result['predicted_price'] = predictions\n",
                "        return result\n",
                "    \n",
                "    def get_model_info(self):\n",
                "        \"\"\"Return model information.\"\"\"\n",
                "        return self.metadata['model_info']\n",
                "\n",
                "# Test the predictor\n",
                "predictor = HousePricePredictor()\n",
                "print(\"\\nüìä Model Info:\")\n",
                "print(json.dumps(predictor.get_model_info(), indent=2))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test single prediction\n",
                "sample_house = {\n",
                "    'longitude': -122.23,\n",
                "    'latitude': 37.88,\n",
                "    'housing_median_age': 41.0,\n",
                "    'total_rooms': 880.0,\n",
                "    'total_bedrooms': 129.0,\n",
                "    'population': 322.0,\n",
                "    'households': 126.0,\n",
                "    'median_income': 8.3252\n",
                "}\n",
                "\n",
                "print(\"üè† Making prediction for sample house...\")\n",
                "result = predictor.predict_single(sample_house)\n",
                "\n",
                "print(\"\\nüìä Prediction Result:\")\n",
                "print(json.dumps(result, indent=2))\n",
                "print(f\"\\nüí∞ Predicted Price: ${result['predicted_price']:,.0f}\")\n",
                "print(f\"üìà 95% Confidence Interval: \"\n",
                "      f\"${result['confidence_interval_95']['lower']:,.0f} - \"\n",
                "      f\"${result['confidence_interval_95']['upper']:,.0f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 3: Creating a Simple REST API\n",
                "### 6Ô∏è‚É£ Flask API Implementation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save Flask API code to a file\n",
                "flask_api_code = '''\n",
                "from flask import Flask, request, jsonify\n",
                "import joblib\n",
                "import pandas as pd\n",
                "import json\n",
                "from datetime import datetime\n",
                "\n",
                "# Initialize Flask app\n",
                "app = Flask(__name__)\n",
                "\n",
                "# Load model and metadata\n",
                "model = joblib.load('models/model_joblib.pkl')\n",
                "with open('models/model_metadata.json', 'r') as f:\n",
                "    metadata = json.load(f)\n",
                "\n",
                "feature_names = metadata['training_info']['features']\n",
                "\n",
                "@app.route('/', methods=['GET'])\n",
                "def home():\n",
                "    \"\"\"API home endpoint.\"\"\"\n",
                "    return jsonify({\n",
                "        'message': 'House Price Prediction API',\n",
                "        'version': metadata['model_info']['version'],\n",
                "        'endpoints': {\n",
                "            '/': 'API information',\n",
                "            '/predict': 'Make single prediction (POST)',\n",
                "            '/batch_predict': 'Make batch predictions (POST)',\n",
                "            '/model_info': 'Get model information (GET)',\n",
                "            '/health': 'Health check (GET)'\n",
                "        }\n",
                "    })\n",
                "\n",
                "@app.route('/predict', methods=['POST'])\n",
                "def predict():\n",
                "    \"\"\"Single prediction endpoint.\"\"\"\n",
                "    try:\n",
                "        # Get input data\n",
                "        data = request.get_json()\n",
                "        \n",
                "        # Validate features\n",
                "        missing = set(feature_names) - set(data.keys())\n",
                "        if missing:\n",
                "            return jsonify({'error': f'Missing features: {list(missing)}'}), 400\n",
                "        \n",
                "        # Make prediction\n",
                "        df = pd.DataFrame([data])[feature_names]\n",
                "        prediction = model.predict(df)[0]\n",
                "        \n",
                "        # Calculate confidence interval\n",
                "        rmse = metadata['performance']['test_rmse']\n",
                "        \n",
                "        return jsonify({\n",
                "            'predicted_price': float(prediction),\n",
                "            'confidence_interval_95': {\n",
                "                'lower': float(prediction - 2*rmse),\n",
                "                'upper': float(prediction + 2*rmse)\n",
                "            },\n",
                "            'model_version': metadata['model_info']['version'],\n",
                "            'timestamp': datetime.now().isoformat()\n",
                "        })\n",
                "    \n",
                "    except Exception as e:\n",
                "        return jsonify({'error': str(e)}), 500\n",
                "\n",
                "@app.route('/batch_predict', methods=['POST'])\n",
                "def batch_predict():\n",
                "    \"\"\"Batch prediction endpoint.\"\"\"\n",
                "    try:\n",
                "        # Get input data (list of dictionaries)\n",
                "        data = request.get_json()\n",
                "        \n",
                "        if not isinstance(data, list):\n",
                "            return jsonify({'error': 'Input must be a list of objects'}), 400\n",
                "        \n",
                "        # Make predictions\n",
                "        df = pd.DataFrame(data)[feature_names]\n",
                "        predictions = model.predict(df)\n",
                "        \n",
                "        return jsonify({\n",
                "            'predictions': predictions.tolist(),\n",
                "            'count': len(predictions),\n",
                "            'model_version': metadata['model_info']['version'],\n",
                "            'timestamp': datetime.now().isoformat()\n",
                "        })\n",
                "    \n",
                "    except Exception as e:\n",
                "        return jsonify({'error': str(e)}), 500\n",
                "\n",
                "@app.route('/model_info', methods=['GET'])\n",
                "def model_info():\n",
                "    \"\"\"Get model information.\"\"\"\n",
                "    return jsonify(metadata)\n",
                "\n",
                "@app.route('/health', methods=['GET'])\n",
                "def health():\n",
                "    \"\"\"Health check endpoint.\"\"\"\n",
                "    return jsonify({\n",
                "        'status': 'healthy',\n",
                "        'timestamp': datetime.now().isoformat()\n",
                "    })\n",
                "\n",
                "if __name__ == '__main__':\n",
                "    app.run(debug=True, host='0.0.0.0', port=5000)\n",
                "'''\n",
                "\n",
                "# Save to file\n",
                "with open('models/app.py', 'w') as f:\n",
                "    f.write(flask_api_code)\n",
                "\n",
                "print(\"‚úÖ Flask API code saved to: models/app.py\")\n",
                "print(\"\\nüìù To run the API:\")\n",
                "print(\"1. Install Flask: pip install flask\")\n",
                "print(\"2. Run: python models/app.py\")\n",
                "print(\"3. API will be available at: http://localhost:5000\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 7Ô∏è‚É£ API Testing Examples"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create example API requests\n",
                "api_examples = {\n",
                "    'single_prediction': {\n",
                "        'method': 'POST',\n",
                "        'endpoint': '/predict',\n",
                "        'body': sample_house,\n",
                "        'curl_example': f'''curl -X POST http://localhost:5000/predict \\\\\n",
                "  -H \"Content-Type: application/json\" \\\\\n",
                "  -d '{json.dumps(sample_house)}'\n",
                "'''\n",
                "    },\n",
                "    'batch_prediction': {\n",
                "        'method': 'POST',\n",
                "        'endpoint': '/batch_predict',\n",
                "        'body': [sample_house, sample_house],\n",
                "        'curl_example': f'''curl -X POST http://localhost:5000/batch_predict \\\\\n",
                "  -H \"Content-Type: application/json\" \\\\\n",
                "  -d '{json.dumps([sample_house, sample_house])}'\n",
                "'''\n",
                "    },\n",
                "    'model_info': {\n",
                "        'method': 'GET',\n",
                "        'endpoint': '/model_info',\n",
                "        'curl_example': 'curl http://localhost:5000/model_info'\n",
                "    },\n",
                "    'health_check': {\n",
                "        'method': 'GET',\n",
                "        'endpoint': '/health',\n",
                "        'curl_example': 'curl http://localhost:5000/health'\n",
                "    }\n",
                "}\n",
                "\n",
                "# Save examples\n",
                "with open('models/api_examples.json', 'w') as f:\n",
                "    json.dump(api_examples, f, indent=2)\n",
                "\n",
                "print(\"‚úÖ API examples saved to: models/api_examples.json\")\n",
                "print(\"\\nüìö Example API Calls:\\n\")\n",
                "\n",
                "for name, example in api_examples.items():\n",
                "    print(f\"\\n{name.upper().replace('_', ' ')}:\")\n",
                "    print(example['curl_example'])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 4: Production Considerations\n",
                "### 8Ô∏è‚É£ Model Versioning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import shutil\n",
                "from pathlib import Path\n",
                "\n",
                "class ModelVersionManager:\n",
                "    \"\"\"\n",
                "    Manage multiple versions of trained models.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, base_dir='models/versions'):\n",
                "        self.base_dir = Path(base_dir)\n",
                "        self.base_dir.mkdir(parents=True, exist_ok=True)\n",
                "    \n",
                "    def save_version(self, model, metadata, version):\n",
                "        \"\"\"Save a new model version.\"\"\"\n",
                "        version_dir = self.base_dir / f\"v{version}\"\n",
                "        version_dir.mkdir(exist_ok=True)\n",
                "        \n",
                "        # Save model\n",
                "        model_path = version_dir / 'model.pkl'\n",
                "        joblib.dump(model, model_path)\n",
                "        \n",
                "        # Save metadata\n",
                "        metadata['model_info']['version'] = version\n",
                "        metadata_path = version_dir / 'metadata.json'\n",
                "        with open(metadata_path, 'w') as f:\n",
                "            json.dump(metadata, f, indent=2)\n",
                "        \n",
                "        print(f\"‚úÖ Saved model version {version}\")\n",
                "        return version_dir\n",
                "    \n",
                "    def load_version(self, version):\n",
                "        \"\"\"Load a specific model version.\"\"\"\n",
                "        version_dir = self.base_dir / f\"v{version}\"\n",
                "        \n",
                "        if not version_dir.exists():\n",
                "            raise ValueError(f\"Version {version} not found\")\n",
                "        \n",
                "        model = joblib.load(version_dir / 'model.pkl')\n",
                "        with open(version_dir / 'metadata.json', 'r') as f:\n",
                "            metadata = json.load(f)\n",
                "        \n",
                "        return model, metadata\n",
                "    \n",
                "    def list_versions(self):\n",
                "        \"\"\"List all available versions.\"\"\"\n",
                "        versions = []\n",
                "        for version_dir in self.base_dir.glob('v*'):\n",
                "            if version_dir.is_dir():\n",
                "                with open(version_dir / 'metadata.json', 'r') as f:\n",
                "                    metadata = json.load(f)\n",
                "                versions.append({\n",
                "                    'version': metadata['model_info']['version'],\n",
                "                    'created': metadata['model_info']['created_date'],\n",
                "                    'performance': metadata['performance']\n",
                "                })\n",
                "        return versions\n",
                "    \n",
                "    def set_production(self, version):\n",
                "        \"\"\"Set a version as production.\"\"\"\n",
                "        version_dir = self.base_dir / f\"v{version}\"\n",
                "        production_dir = self.base_dir.parent / 'production'\n",
                "        \n",
                "        # Remove old production\n",
                "        if production_dir.exists():\n",
                "            shutil.rmtree(production_dir)\n",
                "        \n",
                "        # Copy version to production\n",
                "        shutil.copytree(version_dir, production_dir)\n",
                "        \n",
                "        print(f\"‚úÖ Set version {version} as production\")\n",
                "\n",
                "# Test version manager\n",
                "vm = ModelVersionManager()\n",
                "vm.save_version(production_pipeline, metadata, '1.0.0')\n",
                "vm.set_production('1.0.0')\n",
                "\n",
                "print(\"\\nüì¶ Available Versions:\")\n",
                "for v in vm.list_versions():\n",
                "    print(f\"  Version {v['version']}: R¬≤={v['performance']['test_r2']:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 9Ô∏è‚É£ Monitoring and Logging"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import logging\n",
                "from collections import defaultdict\n",
                "\n",
                "# Set up logging\n",
                "logging.basicConfig(\n",
                "    level=logging.INFO,\n",
                "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
                "    handlers=[\n",
                "        logging.FileHandler('models/prediction.log'),\n",
                "        logging.StreamHandler()\n",
                "    ]\n",
                ")\n",
                "\n",
                "logger = logging.getLogger('HousePricePredictor')\n",
                "\n",
                "class MonitoredPredictor(HousePricePredictor):\n",
                "    \"\"\"\n",
                "    Predictor with monitoring and logging capabilities.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, *args, **kwargs):\n",
                "        super().__init__(*args, **kwargs)\n",
                "        self.prediction_stats = defaultdict(list)\n",
                "        logger.info(f\"Initialized predictor v{self.metadata['model_info']['version']}\")\n",
                "    \n",
                "    def predict_single(self, input_data):\n",
                "        \"\"\"Make prediction with logging.\"\"\"\n",
                "        try:\n",
                "            # Log request\n",
                "            logger.info(f\"Prediction request received\")\n",
                "            \n",
                "            # Make prediction\n",
                "            result = super().predict_single(input_data)\n",
                "            \n",
                "            # Track statistics\n",
                "            self.prediction_stats['predictions'].append(result['predicted_price'])\n",
                "            self.prediction_stats['timestamps'].append(result['timestamp'])\n",
                "            \n",
                "            # Log success\n",
                "            logger.info(f\"Prediction successful: ${result['predicted_price']:,.0f}\")\n",
                "            \n",
                "            return result\n",
                "        \n",
                "        except Exception as e:\n",
                "            logger.error(f\"Prediction failed: {str(e)}\")\n",
                "            raise\n",
                "    \n",
                "    def get_stats(self):\n",
                "        \"\"\"Get prediction statistics.\"\"\"\n",
                "        if not self.prediction_stats['predictions']:\n",
                "            return {'message': 'No predictions yet'}\n",
                "        \n",
                "        predictions = self.prediction_stats['predictions']\n",
                "        return {\n",
                "            'total_predictions': len(predictions),\n",
                "            'average_prediction': np.mean(predictions),\n",
                "            'min_prediction': np.min(predictions),\n",
                "            'max_prediction': np.max(predictions),\n",
                "            'std_prediction': np.std(predictions)\n",
                "        }\n",
                "\n",
                "# Test monitored predictor\n",
                "monitored_predictor = MonitoredPredictor()\n",
                "\n",
                "# Make some predictions\n",
                "for i in range(5):\n",
                "    result = monitored_predictor.predict_single(sample_house)\n",
                "\n",
                "print(\"\\nüìä Prediction Statistics:\")\n",
                "stats = monitored_predictor.get_stats()\n",
                "print(json.dumps(stats, indent=2, default=str))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### üîü Creating Deployment Documentation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "deployment_docs = '''\n",
                "# House Price Prediction Model - Deployment Guide\n",
                "\n",
                "## Model Information\n",
                "- **Model Name**: Housing Price Predictor\n",
                "- **Version**: 1.0.0\n",
                "- **Algorithm**: Random Forest Regressor\n",
                "- **Performance**: R¬≤ = {r2:.4f}, RMSE = ${rmse:,.2f}\n",
                "\n",
                "## Quick Start\n",
                "\n",
                "### 1. Installation\n",
                "```bash\n",
                "pip install -r requirements.txt\n",
                "```\n",
                "\n",
                "### 2. Run API Server\n",
                "```bash\n",
                "python models/app.py\n",
                "```\n",
                "\n",
                "### 3. Make Predictions\n",
                "```python\n",
                "import requests\n",
                "\n",
                "# Single prediction\n",
                "response = requests.post(\n",
                "    'http://localhost:5000/predict',\n",
                "    json={sample_data}\n",
                ")\n",
                "print(response.json())\n",
                "```\n",
                "\n",
                "## API Endpoints\n",
                "\n",
                "### GET /\n",
                "API information and available endpoints\n",
                "\n",
                "### POST /predict\n",
                "Make single prediction\n",
                "- **Input**: JSON object with house features\n",
                "- **Output**: Predicted price with confidence interval\n",
                "\n",
                "### POST /batch_predict\n",
                "Make batch predictions\n",
                "- **Input**: Array of JSON objects\n",
                "- **Output**: Array of predictions\n",
                "\n",
                "### GET /model_info\n",
                "Get model metadata and performance metrics\n",
                "\n",
                "### GET /health\n",
                "Health check endpoint\n",
                "\n",
                "## Required Features\n",
                "1. longitude\n",
                "2. latitude\n",
                "3. housing_median_age\n",
                "4. total_rooms\n",
                "5. total_bedrooms\n",
                "6. population\n",
                "7. households\n",
                "8. median_income\n",
                "\n",
                "## Production Deployment\n",
                "\n",
                "### Docker Deployment\n",
                "```dockerfile\n",
                "FROM python:3.10-slim\n",
                "WORKDIR /app\n",
                "COPY requirements.txt .\n",
                "RUN pip install -r requirements.txt\n",
                "COPY models/ models/\n",
                "CMD [\"python\", \"models/app.py\"]\n",
                "```\n",
                "\n",
                "### Environment Variables\n",
                "- `MODEL_PATH`: Path to model file\n",
                "- `PORT`: API port (default: 5000)\n",
                "- `LOG_LEVEL`: Logging level (default: INFO)\n",
                "\n",
                "## Monitoring\n",
                "\n",
                "### Metrics to Track\n",
                "1. **Prediction Latency**: Response time\n",
                "2. **Prediction Distribution**: Monitor for drift\n",
                "3. **Error Rate**: Failed predictions\n",
                "4. **Model Performance**: Compare predictions vs actuals\n",
                "\n",
                "### Logging\n",
                "All predictions are logged to `models/prediction.log`\n",
                "\n",
                "## Model Retraining\n",
                "\n",
                "### When to Retrain\n",
                "- Performance degradation detected\n",
                "- New data available\n",
                "- Scheduled (e.g., monthly)\n",
                "\n",
                "### Retraining Process\n",
                "1. Collect new data\n",
                "2. Train new model\n",
                "3. Validate performance\n",
                "4. Save as new version\n",
                "5. A/B test before full deployment\n",
                "\n",
                "## Troubleshooting\n",
                "\n",
                "### Common Issues\n",
                "1. **Missing Features**: Ensure all 8 features are provided\n",
                "2. **Invalid Values**: Check for negative values\n",
                "3. **Model Not Found**: Verify model path\n",
                "\n",
                "## Support\n",
                "For issues or questions, contact: ml-team@example.com\n",
                "'''.format(r2=r2, rmse=rmse, sample_data=json.dumps(sample_house, indent=2))\n",
                "\n",
                "# Save documentation\n",
                "with open('models/DEPLOYMENT.md', 'w') as f:\n",
                "    f.write(deployment_docs)\n",
                "\n",
                "print(\"‚úÖ Deployment documentation saved to: models/DEPLOYMENT.md\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìä Key Takeaways\n",
                "\n",
                "### Deployment Best Practices:\n",
                "\n",
                "#### 1. **Model Saving**:\n",
                "‚úÖ Use joblib for sklearn models (more efficient)  \n",
                "‚úÖ Save complete pipelines, not just models  \n",
                "‚úÖ Include preprocessing steps  \n",
                "‚úÖ Version your models  \n",
                "‚úÖ Save metadata with models  \n",
                "\n",
                "#### 2. **API Design**:\n",
                "‚úÖ RESTful endpoints  \n",
                "‚úÖ Input validation  \n",
                "‚úÖ Error handling  \n",
                "‚úÖ Health checks  \n",
                "‚úÖ Documentation  \n",
                "\n",
                "#### 3. **Production Considerations**:\n",
                "‚úÖ Logging and monitoring  \n",
                "‚úÖ Model versioning  \n",
                "‚úÖ A/B testing capability  \n",
                "‚úÖ Rollback strategy  \n",
                "‚úÖ Performance metrics  \n",
                "\n",
                "#### 4. **Monitoring**:\n",
                "‚úÖ Prediction latency  \n",
                "‚úÖ Error rates  \n",
                "‚úÖ Model drift detection  \n",
                "‚úÖ Data quality checks  \n",
                "‚úÖ Resource usage  \n",
                "\n",
                "### Deployment Checklist:\n",
                "\n",
                "- [ ] Model trained and validated\n",
                "- [ ] Model saved with metadata\n",
                "- [ ] Prediction function tested\n",
                "- [ ] API endpoints implemented\n",
                "- [ ] Input validation added\n",
                "- [ ] Error handling implemented\n",
                "- [ ] Logging configured\n",
                "- [ ] Documentation created\n",
                "- [ ] Health checks added\n",
                "- [ ] Monitoring set up\n",
                "- [ ] Version control in place\n",
                "- [ ] Rollback strategy defined\n",
                "\n",
                "### Common Deployment Patterns:\n",
                "\n",
                "| Pattern | Use Case | Pros | Cons |\n",
                "|---------|----------|------|------|\n",
                "| **Batch** | Scheduled predictions | Simple, efficient | Not real-time |\n",
                "| **REST API** | On-demand predictions | Flexible, scalable | Requires server |\n",
                "| **Streaming** | Real-time data | Low latency | Complex setup |\n",
                "| **Embedded** | Mobile/Edge | No network needed | Limited resources |\n",
                "\n",
                "### Tools and Frameworks:\n",
                "\n",
                "**API Frameworks:**\n",
                "- Flask (simple, lightweight)\n",
                "- FastAPI (modern, fast)\n",
                "- Django REST (full-featured)\n",
                "\n",
                "**Deployment Platforms:**\n",
                "- Docker (containerization)\n",
                "- Kubernetes (orchestration)\n",
                "- AWS SageMaker (managed ML)\n",
                "- Google Cloud AI Platform\n",
                "- Azure ML\n",
                "\n",
                "**Monitoring:**\n",
                "- Prometheus + Grafana\n",
                "- ELK Stack (Elasticsearch, Logstash, Kibana)\n",
                "- MLflow\n",
                "- Weights & Biases\n",
                "\n",
                "### Next Steps:\n",
                "1. Deploy to cloud platform (AWS, GCP, Azure)\n",
                "2. Implement CI/CD pipeline\n",
                "3. Add model monitoring dashboard\n",
                "4. Set up automated retraining\n",
                "5. Implement A/B testing framework"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "ml_env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.19"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}