{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Naive Bayes - Complete Guide\n",
                "\n",
                "## ðŸ“š Learning Objectives\n",
                "- Understand Naive Bayes algorithm and Bayes' theorem\n",
                "- Implement Gaussian, Multinomial, and Bernoulli Naive Bayes\n",
                "- Apply to text classification and spam detection\n",
                "- Understand the \"naive\" assumption and its implications\n",
                "- Compare different Naive Bayes variants\n",
                "- Handle real-world classification problems\n",
                "\n",
                "## ðŸŽ¯ What is Naive Bayes?\n",
                "\n",
                "**Naive Bayes** is a probabilistic classifier based on Bayes' theorem with the \"naive\" assumption of feature independence.\n",
                "\n",
                "### Bayes' Theorem:\n",
                "$$P(y|X) = \\frac{P(X|y) \\cdot P(y)}{P(X)}$$\n",
                "\n",
                "Where:\n",
                "- $P(y|X)$ = Posterior probability (probability of class y given features X)\n",
                "- $P(X|y)$ = Likelihood (probability of features X given class y)\n",
                "- $P(y)$ = Prior probability (probability of class y)\n",
                "- $P(X)$ = Evidence (probability of features X)\n",
                "\n",
                "### The \"Naive\" Assumption:\n",
                "Assumes all features are **independent** given the class:\n",
                "$$P(X|y) = P(x_1|y) \\cdot P(x_2|y) \\cdot ... \\cdot P(x_n|y)$$\n",
                "\n",
                "### Types of Naive Bayes:\n",
                "1. **Gaussian NB**: Continuous features (assumes normal distribution)\n",
                "2. **Multinomial NB**: Discrete counts (text classification, word counts)\n",
                "3. **Bernoulli NB**: Binary features (presence/absence)\n",
                "\n",
                "### When to Use:\n",
                "âœ… Text classification (spam detection, sentiment analysis)  \n",
                "âœ… Document categorization  \n",
                "âœ… Real-time prediction (very fast)  \n",
                "âœ… Small training datasets  \n",
                "âœ… High-dimensional data  \n",
                "âœ… Baseline model  "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.model_selection import train_test_split, cross_val_score\n",
                "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
                "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
                "from sklearn.metrics import (\n",
                "    accuracy_score, precision_score, recall_score, f1_score,\n",
                "    confusion_matrix, classification_report, roc_auc_score\n",
                ")\n",
                "from sklearn.datasets import load_iris, load_wine, fetch_20newsgroups\n",
                "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
                "from sklearn.pipeline import Pipeline\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Set style\n",
                "sns.set_style('whitegrid')\n",
                "plt.rcParams['figure.figsize'] = (12, 6)\n",
                "\n",
                "print(\"âœ… Libraries imported successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 1: Gaussian Naive Bayes\n",
                "### For continuous features (assumes normal distribution)\n",
                "### 1ï¸âƒ£ Load and Explore Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load iris dataset\n",
                "iris = load_iris()\n",
                "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
                "y = pd.Series(iris.target, name='species')\n",
                "\n",
                "print(f\"Dataset shape: {X.shape}\")\n",
                "print(f\"\\nClasses: {iris.target_names}\")\n",
                "print(f\"\\nClass distribution:\")\n",
                "print(y.value_counts().sort_index())\n",
                "\n",
                "# Display statistics\n",
                "print(\"\\nðŸ“Š Feature Statistics:\")\n",
                "print(X.describe())\n",
                "\n",
                "# Visualize distributions\n",
                "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
                "axes = axes.ravel()\n",
                "\n",
                "for idx, col in enumerate(X.columns):\n",
                "    for species in range(3):\n",
                "        data = X[y == species][col]\n",
                "        axes[idx].hist(data, bins=20, alpha=0.6, label=iris.target_names[species], edgecolor='black')\n",
                "    \n",
                "    axes[idx].set_xlabel(col, fontsize=11)\n",
                "    axes[idx].set_ylabel('Frequency', fontsize=11)\n",
                "    axes[idx].set_title(f'Distribution of {col}', fontsize=12, fontweight='bold')\n",
                "    axes[idx].legend()\n",
                "    axes[idx].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nðŸ’¡ Gaussian NB assumes features follow normal distribution within each class\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2ï¸âƒ£ Train Gaussian Naive Bayes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Split data\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, test_size=0.2, random_state=42, stratify=y\n",
                ")\n",
                "\n",
                "# Train Gaussian Naive Bayes\n",
                "gnb = GaussianNB()\n",
                "gnb.fit(X_train, y_train)\n",
                "\n",
                "# Predictions\n",
                "y_pred = gnb.predict(X_test)\n",
                "y_pred_proba = gnb.predict_proba(X_test)\n",
                "\n",
                "# Evaluate\n",
                "accuracy = accuracy_score(y_test, y_pred)\n",
                "print(f\"\\nðŸ“Š Gaussian Naive Bayes Performance:\")\n",
                "print(f\"Accuracy: {accuracy:.4f}\")\n",
                "print(f\"\\nðŸ“‹ Classification Report:\")\n",
                "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
                "\n",
                "# Cross-validation\n",
                "cv_scores = cross_val_score(gnb, X, y, cv=5, scoring='accuracy')\n",
                "print(f\"\\nðŸ”„ Cross-Validation Scores: {cv_scores}\")\n",
                "print(f\"Mean CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize results\n",
                "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
                "\n",
                "# Confusion Matrix\n",
                "cm = confusion_matrix(y_test, y_pred)\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
                "            xticklabels=iris.target_names,\n",
                "            yticklabels=iris.target_names,\n",
                "            cbar_kws={'label': 'Count'})\n",
                "axes[0].set_xlabel('Predicted Label', fontsize=12)\n",
                "axes[0].set_ylabel('True Label', fontsize=12)\n",
                "axes[0].set_title('Confusion Matrix - Gaussian NB', fontsize=14, fontweight='bold')\n",
                "\n",
                "# Prediction probabilities\n",
                "prob_df = pd.DataFrame(y_pred_proba, columns=iris.target_names)\n",
                "prob_df['True Class'] = y_test.values\n",
                "prob_df['Predicted Class'] = y_pred\n",
                "\n",
                "# Plot probability distribution\n",
                "for i in range(3):\n",
                "    class_probs = prob_df[prob_df['True Class'] == i].iloc[:, :3].max(axis=1)\n",
                "    axes[1].hist(class_probs, bins=20, alpha=0.6, label=iris.target_names[i], edgecolor='black')\n",
                "\n",
                "axes[1].set_xlabel('Maximum Predicted Probability', fontsize=12)\n",
                "axes[1].set_ylabel('Frequency', fontsize=12)\n",
                "axes[1].set_title('Prediction Confidence Distribution', fontsize=14, fontweight='bold')\n",
                "axes[1].legend()\n",
                "axes[1].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3ï¸âƒ£ Understanding Class Priors and Parameters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Class priors (learned from training data)\n",
                "print(\"ðŸ“Š Class Priors (P(y)):\")\n",
                "for i, prior in enumerate(gnb.class_prior_):\n",
                "    print(f\"{iris.target_names[i]}: {prior:.4f}\")\n",
                "\n",
                "# Feature means for each class (theta)\n",
                "print(\"\\nðŸ“Š Feature Means per Class (Î¼):\")\n",
                "theta_df = pd.DataFrame(gnb.theta_, columns=X.columns, index=iris.target_names)\n",
                "print(theta_df)\n",
                "\n",
                "# Feature variances for each class (sigma)\n",
                "print(\"\\nðŸ“Š Feature Variances per Class (ÏƒÂ²):\")\n",
                "sigma_df = pd.DataFrame(gnb.var_, columns=X.columns, index=iris.target_names)\n",
                "print(sigma_df)\n",
                "\n",
                "# Visualize means\n",
                "theta_df.T.plot(kind='bar', figsize=(12, 6), edgecolor='black')\n",
                "plt.title('Feature Means by Class', fontsize=14, fontweight='bold')\n",
                "plt.xlabel('Features', fontsize=12)\n",
                "plt.ylabel('Mean Value', fontsize=12)\n",
                "plt.xticks(rotation=45, ha='right')\n",
                "plt.legend(title='Species')\n",
                "plt.grid(True, alpha=0.3, axis='y')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 2: Multinomial Naive Bayes\n",
                "### For discrete count data (text classification)\n",
                "### 4ï¸âƒ£ Text Classification Example"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load text data (20 newsgroups dataset - subset)\n",
                "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
                "\n",
                "print(\"Loading 20 newsgroups dataset...\")\n",
                "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, \n",
                "                                      shuffle=True, random_state=42, remove=('headers', 'footers', 'quotes'))\n",
                "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories,\n",
                "                                     shuffle=True, random_state=42, remove=('headers', 'footers', 'quotes'))\n",
                "\n",
                "print(f\"\\nâœ… Data loaded!\")\n",
                "print(f\"Training samples: {len(newsgroups_train.data)}\")\n",
                "print(f\"Test samples: {len(newsgroups_test.data)}\")\n",
                "print(f\"\\nCategories: {newsgroups_train.target_names}\")\n",
                "\n",
                "# Show example\n",
                "print(f\"\\nðŸ“„ Example document:\")\n",
                "print(f\"Category: {newsgroups_train.target_names[newsgroups_train.target[0]]}\")\n",
                "print(f\"Text: {newsgroups_train.data[0][:300]}...\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create pipeline with CountVectorizer and Multinomial NB\n",
                "text_clf = Pipeline([\n",
                "    ('vect', CountVectorizer(max_features=5000, stop_words='english')),\n",
                "    ('clf', MultinomialNB(alpha=1.0))  # alpha is smoothing parameter\n",
                "])\n",
                "\n",
                "# Train\n",
                "print(\"Training Multinomial Naive Bayes...\")\n",
                "text_clf.fit(newsgroups_train.data, newsgroups_train.target)\n",
                "print(\"âœ… Training complete!\")\n",
                "\n",
                "# Predict\n",
                "y_pred_text = text_clf.predict(newsgroups_test.data)\n",
                "\n",
                "# Evaluate\n",
                "accuracy_text = accuracy_score(newsgroups_test.target, y_pred_text)\n",
                "print(f\"\\nðŸ“Š Multinomial Naive Bayes Performance:\")\n",
                "print(f\"Accuracy: {accuracy_text:.4f}\")\n",
                "print(f\"\\nðŸ“‹ Classification Report:\")\n",
                "print(classification_report(newsgroups_test.target, y_pred_text, \n",
                "                          target_names=newsgroups_test.target_names))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Confusion matrix for text classification\n",
                "cm_text = confusion_matrix(newsgroups_test.target, y_pred_text)\n",
                "\n",
                "plt.figure(figsize=(10, 8))\n",
                "sns.heatmap(cm_text, annot=True, fmt='d', cmap='YlOrRd',\n",
                "            xticklabels=newsgroups_test.target_names,\n",
                "            yticklabels=newsgroups_test.target_names,\n",
                "            cbar_kws={'label': 'Count'})\n",
                "plt.xlabel('Predicted Category', fontsize=12)\n",
                "plt.ylabel('True Category', fontsize=12)\n",
                "plt.title('Confusion Matrix - Text Classification', fontsize=14, fontweight='bold')\n",
                "plt.xticks(rotation=45, ha='right')\n",
                "plt.yticks(rotation=0)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5ï¸âƒ£ Feature Importance in Text Classification"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get feature names (words)\n",
                "feature_names = text_clf.named_steps['vect'].get_feature_names_out()\n",
                "\n",
                "# Get log probabilities for each class\n",
                "feature_log_prob = text_clf.named_steps['clf'].feature_log_prob_\n",
                "\n",
                "# Find top words for each category\n",
                "n_top_words = 15\n",
                "\n",
                "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
                "axes = axes.ravel()\n",
                "\n",
                "for idx, category in enumerate(newsgroups_test.target_names):\n",
                "    # Get top features for this category\n",
                "    top_indices = np.argsort(feature_log_prob[idx])[-n_top_words:]\n",
                "    top_features = [feature_names[i] for i in top_indices]\n",
                "    top_scores = feature_log_prob[idx][top_indices]\n",
                "    \n",
                "    # Plot\n",
                "    axes[idx].barh(range(n_top_words), top_scores, color='skyblue', edgecolor='black')\n",
                "    axes[idx].set_yticks(range(n_top_words))\n",
                "    axes[idx].set_yticklabels(top_features)\n",
                "    axes[idx].set_xlabel('Log Probability', fontsize=11)\n",
                "    axes[idx].set_title(f'Top {n_top_words} Words: {category}', fontsize=12, fontweight='bold')\n",
                "    axes[idx].grid(True, alpha=0.3, axis='x')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nðŸ’¡ These words have highest probability for each category\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6ï¸âƒ£ Smoothing Parameter (Alpha) Tuning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test different alpha values (Laplace smoothing)\n",
                "alphas = [0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
                "accuracies = []\n",
                "\n",
                "for alpha in alphas:\n",
                "    clf = Pipeline([\n",
                "        ('vect', CountVectorizer(max_features=5000, stop_words='english')),\n",
                "        ('clf', MultinomialNB(alpha=alpha))\n",
                "    ])\n",
                "    \n",
                "    clf.fit(newsgroups_train.data, newsgroups_train.target)\n",
                "    y_pred = clf.predict(newsgroups_test.data)\n",
                "    acc = accuracy_score(newsgroups_test.target, y_pred)\n",
                "    accuracies.append(acc)\n",
                "    print(f\"Alpha = {alpha:6.3f} -> Accuracy = {acc:.4f}\")\n",
                "\n",
                "# Plot\n",
                "plt.figure(figsize=(12, 6))\n",
                "plt.plot(alphas, accuracies, marker='o', linewidth=2, markersize=8, color='green')\n",
                "plt.xscale('log')\n",
                "plt.xlabel('Alpha (Smoothing Parameter)', fontsize=12)\n",
                "plt.ylabel('Accuracy', fontsize=12)\n",
                "plt.title('Impact of Smoothing Parameter on Accuracy', fontsize=14, fontweight='bold')\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.axvline(x=alphas[np.argmax(accuracies)], color='red', linestyle='--', \n",
                "           label=f'Best Alpha = {alphas[np.argmax(accuracies)]}')\n",
                "plt.legend(fontsize=11)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nðŸŽ¯ Best Alpha: {alphas[np.argmax(accuracies)]} with Accuracy: {max(accuracies):.4f}\")\n",
                "print(\"\\nðŸ’¡ Alpha = 1.0 is Laplace smoothing (add-one smoothing)\")\n",
                "print(\"ðŸ’¡ Smaller alpha = less smoothing (risk of zero probabilities)\")\n",
                "print(\"ðŸ’¡ Larger alpha = more smoothing (more uniform probabilities)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 3: Bernoulli Naive Bayes\n",
                "### For binary features (presence/absence)\n",
                "### 7ï¸âƒ£ Binary Feature Classification"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create binary features (word presence/absence)\n",
                "binary_clf = Pipeline([\n",
                "    ('vect', CountVectorizer(max_features=5000, stop_words='english', binary=True)),\n",
                "    ('clf', BernoulliNB(alpha=1.0))\n",
                "])\n",
                "\n",
                "# Train\n",
                "print(\"Training Bernoulli Naive Bayes...\")\n",
                "binary_clf.fit(newsgroups_train.data, newsgroups_train.target)\n",
                "print(\"âœ… Training complete!\")\n",
                "\n",
                "# Predict\n",
                "y_pred_binary = binary_clf.predict(newsgroups_test.data)\n",
                "\n",
                "# Evaluate\n",
                "accuracy_binary = accuracy_score(newsgroups_test.target, y_pred_binary)\n",
                "print(f\"\\nðŸ“Š Bernoulli Naive Bayes Performance:\")\n",
                "print(f\"Accuracy: {accuracy_binary:.4f}\")\n",
                "print(f\"\\nðŸ“‹ Classification Report:\")\n",
                "print(classification_report(newsgroups_test.target, y_pred_binary,\n",
                "                          target_names=newsgroups_test.target_names))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 8ï¸âƒ£ Compare All Three Variants"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare Multinomial vs Bernoulli on text data\n",
                "# Also compare with TF-IDF\n",
                "\n",
                "models = [\n",
                "    ('Multinomial NB (Count)', Pipeline([\n",
                "        ('vect', CountVectorizer(max_features=5000, stop_words='english')),\n",
                "        ('clf', MultinomialNB())\n",
                "    ])),\n",
                "    ('Multinomial NB (TF-IDF)', Pipeline([\n",
                "        ('vect', TfidfVectorizer(max_features=5000, stop_words='english')),\n",
                "        ('clf', MultinomialNB())\n",
                "    ])),\n",
                "    ('Bernoulli NB', Pipeline([\n",
                "        ('vect', CountVectorizer(max_features=5000, stop_words='english', binary=True)),\n",
                "        ('clf', BernoulliNB())\n",
                "    ]))\n",
                "]\n",
                "\n",
                "results_comparison = []\n",
                "\n",
                "for name, model in models:\n",
                "    # Train\n",
                "    model.fit(newsgroups_train.data, newsgroups_train.target)\n",
                "    \n",
                "    # Predict\n",
                "    y_pred = model.predict(newsgroups_test.data)\n",
                "    \n",
                "    # Metrics\n",
                "    results_comparison.append({\n",
                "        'Model': name,\n",
                "        'Accuracy': accuracy_score(newsgroups_test.target, y_pred),\n",
                "        'Precision': precision_score(newsgroups_test.target, y_pred, average='weighted'),\n",
                "        'Recall': recall_score(newsgroups_test.target, y_pred, average='weighted'),\n",
                "        'F1-Score': f1_score(newsgroups_test.target, y_pred, average='weighted')\n",
                "    })\n",
                "\n",
                "comparison_df = pd.DataFrame(results_comparison)\n",
                "print(\"\\nðŸ“Š Model Comparison:\")\n",
                "print(comparison_df.to_string(index=False))\n",
                "\n",
                "# Visualize\n",
                "comparison_df.set_index('Model').plot(kind='bar', figsize=(14, 6), edgecolor='black')\n",
                "plt.title('Naive Bayes Variants Comparison', fontsize=14, fontweight='bold')\n",
                "plt.ylabel('Score', fontsize=12)\n",
                "plt.xlabel('Model', fontsize=12)\n",
                "plt.xticks(rotation=45, ha='right')\n",
                "plt.legend(title='Metric', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
                "plt.grid(True, alpha=0.3, axis='y')\n",
                "plt.ylim(0.7, 1.0)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 9ï¸âƒ£ Spam Detection Example"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create synthetic spam detection dataset\n",
                "spam_data = [\n",
                "    # Spam messages\n",
                "    (\"Congratulations! You've won a free iPhone. Click here to claim\", 1),\n",
                "    (\"URGENT: Your account will be closed. Verify now\", 1),\n",
                "    (\"Get rich quick! Make $5000 per week from home\", 1),\n",
                "    (\"Free money! Click here now! Limited time offer!\", 1),\n",
                "    (\"You have won the lottery! Send your details to claim prize\", 1),\n",
                "    (\"Cheap viagra! Buy now! Special discount!\", 1),\n",
                "    (\"Work from home and earn thousands! No experience needed!\", 1),\n",
                "    (\"Your package is waiting. Pay shipping fee to receive\", 1),\n",
                "    \n",
                "    # Ham (legitimate) messages\n",
                "    (\"Hi, are we still meeting for lunch tomorrow?\", 0),\n",
                "    (\"The project deadline has been extended to next Friday\", 0),\n",
                "    (\"Can you send me the report when you get a chance?\", 0),\n",
                "    (\"Thanks for your help with the presentation yesterday\", 0),\n",
                "    (\"Meeting scheduled for 3 PM in conference room B\", 0),\n",
                "    (\"Please review the attached document and provide feedback\", 0),\n",
                "    (\"Your order has been shipped and will arrive in 3-5 days\", 0),\n",
                "    (\"Reminder: Team meeting at 10 AM tomorrow\", 0)\n",
                "]\n",
                "\n",
                "# Expand dataset with variations\n",
                "spam_messages = [msg for msg, label in spam_data if label == 1] * 10\n",
                "ham_messages = [msg for msg, label in spam_data if label == 0] * 10\n",
                "\n",
                "all_messages = spam_messages + ham_messages\n",
                "all_labels = [1] * len(spam_messages) + [0] * len(ham_messages)\n",
                "\n",
                "# Split data\n",
                "X_train_spam, X_test_spam, y_train_spam, y_test_spam = train_test_split(\n",
                "    all_messages, all_labels, test_size=0.2, random_state=42, stratify=all_labels\n",
                ")\n",
                "\n",
                "print(f\"Spam detection dataset:\")\n",
                "print(f\"Training samples: {len(X_train_spam)}\")\n",
                "print(f\"Test samples: {len(X_test_spam)}\")\n",
                "print(f\"\\nClass distribution:\")\n",
                "print(f\"Spam: {sum(y_train_spam)} ({sum(y_train_spam)/len(y_train_spam)*100:.1f}%)\")\n",
                "print(f\"Ham: {len(y_train_spam) - sum(y_train_spam)} ({(len(y_train_spam) - sum(y_train_spam))/len(y_train_spam)*100:.1f}%)\")\n",
                "\n",
                "# Train spam classifier\n",
                "spam_clf = Pipeline([\n",
                "    ('vect', CountVectorizer(stop_words='english')),\n",
                "    ('clf', MultinomialNB(alpha=1.0))\n",
                "])\n",
                "\n",
                "spam_clf.fit(X_train_spam, y_train_spam)\n",
                "y_pred_spam = spam_clf.predict(X_test_spam)\n",
                "\n",
                "# Evaluate\n",
                "print(f\"\\nðŸ“Š Spam Detection Performance:\")\n",
                "print(f\"Accuracy: {accuracy_score(y_test_spam, y_pred_spam):.4f}\")\n",
                "print(f\"\\nðŸ“‹ Classification Report:\")\n",
                "print(classification_report(y_test_spam, y_pred_spam, target_names=['Ham', 'Spam']))\n",
                "\n",
                "# Test on new messages\n",
                "test_messages = [\n",
                "    \"Free money! Click now!\",\n",
                "    \"Meeting at 3 PM tomorrow\",\n",
                "    \"You won a prize! Claim now!\",\n",
                "    \"Can you review this document?\"\n",
                "]\n",
                "\n",
                "predictions = spam_clf.predict(test_messages)\n",
                "probabilities = spam_clf.predict_proba(test_messages)\n",
                "\n",
                "print(\"\\nðŸ” Test Predictions:\")\n",
                "for msg, pred, prob in zip(test_messages, predictions, probabilities):\n",
                "    label = 'SPAM' if pred == 1 else 'HAM'\n",
                "    confidence = prob[pred] * 100\n",
                "    print(f\"\\nMessage: '{msg}'\")\n",
                "    print(f\"Prediction: {label} (Confidence: {confidence:.1f}%)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ“Š Key Takeaways\n",
                "\n",
                "### Naive Bayes Advantages:\n",
                "âœ… **Fast**: Very quick training and prediction  \n",
                "âœ… **Simple**: Easy to understand and implement  \n",
                "âœ… **Probabilistic**: Provides probability estimates  \n",
                "âœ… **Works with small data**: Effective even with limited training data  \n",
                "âœ… **High-dimensional**: Handles many features well  \n",
                "âœ… **Baseline**: Excellent baseline for text classification  \n",
                "\n",
                "### Disadvantages:\n",
                "âŒ **Independence assumption**: Assumes features are independent (often violated)  \n",
                "âŒ **Zero probability**: Can assign zero probability to unseen features (use smoothing)  \n",
                "âŒ **Not always accurate**: Simpler models may underperform on complex tasks  \n",
                "\n",
                "### Which Variant to Use?\n",
                "\n",
                "| Variant | Use Case | Feature Type | Example |\n",
                "|---------|----------|--------------|----------|\n",
                "| **Gaussian NB** | Continuous features | Real numbers | Iris classification, sensor data |\n",
                "| **Multinomial NB** | Discrete counts | Word counts, frequencies | Text classification, document categorization |\n",
                "| **Bernoulli NB** | Binary features | Presence/absence | Spam detection, sentiment analysis |\n",
                "\n",
                "### Best Practices:\n",
                "1. **Use smoothing** - Always use alpha > 0 to avoid zero probabilities\n",
                "2. **Feature engineering** - Remove irrelevant features, use domain knowledge\n",
                "3. **Text preprocessing** - Remove stop words, use stemming/lemmatization\n",
                "4. **Try different variants** - Test Multinomial vs Bernoulli for text\n",
                "5. **Tune alpha** - Cross-validate to find optimal smoothing parameter\n",
                "\n",
                "### When to Use Naive Bayes:\n",
                "âœ… Text classification (spam, sentiment, topic)  \n",
                "âœ… Real-time prediction (very fast)  \n",
                "âœ… Small training datasets  \n",
                "âœ… Baseline model for comparison  \n",
                "âœ… High-dimensional sparse data  \n",
                "âœ… Probabilistic predictions needed  \n",
                "\n",
                "### When NOT to Use:\n",
                "âŒ Features are highly correlated  \n",
                "âŒ Need maximum accuracy (use ensemble methods)  \n",
                "âŒ Complex feature interactions  \n",
                "âŒ Non-linear decision boundaries  \n",
                "\n",
                "### Smoothing (Alpha Parameter):\n",
                "- **Alpha = 0**: No smoothing (risk of zero probabilities)\n",
                "- **Alpha = 1**: Laplace smoothing (add-one smoothing) - **recommended default**\n",
                "- **Alpha > 1**: More aggressive smoothing\n",
                "- **Tune via cross-validation** for best results\n",
                "\n",
                "### Real-World Applications:\n",
                "1. **Email spam filtering** - Gmail, Outlook\n",
                "2. **Sentiment analysis** - Social media monitoring\n",
                "3. **Document classification** - News categorization\n",
                "4. **Medical diagnosis** - Disease prediction\n",
                "5. **Recommendation systems** - Content filtering\n",
                "\n",
                "### Performance Tips:\n",
                "- For text: Try both CountVectorizer and TfidfVectorizer\n",
                "- Limit vocabulary size (max_features) for speed\n",
                "- Remove stop words and rare words\n",
                "- Use binary features for presence/absence tasks\n",
                "- Consider feature selection for high-dimensional data\n",
                "\n",
                "### Next Steps:\n",
                "1. Apply to your own text classification problem\n",
                "2. Compare with Logistic Regression and SVM\n",
                "3. Try ensemble methods (combine with other classifiers)\n",
                "4. Experiment with feature engineering\n",
                "5. Deploy as real-time spam filter"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "ml_env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.19"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}