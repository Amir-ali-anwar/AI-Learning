{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "intro",
            "metadata": {},
            "source": [
                "# Gradient Boosting Regression\n",
                "\n",
                "This notebook explores Boosting algorithms (Gradient Boosting, AdaBoost, XGBoost) using Scikit-Learn Pipelines on the Housing Dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "imports",
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install xgboost -q\n",
                "\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor\n",
                "from xgboost import XGBRegressor\n",
                "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
                "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
                "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
                "from sklearn.impute import SimpleImputer\n",
                "from sklearn.compose import ColumnTransformer\n",
                "from sklearn.pipeline import Pipeline"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "load_data",
            "metadata": {},
            "source": [
                "### 1️⃣ Load & Split Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "load_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "df = pd.read_csv(\"../Linear Regression/data/dataset.csv\") \n",
                "target_column = 'median_house_value'\n",
                "X = df.drop(columns=[target_column])\n",
                "y = df[target_column]\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "print(\"Training set:\", X_train.shape, \"Testing set:\", X_test.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "pipeline_setup",
            "metadata": {},
            "source": [
                "### 2️⃣ Preprocessing Pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "pipeline_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "num_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
                "cat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
                "\n",
                "# Boosting models benefit from imputation. Scaling is not strictly required but harmless.\n",
                "num_transformer = Pipeline(steps=[\n",
                "    ('imputer', SimpleImputer(strategy='median')),\n",
                "    ('scaler', StandardScaler()) \n",
                "])\n",
                "\n",
                "cat_transformer = Pipeline(steps=[\n",
                "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
                "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
                "])\n",
                "\n",
                "preprocessor = ColumnTransformer(\n",
                "    transformers=[\n",
                "        ('num', num_transformer, num_cols),\n",
                "        ('cat', cat_transformer, cat_cols)\n",
                "    ])"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "gbr_model",
            "metadata": {},
            "source": [
                "### 3️⃣ Gradient Boosting Regressor (GBR)\n",
                "GBR builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "gbr_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "gbr_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
                "                               ('regressor', GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42))])\n",
                "\n",
                "gbr_pipeline.fit(X_train, y_train)\n",
                "y_pred_gbr = gbr_pipeline.predict(X_test)\n",
                "\n",
                "print(\"--- Gradient Boosting ---\")\n",
                "print(\"MSE:\", mean_squared_error(y_test, y_pred_gbr))\n",
                "print(\"MAE:\", mean_absolute_error(y_test, y_pred_gbr))\n",
                "print(\"R2:\", r2_score(y_test, y_pred_gbr))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "xgboost_model",
            "metadata": {},
            "source": [
                "### 4️⃣ XGBoost Regressor\n",
                "eXtreme Gradient Boosting is an efficient and scalable implementation of gradient boosting."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "xgb_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "xgb_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
                "                               ('regressor', XGBRegressor(n_estimators=100, learning_rate=0.1, n_jobs=-1, random_state=42))])\n",
                "\n",
                "xgb_pipeline.fit(X_train, y_train)\n",
                "y_pred_xgb = xgb_pipeline.predict(X_test)\n",
                "\n",
                "print(\"--- XGBoost ---\")\n",
                "print(\"MAE:\", mean_absolute_error(y_test, y_pred_xgb))\n",
                "print(\"R2:\", r2_score(y_test, y_pred_xgb))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "tuning",
            "metadata": {},
            "source": [
                "### 5️⃣ Hyperparameter Tuning (GridSearchCV on GBR)\n",
                "We can tune parameters like `n_estimators`, `learning_rate`, and `max_depth` inside the pipeline."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "grid_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "param_grid = {\n",
                "    'regressor__n_estimators': [100, 200],\n",
                "    'regressor__learning_rate': [0.05, 0.1],\n",
                "    'regressor__max_depth': [3, 5]\n",
                "}\n",
                "\n",
                "# Using a smaller grid for demonstration speed. Expand for better results.\n",
                "search = GridSearchCV(gbr_pipeline, param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
                "search.fit(X_train, y_train)\n",
                "\n",
                "print(\"Best Parameters:\", search.best_params_)\n",
                "print(\"Best CV Score (RMSE):\", np.sqrt(-search.best_score_))\n",
                "\n",
                "best_model = search.best_estimator_\n",
                "y_pred_best = best_model.predict(X_test)\n",
                "print(\"Test R2 (Tuned Model):\", r2_score(y_test, y_pred_best))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}