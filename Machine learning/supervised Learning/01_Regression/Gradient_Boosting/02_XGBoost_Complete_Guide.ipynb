{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# XGBoost - Complete Guide\n",
                "\n",
                "## üìö Learning Objectives\n",
                "- Understand XGBoost algorithm and its advantages\n",
                "- Implement XGBoost for regression and classification\n",
                "- Master hyperparameter tuning\n",
                "- Learn feature importance analysis\n",
                "- Handle imbalanced datasets\n",
                "- Optimize model performance\n",
                "\n",
                "## üéØ What is XGBoost?\n",
                "\n",
                "**XGBoost (eXtreme Gradient Boosting)** is an optimized distributed gradient boosting library designed to be:\n",
                "- **Highly efficient**: Parallel processing, cache optimization\n",
                "- **Flexible**: Custom objectives and evaluation metrics\n",
                "- **Portable**: Works on various platforms\n",
                "\n",
                "### Key Features:\n",
                "1. **Regularization**: L1 (Lasso) and L2 (Ridge) to prevent overfitting\n",
                "2. **Handling Missing Values**: Built-in support\n",
                "3. **Tree Pruning**: Uses max_depth and then prunes backward\n",
                "4. **Built-in Cross-Validation**: Easy model evaluation\n",
                "5. **Parallel Processing**: Fast training\n",
                "\n",
                "### Why XGBoost?\n",
                "‚úÖ State-of-the-art performance on structured data  \n",
                "‚úÖ Wins many Kaggle competitions  \n",
                "‚úÖ Handles various types of data  \n",
                "‚úÖ Built-in regularization  \n",
                "‚úÖ Feature importance  "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install XGBoost if not already installed\n",
                "# !pip install xgboost\n",
                "\n",
                "# Import libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import xgboost as xgb\n",
                "from xgboost import XGBRegressor, XGBClassifier\n",
                "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
                "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
                "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
                "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
                "from sklearn.datasets import load_breast_cancer\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Set style\n",
                "sns.set_style('whitegrid')\n",
                "plt.rcParams['figure.figsize'] = (12, 6)\n",
                "\n",
                "print(f\"XGBoost version: {xgb.__version__}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 1: XGBoost for Regression\n",
                "### 1Ô∏è‚É£ Load and Prepare Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load California Housing dataset\n",
                "df = pd.read_csv('../../Linear Regression/data/dataset.csv')\n",
                "\n",
                "print(f\"Dataset shape: {df.shape}\")\n",
                "print(f\"\\nColumns: {list(df.columns)}\")\n",
                "print(f\"\\nMissing values:\")\n",
                "print(df.isnull().sum())\n",
                "\n",
                "# Display first few rows\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare features and target\n",
                "target_col = 'median_house_value'\n",
                "X = df.drop(columns=[target_col])\n",
                "y = df[target_col]\n",
                "\n",
                "# Handle categorical variables\n",
                "if 'ocean_proximity' in X.columns:\n",
                "    # XGBoost can't handle categorical directly, so we'll encode\n",
                "    le = LabelEncoder()\n",
                "    X['ocean_proximity'] = le.fit_transform(X['ocean_proximity'])\n",
                "    print(f\"\\nEncoded ocean_proximity: {le.classes_}\")\n",
                "\n",
                "# Handle missing values (XGBoost can handle them, but let's fill for comparison)\n",
                "X = X.fillna(X.median())\n",
                "\n",
                "print(f\"\\nFeatures shape: {X.shape}\")\n",
                "print(f\"Target shape: {y.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train-test split\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, test_size=0.2, random_state=42\n",
                ")\n",
                "\n",
                "print(f\"Training set: {X_train.shape[0]} samples\")\n",
                "print(f\"Test set: {X_test.shape[0]} samples\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2Ô∏è‚É£ Basic XGBoost Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create and train basic XGBoost model\n",
                "xgb_basic = XGBRegressor(\n",
                "    n_estimators=100,\n",
                "    learning_rate=0.1,\n",
                "    max_depth=5,\n",
                "    random_state=42,\n",
                "    n_jobs=-1  # Use all CPU cores\n",
                ")\n",
                "\n",
                "# Train model\n",
                "print(\"Training XGBoost model...\")\n",
                "xgb_basic.fit(X_train, y_train)\n",
                "print(\"‚úÖ Training complete!\")\n",
                "\n",
                "# Make predictions\n",
                "y_train_pred = xgb_basic.predict(X_train)\n",
                "y_test_pred = xgb_basic.predict(X_test)\n",
                "\n",
                "# Evaluate\n",
                "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
                "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
                "train_r2 = r2_score(y_train, y_train_pred)\n",
                "test_r2 = r2_score(y_test, y_test_pred)\n",
                "\n",
                "print(f\"\\nüìä Basic XGBoost Performance:\")\n",
                "print(f\"\\nTraining Metrics:\")\n",
                "print(f\"  RMSE: ${train_rmse:,.2f}\")\n",
                "print(f\"  R¬≤:   {train_r2:.4f}\")\n",
                "print(f\"\\nTest Metrics:\")\n",
                "print(f\"  RMSE: ${test_rmse:,.2f}\")\n",
                "print(f\"  R¬≤:   {test_r2:.4f}\")\n",
                "print(f\"\\nOverfitting Check: {train_r2 - test_r2:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3Ô∏è‚É£ Feature Importance Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get feature importance\n",
                "feature_importance = pd.DataFrame({\n",
                "    'feature': X.columns,\n",
                "    'importance': xgb_basic.feature_importances_\n",
                "}).sort_values('importance', ascending=False)\n",
                "\n",
                "print(\"\\nüîç Feature Importance:\")\n",
                "print(feature_importance)\n",
                "\n",
                "# Visualize feature importance\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
                "\n",
                "# Bar plot\n",
                "feature_importance.plot(x='feature', y='importance', kind='barh', ax=ax1, color='skyblue', edgecolor='black')\n",
                "ax1.set_xlabel('Importance Score', fontsize=12)\n",
                "ax1.set_ylabel('Features', fontsize=12)\n",
                "ax1.set_title('Feature Importance (Gain)', fontsize=14, fontweight='bold')\n",
                "ax1.grid(True, alpha=0.3, axis='x')\n",
                "\n",
                "# Built-in XGBoost plot\n",
                "xgb.plot_importance(xgb_basic, ax=ax2, importance_type='weight', max_num_features=10)\n",
                "ax2.set_title('Feature Importance (Weight)', fontsize=14, fontweight='bold')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4Ô∏è‚É£ Hyperparameter Tuning\n",
                "\n",
                "#### Key XGBoost Parameters:\n",
                "\n",
                "**Tree Parameters:**\n",
                "- `max_depth`: Maximum depth of trees (3-10)\n",
                "- `min_child_weight`: Minimum sum of instance weight in a child (1-10)\n",
                "- `gamma`: Minimum loss reduction for split (0-5)\n",
                "\n",
                "**Boosting Parameters:**\n",
                "- `learning_rate` (eta): Step size shrinkage (0.01-0.3)\n",
                "- `n_estimators`: Number of boosting rounds (100-1000)\n",
                "- `subsample`: Fraction of samples for training (0.5-1.0)\n",
                "- `colsample_bytree`: Fraction of features for training (0.5-1.0)\n",
                "\n",
                "**Regularization:**\n",
                "- `reg_alpha`: L1 regularization (0-1)\n",
                "- `reg_lambda`: L2 regularization (0-1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define parameter grid for tuning\n",
                "param_grid = {\n",
                "    'max_depth': [3, 5, 7],\n",
                "    'learning_rate': [0.01, 0.1, 0.3],\n",
                "    'n_estimators': [100, 200, 300],\n",
                "    'subsample': [0.8, 1.0],\n",
                "    'colsample_bytree': [0.8, 1.0],\n",
                "    'reg_alpha': [0, 0.1],\n",
                "    'reg_lambda': [1, 1.5]\n",
                "}\n",
                "\n",
                "# Use RandomizedSearchCV for efficiency (GridSearchCV would take too long)\n",
                "print(\"üîç Starting hyperparameter tuning...\")\n",
                "print(\"This may take a few minutes...\\n\")\n",
                "\n",
                "xgb_random = RandomizedSearchCV(\n",
                "    XGBRegressor(random_state=42, n_jobs=-1),\n",
                "    param_distributions=param_grid,\n",
                "    n_iter=20,  # Number of parameter settings sampled\n",
                "    cv=3,\n",
                "    scoring='neg_mean_squared_error',\n",
                "    random_state=42,\n",
                "    n_jobs=-1,\n",
                "    verbose=1\n",
                ")\n",
                "\n",
                "xgb_random.fit(X_train, y_train)\n",
                "\n",
                "print(f\"\\nüèÜ Best Parameters: {xgb_random.best_params_}\")\n",
                "print(f\"Best CV Score (RMSE): ${np.sqrt(-xgb_random.best_score_):,.2f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate tuned model\n",
                "best_xgb = xgb_random.best_estimator_\n",
                "\n",
                "y_train_pred_tuned = best_xgb.predict(X_train)\n",
                "y_test_pred_tuned = best_xgb.predict(X_test)\n",
                "\n",
                "train_rmse_tuned = np.sqrt(mean_squared_error(y_train, y_train_pred_tuned))\n",
                "test_rmse_tuned = np.sqrt(mean_squared_error(y_test, y_test_pred_tuned))\n",
                "train_r2_tuned = r2_score(y_train, y_train_pred_tuned)\n",
                "test_r2_tuned = r2_score(y_test, y_test_pred_tuned)\n",
                "\n",
                "print(f\"\\nüìä Tuned XGBoost Performance:\")\n",
                "print(f\"\\nTraining Metrics:\")\n",
                "print(f\"  RMSE: ${train_rmse_tuned:,.2f}\")\n",
                "print(f\"  R¬≤:   {train_r2_tuned:.4f}\")\n",
                "print(f\"\\nTest Metrics:\")\n",
                "print(f\"  RMSE: ${test_rmse_tuned:,.2f}\")\n",
                "print(f\"  R¬≤:   {test_r2_tuned:.4f}\")\n",
                "\n",
                "# Compare with basic model\n",
                "print(f\"\\nüìà Improvement:\")\n",
                "print(f\"  RMSE: ${test_rmse - test_rmse_tuned:,.2f} better\")\n",
                "print(f\"  R¬≤:   {test_r2_tuned - test_r2:.4f} better\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5Ô∏è‚É£ Learning Curves and Early Stopping"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train with early stopping\n",
                "xgb_early = XGBRegressor(\n",
                "    n_estimators=1000,\n",
                "    learning_rate=0.1,\n",
                "    max_depth=5,\n",
                "    random_state=42,\n",
                "    n_jobs=-1,\n",
                "    early_stopping_rounds=50  # Stop if no improvement for 50 rounds\n",
                ")\n",
                "\n",
                "# Fit with evaluation set\n",
                "xgb_early.fit(\n",
                "    X_train, y_train,\n",
                "    eval_set=[(X_train, y_train), (X_test, y_test)],\n",
                "    verbose=False\n",
                ")\n",
                "\n",
                "# Get evaluation results\n",
                "results = xgb_early.evals_result()\n",
                "\n",
                "# Plot learning curves\n",
                "epochs = len(results['validation_0']['rmse'])\n",
                "x_axis = range(0, epochs)\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(12, 6))\n",
                "ax.plot(x_axis, results['validation_0']['rmse'], label='Train', linewidth=2)\n",
                "ax.plot(x_axis, results['validation_1']['rmse'], label='Test', linewidth=2)\n",
                "ax.axvline(x=xgb_early.best_iteration, color='r', linestyle='--', \n",
                "           linewidth=2, label=f'Best Iteration ({xgb_early.best_iteration})')\n",
                "ax.set_xlabel('Boosting Rounds', fontsize=12)\n",
                "ax.set_ylabel('RMSE', fontsize=12)\n",
                "ax.set_title('XGBoost Learning Curves with Early Stopping', fontsize=14, fontweight='bold')\n",
                "ax.legend(fontsize=12)\n",
                "ax.grid(True, alpha=0.3)\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nBest iteration: {xgb_early.best_iteration}\")\n",
                "print(f\"Best RMSE: ${xgb_early.best_score:,.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 2: XGBoost for Classification\n",
                "### 6Ô∏è‚É£ Binary Classification Example"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load breast cancer dataset\n",
                "cancer = load_breast_cancer()\n",
                "X_cancer = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
                "y_cancer = pd.Series(cancer.target, name='diagnosis')\n",
                "\n",
                "print(f\"Classification dataset shape: {X_cancer.shape}\")\n",
                "print(f\"\\nClass distribution:\")\n",
                "print(y_cancer.value_counts())\n",
                "print(f\"\\nClass names: {cancer.target_names}\")\n",
                "\n",
                "# Split data\n",
                "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n",
                "    X_cancer, y_cancer, test_size=0.2, random_state=42, stratify=y_cancer\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train XGBoost classifier\n",
                "xgb_clf = XGBClassifier(\n",
                "    n_estimators=100,\n",
                "    learning_rate=0.1,\n",
                "    max_depth=5,\n",
                "    random_state=42,\n",
                "    n_jobs=-1,\n",
                "    eval_metric='logloss'  # For binary classification\n",
                ")\n",
                "\n",
                "xgb_clf.fit(X_train_c, y_train_c)\n",
                "\n",
                "# Predictions\n",
                "y_pred_c = xgb_clf.predict(X_test_c)\n",
                "y_pred_proba_c = xgb_clf.predict_proba(X_test_c)[:, 1]\n",
                "\n",
                "# Evaluate\n",
                "accuracy = accuracy_score(y_test_c, y_pred_c)\n",
                "roc_auc = roc_auc_score(y_test_c, y_pred_proba_c)\n",
                "\n",
                "print(f\"\\nüìä XGBoost Classification Performance:\")\n",
                "print(f\"Accuracy: {accuracy:.4f}\")\n",
                "print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
                "print(f\"\\nüìã Classification Report:\")\n",
                "print(classification_report(y_test_c, y_pred_c, target_names=cancer.target_names))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Confusion Matrix\n",
                "cm = confusion_matrix(y_test_c, y_pred_c)\n",
                "\n",
                "plt.figure(figsize=(8, 6))\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
                "            xticklabels=cancer.target_names,\n",
                "            yticklabels=cancer.target_names,\n",
                "            cbar_kws={'label': 'Count'})\n",
                "plt.xlabel('Predicted', fontsize=12)\n",
                "plt.ylabel('Actual', fontsize=12)\n",
                "plt.title('XGBoost Classification - Confusion Matrix', fontsize=14, fontweight='bold')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 7Ô∏è‚É£ Model Comparison: XGBoost vs Others"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
                "from sklearn.linear_model import LinearRegression\n",
                "\n",
                "# Define models\n",
                "models = {\n",
                "    'Linear Regression': LinearRegression(),\n",
                "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
                "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
                "    'XGBoost': XGBRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
                "}\n",
                "\n",
                "# Train and evaluate all models\n",
                "results_comparison = {}\n",
                "\n",
                "for name, model in models.items():\n",
                "    print(f\"Training {name}...\")\n",
                "    model.fit(X_train, y_train)\n",
                "    y_pred = model.predict(X_test)\n",
                "    \n",
                "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
                "    r2 = r2_score(y_test, y_pred)\n",
                "    \n",
                "    results_comparison[name] = {'RMSE': rmse, 'R2': r2}\n",
                "\n",
                "# Display results\n",
                "comparison_df = pd.DataFrame(results_comparison).T\n",
                "print(\"\\nüìä Model Comparison:\")\n",
                "print(comparison_df.sort_values('R2', ascending=False))\n",
                "\n",
                "# Visualize comparison\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
                "\n",
                "comparison_df['RMSE'].plot(kind='barh', ax=ax1, color='coral', edgecolor='black')\n",
                "ax1.set_xlabel('RMSE ($)', fontsize=12)\n",
                "ax1.set_title('Model Comparison - RMSE', fontsize=14, fontweight='bold')\n",
                "ax1.grid(True, alpha=0.3, axis='x')\n",
                "\n",
                "comparison_df['R2'].plot(kind='barh', ax=ax2, color='skyblue', edgecolor='black')\n",
                "ax2.set_xlabel('R¬≤ Score', fontsize=12)\n",
                "ax2.set_title('Model Comparison - R¬≤', fontsize=14, fontweight='bold')\n",
                "ax2.grid(True, alpha=0.3, axis='x')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìä Key Takeaways\n",
                "\n",
                "### XGBoost Advantages:\n",
                "‚úÖ **Superior Performance**: Often best for structured data  \n",
                "‚úÖ **Built-in Regularization**: Prevents overfitting  \n",
                "‚úÖ **Handles Missing Values**: No need for imputation  \n",
                "‚úÖ **Feature Importance**: Easy interpretation  \n",
                "‚úÖ **Parallel Processing**: Fast training  \n",
                "‚úÖ **Early Stopping**: Automatic optimization  \n",
                "\n",
                "### Best Practices:\n",
                "1. **Start with default parameters**, then tune\n",
                "2. **Use early stopping** to prevent overfitting\n",
                "3. **Monitor learning curves** for convergence\n",
                "4. **Scale features** (optional but can help)\n",
                "5. **Use cross-validation** for robust evaluation\n",
                "\n",
                "### Hyperparameter Tuning Strategy:\n",
                "1. **Fix learning_rate** at 0.1\n",
                "2. **Tune tree parameters**: max_depth, min_child_weight\n",
                "3. **Tune regularization**: reg_alpha, reg_lambda\n",
                "4. **Tune sampling**: subsample, colsample_bytree\n",
                "5. **Lower learning_rate** and increase n_estimators\n",
                "\n",
                "### When to Use XGBoost:\n",
                "‚úÖ Structured/tabular data  \n",
                "‚úÖ Medium to large datasets  \n",
                "‚úÖ Need for interpretability  \n",
                "‚úÖ Kaggle competitions  \n",
                "‚úÖ Production systems (fast inference)  \n",
                "\n",
                "### Common Pitfalls:\n",
                "‚ùå Over-tuning on test set  \n",
                "‚ùå Not using early stopping  \n",
                "‚ùå Ignoring feature engineering  \n",
                "‚ùå Too many boosting rounds  \n",
                "‚ùå Not monitoring overfitting  \n",
                "\n",
                "### Next Steps:\n",
                "1. Try **LightGBM** for comparison\n",
                "2. Implement **custom objectives**\n",
                "3. Use **SHAP** for model interpretation\n",
                "4. Deploy model to production\n",
                "5. Monitor model performance over time"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "ml_env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.19"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}