{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# LightGBM - Complete Guide\n",
                "\n",
                "## ðŸ“š Learning Objectives\n",
                "- Understand LightGBM and its advantages over XGBoost\n",
                "- Implement LightGBM for regression and classification\n",
                "- Master categorical feature handling\n",
                "- Optimize training speed and memory usage\n",
                "- Compare LightGBM with XGBoost\n",
                "\n",
                "## ðŸŽ¯ What is LightGBM?\n",
                "\n",
                "**LightGBM (Light Gradient Boosting Machine)** is a gradient boosting framework developed by Microsoft that uses tree-based learning algorithms.\n",
                "\n",
                "### Key Innovations:\n",
                "1. **Leaf-wise Growth**: Grows trees leaf-wise (best-first) instead of level-wise\n",
                "2. **Histogram-based Algorithm**: Bins continuous features for faster training\n",
                "3. **Native Categorical Support**: No need for one-hot encoding\n",
                "4. **GOSS (Gradient-based One-Side Sampling)**: Faster training\n",
                "5. **EFB (Exclusive Feature Bundling)**: Reduces dimensionality\n",
                "\n",
                "### LightGBM vs XGBoost:\n",
                "\n",
                "| Feature | LightGBM | XGBoost |\n",
                "|---------|----------|----------|\n",
                "| **Speed** | âš¡ Faster | Slower |\n",
                "| **Memory** | ðŸ’¾ Lower | Higher |\n",
                "| **Accuracy** | Similar | Similar |\n",
                "| **Categorical** | âœ… Native | âŒ Needs encoding |\n",
                "| **Tree Growth** | Leaf-wise | Level-wise |\n",
                "| **Large Datasets** | âœ… Better | Good |\n",
                "\n",
                "### When to Use LightGBM:\n",
                "âœ… Large datasets (>10K rows)  \n",
                "âœ… Many categorical features  \n",
                "âœ… Need for speed  \n",
                "âœ… Memory constraints  \n",
                "âœ… High-dimensional data  "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install LightGBM if not already installed\n",
                "# !pip install lightgbm\n",
                "\n",
                "# Import libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import lightgbm as lgb\n",
                "from lightgbm import LGBMRegressor, LGBMClassifier\n",
                "from sklearn.model_selection import train_test_split, cross_val_score\n",
                "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
                "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
                "from sklearn.datasets import load_breast_cancer\n",
                "import time\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Set style\n",
                "sns.set_style('whitegrid')\n",
                "plt.rcParams['figure.figsize'] = (12, 6)\n",
                "\n",
                "print(f\"LightGBM version: {lgb.__version__}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 1: LightGBM for Regression\n",
                "### 1ï¸âƒ£ Load and Prepare Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load California Housing dataset\n",
                "df = pd.read_csv('../../Linear Regression/data/dataset.csv')\n",
                "\n",
                "print(f\"Dataset shape: {df.shape}\")\n",
                "print(f\"\\nColumns: {list(df.columns)}\")\n",
                "print(f\"\\nData types:\")\n",
                "print(df.dtypes)\n",
                "\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare features and target\n",
                "target_col = 'median_house_value'\n",
                "X = df.drop(columns=[target_col])\n",
                "y = df[target_col]\n",
                "\n",
                "# LightGBM can handle categorical features natively!\n",
                "# Mark categorical columns\n",
                "categorical_features = ['ocean_proximity'] if 'ocean_proximity' in X.columns else []\n",
                "\n",
                "# Convert to category dtype for LightGBM\n",
                "for col in categorical_features:\n",
                "    X[col] = X[col].astype('category')\n",
                "\n",
                "print(f\"\\nCategorical features: {categorical_features}\")\n",
                "print(f\"Features shape: {X.shape}\")\n",
                "print(f\"Target shape: {y.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train-test split\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, test_size=0.2, random_state=42\n",
                ")\n",
                "\n",
                "print(f\"Training set: {X_train.shape[0]} samples\")\n",
                "print(f\"Test set: {X_test.shape[0]} samples\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2ï¸âƒ£ Basic LightGBM Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create and train basic LightGBM model\n",
                "lgbm_basic = LGBMRegressor(\n",
                "    n_estimators=100,\n",
                "    learning_rate=0.1,\n",
                "    max_depth=5,\n",
                "    num_leaves=31,  # LightGBM specific parameter\n",
                "    random_state=42,\n",
                "    n_jobs=-1,\n",
                "    verbose=-1  # Suppress warnings\n",
                ")\n",
                "\n",
                "# Measure training time\n",
                "print(\"Training LightGBM model...\")\n",
                "start_time = time.time()\n",
                "\n",
                "lgbm_basic.fit(\n",
                "    X_train, y_train,\n",
                "    categorical_feature=categorical_features  # Specify categorical features\n",
                ")\n",
                "\n",
                "training_time = time.time() - start_time\n",
                "print(f\"âœ… Training complete in {training_time:.2f} seconds!\")\n",
                "\n",
                "# Make predictions\n",
                "y_train_pred = lgbm_basic.predict(X_train)\n",
                "y_test_pred = lgbm_basic.predict(X_test)\n",
                "\n",
                "# Evaluate\n",
                "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
                "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
                "train_r2 = r2_score(y_train, y_train_pred)\n",
                "test_r2 = r2_score(y_test, y_test_pred)\n",
                "\n",
                "print(f\"\\nðŸ“Š Basic LightGBM Performance:\")\n",
                "print(f\"\\nTraining Metrics:\")\n",
                "print(f\"  RMSE: ${train_rmse:,.2f}\")\n",
                "print(f\"  RÂ²:   {train_r2:.4f}\")\n",
                "print(f\"\\nTest Metrics:\")\n",
                "print(f\"  RMSE: ${test_rmse:,.2f}\")\n",
                "print(f\"  RÂ²:   {test_r2:.4f}\")\n",
                "print(f\"\\nOverfitting Check: {train_r2 - test_r2:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3ï¸âƒ£ Feature Importance Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get feature importance\n",
                "feature_importance = pd.DataFrame({\n",
                "    'feature': X.columns,\n",
                "    'importance': lgbm_basic.feature_importances_\n",
                "}).sort_values('importance', ascending=False)\n",
                "\n",
                "print(\"\\nðŸ” Feature Importance:\")\n",
                "print(feature_importance)\n",
                "\n",
                "# Visualize feature importance\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
                "\n",
                "# Bar plot\n",
                "feature_importance.plot(x='feature', y='importance', kind='barh', ax=ax1, \n",
                "                       color='lightgreen', edgecolor='black')\n",
                "ax1.set_xlabel('Importance Score', fontsize=12)\n",
                "ax1.set_ylabel('Features', fontsize=12)\n",
                "ax1.set_title('Feature Importance (Split)', fontsize=14, fontweight='bold')\n",
                "ax1.grid(True, alpha=0.3, axis='x')\n",
                "\n",
                "# Built-in LightGBM plot\n",
                "lgb.plot_importance(lgbm_basic, ax=ax2, importance_type='gain', max_num_features=10)\n",
                "ax2.set_title('Feature Importance (Gain)', fontsize=14, fontweight='bold')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4ï¸âƒ£ Hyperparameter Tuning\n",
                "\n",
                "#### Key LightGBM Parameters:\n",
                "\n",
                "**Core Parameters:**\n",
                "- `num_leaves`: Max number of leaves (default: 31)\n",
                "- `max_depth`: Maximum tree depth (-1 = no limit)\n",
                "- `learning_rate`: Step size shrinkage (0.01-0.3)\n",
                "- `n_estimators`: Number of boosting rounds\n",
                "\n",
                "**Learning Control:**\n",
                "- `min_child_samples`: Minimum data in a leaf (default: 20)\n",
                "- `subsample`: Fraction of data for training (0.5-1.0)\n",
                "- `colsample_bytree`: Fraction of features (0.5-1.0)\n",
                "\n",
                "**Regularization:**\n",
                "- `reg_alpha`: L1 regularization\n",
                "- `reg_lambda`: L2 regularization\n",
                "- `min_split_gain`: Minimum gain to split"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.model_selection import RandomizedSearchCV\n",
                "\n",
                "# Define parameter grid\n",
                "param_grid = {\n",
                "    'num_leaves': [15, 31, 63],\n",
                "    'max_depth': [5, 7, 10, -1],\n",
                "    'learning_rate': [0.01, 0.05, 0.1],\n",
                "    'n_estimators': [100, 200, 300],\n",
                "    'subsample': [0.7, 0.8, 1.0],\n",
                "    'colsample_bytree': [0.7, 0.8, 1.0],\n",
                "    'reg_alpha': [0, 0.1, 0.5],\n",
                "    'reg_lambda': [0, 0.1, 0.5],\n",
                "    'min_child_samples': [10, 20, 30]\n",
                "}\n",
                "\n",
                "print(\"ðŸ” Starting hyperparameter tuning...\")\n",
                "print(\"This may take a few minutes...\\n\")\n",
                "\n",
                "lgbm_random = RandomizedSearchCV(\n",
                "    LGBMRegressor(random_state=42, n_jobs=-1, verbose=-1),\n",
                "    param_distributions=param_grid,\n",
                "    n_iter=20,\n",
                "    cv=3,\n",
                "    scoring='neg_mean_squared_error',\n",
                "    random_state=42,\n",
                "    n_jobs=-1,\n",
                "    verbose=1\n",
                ")\n",
                "\n",
                "lgbm_random.fit(X_train, y_train)\n",
                "\n",
                "print(f\"\\nðŸ† Best Parameters: {lgbm_random.best_params_}\")\n",
                "print(f\"Best CV Score (RMSE): ${np.sqrt(-lgbm_random.best_score_):,.2f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate tuned model\n",
                "best_lgbm = lgbm_random.best_estimator_\n",
                "\n",
                "y_train_pred_tuned = best_lgbm.predict(X_train)\n",
                "y_test_pred_tuned = best_lgbm.predict(X_test)\n",
                "\n",
                "train_rmse_tuned = np.sqrt(mean_squared_error(y_train, y_train_pred_tuned))\n",
                "test_rmse_tuned = np.sqrt(mean_squared_error(y_test, y_test_pred_tuned))\n",
                "train_r2_tuned = r2_score(y_train, y_train_pred_tuned)\n",
                "test_r2_tuned = r2_score(y_test, y_test_pred_tuned)\n",
                "\n",
                "print(f\"\\nðŸ“Š Tuned LightGBM Performance:\")\n",
                "print(f\"\\nTraining Metrics:\")\n",
                "print(f\"  RMSE: ${train_rmse_tuned:,.2f}\")\n",
                "print(f\"  RÂ²:   {train_r2_tuned:.4f}\")\n",
                "print(f\"\\nTest Metrics:\")\n",
                "print(f\"  RMSE: ${test_rmse_tuned:,.2f}\")\n",
                "print(f\"  RÂ²:   {test_r2_tuned:.4f}\")\n",
                "\n",
                "# Compare with basic model\n",
                "print(f\"\\nðŸ“ˆ Improvement:\")\n",
                "print(f\"  RMSE: ${test_rmse - test_rmse_tuned:,.2f} better\")\n",
                "print(f\"  RÂ²:   {test_r2_tuned - test_r2:.4f} better\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5ï¸âƒ£ Early Stopping and Callbacks"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train with early stopping using callbacks\n",
                "lgbm_early = LGBMRegressor(\n",
                "    n_estimators=1000,\n",
                "    learning_rate=0.05,\n",
                "    num_leaves=31,\n",
                "    random_state=42,\n",
                "    n_jobs=-1,\n",
                "    verbose=-1\n",
                ")\n",
                "\n",
                "# Create evaluation set\n",
                "eval_set = [(X_train, y_train), (X_test, y_test)]\n",
                "\n",
                "# Fit with callbacks\n",
                "lgbm_early.fit(\n",
                "    X_train, y_train,\n",
                "    eval_set=eval_set,\n",
                "    eval_metric='rmse',\n",
                "    callbacks=[\n",
                "        lgb.early_stopping(stopping_rounds=50),\n",
                "        lgb.log_evaluation(period=100)\n",
                "    ]\n",
                ")\n",
                "\n",
                "print(f\"\\nBest iteration: {lgbm_early.best_iteration_}\")\n",
                "print(f\"Best score: {lgbm_early.best_score_}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 2: LightGBM for Classification\n",
                "### 6ï¸âƒ£ Binary Classification Example"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load breast cancer dataset\n",
                "cancer = load_breast_cancer()\n",
                "X_cancer = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
                "y_cancer = pd.Series(cancer.target, name='diagnosis')\n",
                "\n",
                "print(f\"Classification dataset shape: {X_cancer.shape}\")\n",
                "print(f\"\\nClass distribution:\")\n",
                "print(y_cancer.value_counts())\n",
                "\n",
                "# Split data\n",
                "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n",
                "    X_cancer, y_cancer, test_size=0.2, random_state=42, stratify=y_cancer\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train LightGBM classifier\n",
                "lgbm_clf = LGBMClassifier(\n",
                "    n_estimators=100,\n",
                "    learning_rate=0.1,\n",
                "    num_leaves=31,\n",
                "    random_state=42,\n",
                "    n_jobs=-1,\n",
                "    verbose=-1\n",
                ")\n",
                "\n",
                "lgbm_clf.fit(X_train_c, y_train_c)\n",
                "\n",
                "# Predictions\n",
                "y_pred_c = lgbm_clf.predict(X_test_c)\n",
                "y_pred_proba_c = lgbm_clf.predict_proba(X_test_c)[:, 1]\n",
                "\n",
                "# Evaluate\n",
                "accuracy = accuracy_score(y_test_c, y_pred_c)\n",
                "roc_auc = roc_auc_score(y_test_c, y_pred_proba_c)\n",
                "\n",
                "print(f\"\\nðŸ“Š LightGBM Classification Performance:\")\n",
                "print(f\"Accuracy: {accuracy:.4f}\")\n",
                "print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
                "print(f\"\\nðŸ“‹ Classification Report:\")\n",
                "print(classification_report(y_test_c, y_pred_c, target_names=cancer.target_names))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 7ï¸âƒ£ LightGBM vs XGBoost Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import XGBoost for comparison\n",
                "from xgboost import XGBRegressor\n",
                "\n",
                "# Prepare data for fair comparison\n",
                "X_encoded = X.copy()\n",
                "if 'ocean_proximity' in X_encoded.columns:\n",
                "    from sklearn.preprocessing import LabelEncoder\n",
                "    le = LabelEncoder()\n",
                "    X_encoded['ocean_proximity'] = le.fit_transform(X_encoded['ocean_proximity'])\n",
                "\n",
                "X_train_enc, X_test_enc, y_train_enc, y_test_enc = train_test_split(\n",
                "    X_encoded, y, test_size=0.2, random_state=42\n",
                ")\n",
                "\n",
                "# Define models with similar parameters\n",
                "models = {\n",
                "    'LightGBM': LGBMRegressor(\n",
                "        n_estimators=100,\n",
                "        learning_rate=0.1,\n",
                "        max_depth=5,\n",
                "        random_state=42,\n",
                "        n_jobs=-1,\n",
                "        verbose=-1\n",
                "    ),\n",
                "    'XGBoost': XGBRegressor(\n",
                "        n_estimators=100,\n",
                "        learning_rate=0.1,\n",
                "        max_depth=5,\n",
                "        random_state=42,\n",
                "        n_jobs=-1\n",
                "    )\n",
                "}\n",
                "\n",
                "# Compare training time and performance\n",
                "comparison_results = {}\n",
                "\n",
                "for name, model in models.items():\n",
                "    print(f\"\\nTraining {name}...\")\n",
                "    \n",
                "    # Measure training time\n",
                "    start_time = time.time()\n",
                "    model.fit(X_train_enc, y_train_enc)\n",
                "    training_time = time.time() - start_time\n",
                "    \n",
                "    # Measure prediction time\n",
                "    start_time = time.time()\n",
                "    y_pred = model.predict(X_test_enc)\n",
                "    prediction_time = time.time() - start_time\n",
                "    \n",
                "    # Calculate metrics\n",
                "    rmse = np.sqrt(mean_squared_error(y_test_enc, y_pred))\n",
                "    r2 = r2_score(y_test_enc, y_pred)\n",
                "    \n",
                "    comparison_results[name] = {\n",
                "        'Training Time (s)': training_time,\n",
                "        'Prediction Time (s)': prediction_time,\n",
                "        'RMSE': rmse,\n",
                "        'RÂ²': r2\n",
                "    }\n",
                "    \n",
                "    print(f\"  Training time: {training_time:.2f}s\")\n",
                "    print(f\"  Prediction time: {prediction_time:.4f}s\")\n",
                "    print(f\"  RMSE: ${rmse:,.2f}\")\n",
                "    print(f\"  RÂ²: {r2:.4f}\")\n",
                "\n",
                "# Display comparison\n",
                "comparison_df = pd.DataFrame(comparison_results).T\n",
                "print(\"\\nðŸ“Š LightGBM vs XGBoost Comparison:\")\n",
                "print(comparison_df)\n",
                "\n",
                "# Visualize comparison\n",
                "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
                "\n",
                "# Training time\n",
                "comparison_df['Training Time (s)'].plot(kind='bar', ax=axes[0, 0], color=['lightgreen', 'orange'], edgecolor='black')\n",
                "axes[0, 0].set_title('Training Time Comparison', fontsize=14, fontweight='bold')\n",
                "axes[0, 0].set_ylabel('Time (seconds)')\n",
                "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
                "\n",
                "# Prediction time\n",
                "comparison_df['Prediction Time (s)'].plot(kind='bar', ax=axes[0, 1], color=['lightgreen', 'orange'], edgecolor='black')\n",
                "axes[0, 1].set_title('Prediction Time Comparison', fontsize=14, fontweight='bold')\n",
                "axes[0, 1].set_ylabel('Time (seconds)')\n",
                "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
                "\n",
                "# RMSE\n",
                "comparison_df['RMSE'].plot(kind='bar', ax=axes[1, 0], color=['lightgreen', 'orange'], edgecolor='black')\n",
                "axes[1, 0].set_title('RMSE Comparison', fontsize=14, fontweight='bold')\n",
                "axes[1, 0].set_ylabel('RMSE ($)')\n",
                "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
                "\n",
                "# RÂ²\n",
                "comparison_df['RÂ²'].plot(kind='bar', ax=axes[1, 1], color=['lightgreen', 'orange'], edgecolor='black')\n",
                "axes[1, 1].set_title('RÂ² Score Comparison', fontsize=14, fontweight='bold')\n",
                "axes[1, 1].set_ylabel('RÂ² Score')\n",
                "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ“Š Key Takeaways\n",
                "\n",
                "### LightGBM Advantages:\n",
                "âœ… **Faster Training**: 2-10x faster than XGBoost  \n",
                "âœ… **Lower Memory**: More memory efficient  \n",
                "âœ… **Native Categorical**: No encoding needed  \n",
                "âœ… **Better for Large Data**: Handles millions of rows  \n",
                "âœ… **Leaf-wise Growth**: Can achieve better accuracy  \n",
                "\n",
                "### Best Practices:\n",
                "1. **Use categorical features natively** - Don't one-hot encode\n",
                "2. **Start with num_leaves=31** - Good default\n",
                "3. **Use early stopping** - Prevents overfitting\n",
                "4. **Monitor max_depth** - Prevent too deep trees\n",
                "5. **Tune num_leaves and max_depth together**\n",
                "\n",
                "### Hyperparameter Tuning Strategy:\n",
                "1. **Fix learning_rate** at 0.1\n",
                "2. **Tune num_leaves** (15, 31, 63, 127)\n",
                "3. **Tune max_depth** (5, 7, 10, -1)\n",
                "4. **Tune min_child_samples** (10, 20, 30)\n",
                "5. **Add regularization** (reg_alpha, reg_lambda)\n",
                "6. **Lower learning_rate** and increase n_estimators\n",
                "\n",
                "### LightGBM vs XGBoost:\n",
                "\n",
                "**Choose LightGBM when:**\n",
                "- Large datasets (>100K rows)\n",
                "- Many categorical features\n",
                "- Speed is critical\n",
                "- Memory is limited\n",
                "\n",
                "**Choose XGBoost when:**\n",
                "- Small datasets (<10K rows)\n",
                "- Need maximum stability\n",
                "- More mature ecosystem needed\n",
                "\n",
                "### Common Pitfalls:\n",
                "âŒ **Overfitting with leaf-wise growth** - Monitor carefully  \n",
                "âŒ **Too many leaves** - Can overfit quickly  \n",
                "âŒ **Not using categorical features** - Missing key advantage  \n",
                "âŒ **Ignoring min_child_samples** - Important for regularization  \n",
                "\n",
                "### Advanced Features:\n",
                "1. **Custom objectives** - Define your own loss function\n",
                "2. **Dart mode** - Dropout for trees\n",
                "3. **GOSS** - Gradient-based sampling\n",
                "4. **Feature bundling** - Automatic dimensionality reduction\n",
                "\n",
                "### Next Steps:\n",
                "1. Try **CatBoost** for comparison\n",
                "2. Implement **custom metrics**\n",
                "3. Use **SHAP** for interpretability\n",
                "4. Optimize for **production deployment**\n",
                "5. Experiment with **dart** and **goss** modes"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "ml_env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.19"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}