{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# K-Nearest Neighbors (KNN) - Complete Guide\n",
                "\n",
                "## üìö Learning Objectives\n",
                "- Understand KNN algorithm for classification and regression\n",
                "- Learn how to choose optimal K value\n",
                "- Implement proper feature scaling\n",
                "- Handle distance metrics\n",
                "- Evaluate model performance\n",
                "\n",
                "## üéØ What is KNN?\n",
                "K-Nearest Neighbors is a **non-parametric**, **lazy learning** algorithm that:\n",
                "- Makes predictions based on K closest training examples\n",
                "- Uses distance metrics (usually Euclidean)\n",
                "- Works for both classification and regression\n",
                "\n",
                "### Key Concepts:\n",
                "1. **K**: Number of neighbors to consider\n",
                "2. **Distance Metric**: How to measure similarity (Euclidean, Manhattan, etc.)\n",
                "3. **Weights**: Uniform or distance-based\n",
                "4. **Feature Scaling**: CRITICAL for KNN!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
                "from sklearn.metrics import (\n",
                "    accuracy_score, classification_report, confusion_matrix,\n",
                "    mean_squared_error, r2_score, mean_absolute_error\n",
                ")\n",
                "from sklearn.datasets import load_iris, make_classification\n",
                "from sklearn.pipeline import Pipeline\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Set style\n",
                "sns.set_style('whitegrid')\n",
                "plt.rcParams['figure.figsize'] = (12, 6)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 1: KNN for Classification\n",
                "### 1Ô∏è‚É£ Load and Explore Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Iris dataset\n",
                "iris = load_iris()\n",
                "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
                "y = pd.Series(iris.target, name='species')\n",
                "\n",
                "print(f\"Dataset shape: {X.shape}\")\n",
                "print(f\"\\nFeatures: {list(X.columns)}\")\n",
                "print(f\"\\nTarget classes: {iris.target_names}\")\n",
                "print(f\"\\nClass distribution:\")\n",
                "print(y.value_counts().sort_index())\n",
                "\n",
                "# Display first few rows\n",
                "df = pd.concat([X, y], axis=1)\n",
                "df.head(10)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize feature distributions\n",
                "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
                "axes = axes.ravel()\n",
                "\n",
                "for idx, col in enumerate(X.columns):\n",
                "    for species in range(3):\n",
                "        axes[idx].hist(X[y == species][col], alpha=0.6, label=iris.target_names[species], bins=20)\n",
                "    axes[idx].set_xlabel(col, fontsize=12)\n",
                "    axes[idx].set_ylabel('Frequency')\n",
                "    axes[idx].legend()\n",
                "    axes[idx].grid(True, alpha=0.3)\n",
                "\n",
                "plt.suptitle('Feature Distributions by Species', fontsize=16, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2Ô∏è‚É£ Train-Test Split and Scaling\n",
                "**CRITICAL**: KNN is distance-based, so feature scaling is MANDATORY!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Split data\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, test_size=0.3, random_state=42, stratify=y\n",
                ")\n",
                "\n",
                "print(f\"Training set: {X_train.shape[0]} samples\")\n",
                "print(f\"Test set: {X_test.shape[0]} samples\")\n",
                "\n",
                "# Demonstrate importance of scaling\n",
                "print(\"\\nüìä Feature Ranges BEFORE Scaling:\")\n",
                "print(X_train.describe().loc[['min', 'max']])\n",
                "\n",
                "# Scale features\n",
                "scaler = StandardScaler()\n",
                "X_train_scaled = scaler.fit_transform(X_train)\n",
                "X_test_scaled = scaler.transform(X_test)\n",
                "\n",
                "print(\"\\nüìä Feature Ranges AFTER Scaling:\")\n",
                "print(pd.DataFrame(X_train_scaled, columns=X.columns).describe().loc[['min', 'max']])\n",
                "print(\"\\n‚úÖ All features now on similar scale!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3Ô∏è‚É£ Finding Optimal K Value"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test different K values\n",
                "k_range = range(1, 31)\n",
                "train_scores = []\n",
                "test_scores = []\n",
                "\n",
                "for k in k_range:\n",
                "    knn = KNeighborsClassifier(n_neighbors=k)\n",
                "    knn.fit(X_train_scaled, y_train)\n",
                "    \n",
                "    train_scores.append(knn.score(X_train_scaled, y_train))\n",
                "    test_scores.append(knn.score(X_test_scaled, y_test))\n",
                "\n",
                "# Plot results\n",
                "plt.figure(figsize=(12, 6))\n",
                "plt.plot(k_range, train_scores, 'bo-', label='Training Accuracy', linewidth=2)\n",
                "plt.plot(k_range, test_scores, 'ro-', label='Test Accuracy', linewidth=2)\n",
                "plt.xlabel('K (Number of Neighbors)', fontsize=12)\n",
                "plt.ylabel('Accuracy', fontsize=12)\n",
                "plt.title('KNN: Finding Optimal K Value', fontsize=14, fontweight='bold')\n",
                "plt.legend(fontsize=12)\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.xticks(range(1, 31, 2))\n",
                "\n",
                "# Highlight best K\n",
                "best_k = k_range[np.argmax(test_scores)]\n",
                "plt.axvline(x=best_k, color='g', linestyle='--', linewidth=2, label=f'Best K={best_k}')\n",
                "plt.legend()\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nüèÜ Optimal K: {best_k}\")\n",
                "print(f\"Test Accuracy at K={best_k}: {max(test_scores):.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4Ô∏è‚É£ Train Final Model with Optimal K"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train with optimal K\n",
                "knn_optimal = KNeighborsClassifier(n_neighbors=best_k)\n",
                "knn_optimal.fit(X_train_scaled, y_train)\n",
                "\n",
                "# Make predictions\n",
                "y_pred = knn_optimal.predict(X_test_scaled)\n",
                "\n",
                "# Evaluate\n",
                "accuracy = accuracy_score(y_test, y_pred)\n",
                "print(f\"\\nüìä Model Performance (K={best_k}):\")\n",
                "print(f\"Accuracy: {accuracy:.4f}\")\n",
                "print(f\"\\nüìã Classification Report:\")\n",
                "print(classification_report(y_test, y_pred, target_names=iris.target_names))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5Ô∏è‚É£ Confusion Matrix Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create confusion matrix\n",
                "cm = confusion_matrix(y_test, y_pred)\n",
                "\n",
                "plt.figure(figsize=(10, 8))\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
                "            xticklabels=iris.target_names, \n",
                "            yticklabels=iris.target_names,\n",
                "            cbar_kws={'label': 'Count'})\n",
                "plt.xlabel('Predicted Label', fontsize=12)\n",
                "plt.ylabel('True Label', fontsize=12)\n",
                "plt.title(f'Confusion Matrix - KNN (K={best_k})', fontsize=14, fontweight='bold')\n",
                "plt.show()\n",
                "\n",
                "# Calculate per-class accuracy\n",
                "print(\"\\nüìä Per-Class Accuracy:\")\n",
                "for i, species in enumerate(iris.target_names):\n",
                "    class_acc = cm[i, i] / cm[i, :].sum()\n",
                "    print(f\"{species}: {class_acc:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6Ô∏è‚É£ Comparing Distance Metrics and Weights"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test different configurations\n",
                "configs = [\n",
                "    {'metric': 'euclidean', 'weights': 'uniform'},\n",
                "    {'metric': 'euclidean', 'weights': 'distance'},\n",
                "    {'metric': 'manhattan', 'weights': 'uniform'},\n",
                "    {'metric': 'manhattan', 'weights': 'distance'},\n",
                "]\n",
                "\n",
                "results = []\n",
                "\n",
                "for config in configs:\n",
                "    knn = KNeighborsClassifier(n_neighbors=best_k, **config)\n",
                "    knn.fit(X_train_scaled, y_train)\n",
                "    score = knn.score(X_test_scaled, y_test)\n",
                "    results.append({\n",
                "        'Metric': config['metric'],\n",
                "        'Weights': config['weights'],\n",
                "        'Accuracy': score\n",
                "    })\n",
                "\n",
                "results_df = pd.DataFrame(results)\n",
                "print(\"\\nüìä Comparison of Distance Metrics and Weights:\")\n",
                "print(results_df.to_string(index=False))\n",
                "\n",
                "# Visualize\n",
                "plt.figure(figsize=(10, 6))\n",
                "x = np.arange(len(results_df))\n",
                "plt.bar(x, results_df['Accuracy'], color=['skyblue', 'lightcoral', 'lightgreen', 'gold'], \n",
                "        edgecolor='black', linewidth=1.5)\n",
                "plt.xticks(x, [f\"{row['Metric']}\\n{row['Weights']}\" for _, row in results_df.iterrows()])\n",
                "plt.ylabel('Accuracy', fontsize=12)\n",
                "plt.title('KNN Performance: Distance Metrics & Weights Comparison', fontsize=14, fontweight='bold')\n",
                "plt.ylim(0.9, 1.0)\n",
                "plt.grid(True, alpha=0.3, axis='y')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 2: KNN for Regression\n",
                "### 7Ô∏è‚É£ KNN Regression Example"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load housing data for regression\n",
                "df_housing = pd.read_csv('../Linear Regression/data/dataset.csv')\n",
                "\n",
                "# Select features for regression\n",
                "features = ['longitude', 'latitude', 'housing_median_age', 'total_rooms', \n",
                "            'total_bedrooms', 'population', 'households', 'median_income']\n",
                "target = 'median_house_value'\n",
                "\n",
                "# Prepare data\n",
                "X_reg = df_housing[features].dropna()\n",
                "y_reg = df_housing.loc[X_reg.index, target]\n",
                "\n",
                "# Sample for computational efficiency\n",
                "sample_size = 5000\n",
                "indices = np.random.choice(X_reg.index, sample_size, replace=False)\n",
                "X_reg = X_reg.loc[indices]\n",
                "y_reg = y_reg.loc[indices]\n",
                "\n",
                "print(f\"Regression dataset shape: {X_reg.shape}\")\n",
                "print(f\"Target variable: {target}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Split and scale\n",
                "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
                "    X_reg, y_reg, test_size=0.2, random_state=42\n",
                ")\n",
                "\n",
                "scaler_reg = StandardScaler()\n",
                "X_train_reg_scaled = scaler_reg.fit_transform(X_train_reg)\n",
                "X_test_reg_scaled = scaler_reg.transform(X_test_reg)\n",
                "\n",
                "# Find optimal K for regression\n",
                "k_range_reg = range(1, 21)\n",
                "rmse_scores = []\n",
                "\n",
                "for k in k_range_reg:\n",
                "    knn_reg = KNeighborsRegressor(n_neighbors=k)\n",
                "    knn_reg.fit(X_train_reg_scaled, y_train_reg)\n",
                "    y_pred_reg = knn_reg.predict(X_test_reg_scaled)\n",
                "    rmse = np.sqrt(mean_squared_error(y_test_reg, y_pred_reg))\n",
                "    rmse_scores.append(rmse)\n",
                "\n",
                "# Plot RMSE vs K\n",
                "plt.figure(figsize=(12, 6))\n",
                "plt.plot(k_range_reg, rmse_scores, 'bo-', linewidth=2, markersize=8)\n",
                "plt.xlabel('K (Number of Neighbors)', fontsize=12)\n",
                "plt.ylabel('RMSE ($)', fontsize=12)\n",
                "plt.title('KNN Regression: Finding Optimal K', fontsize=14, fontweight='bold')\n",
                "plt.grid(True, alpha=0.3)\n",
                "\n",
                "best_k_reg = k_range_reg[np.argmin(rmse_scores)]\n",
                "plt.axvline(x=best_k_reg, color='r', linestyle='--', linewidth=2, label=f'Best K={best_k_reg}')\n",
                "plt.legend()\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nüèÜ Optimal K for Regression: {best_k_reg}\")\n",
                "print(f\"Best RMSE: ${min(rmse_scores):,.2f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train final regression model\n",
                "knn_reg_final = KNeighborsRegressor(n_neighbors=best_k_reg)\n",
                "knn_reg_final.fit(X_train_reg_scaled, y_train_reg)\n",
                "\n",
                "# Predictions\n",
                "y_pred_reg_final = knn_reg_final.predict(X_test_reg_scaled)\n",
                "\n",
                "# Metrics\n",
                "rmse_final = np.sqrt(mean_squared_error(y_test_reg, y_pred_reg_final))\n",
                "mae_final = mean_absolute_error(y_test_reg, y_pred_reg_final)\n",
                "r2_final = r2_score(y_test_reg, y_pred_reg_final)\n",
                "\n",
                "print(f\"\\nüìä KNN Regression Performance (K={best_k_reg}):\")\n",
                "print(f\"RMSE: ${rmse_final:,.2f}\")\n",
                "print(f\"MAE:  ${mae_final:,.2f}\")\n",
                "print(f\"R¬≤:   {r2_final:.4f}\")\n",
                "\n",
                "# Visualize predictions\n",
                "plt.figure(figsize=(12, 6))\n",
                "plt.scatter(y_test_reg, y_pred_reg_final, alpha=0.5, edgecolors='k')\n",
                "plt.plot([y_test_reg.min(), y_test_reg.max()], \n",
                "         [y_test_reg.min(), y_test_reg.max()], \n",
                "         'r--', lw=2, label='Perfect Prediction')\n",
                "plt.xlabel('Actual House Value ($)', fontsize=12)\n",
                "plt.ylabel('Predicted House Value ($)', fontsize=12)\n",
                "plt.title(f'KNN Regression: Actual vs Predicted (K={best_k_reg})', fontsize=14, fontweight='bold')\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 8Ô∏è‚É£ Hyperparameter Tuning with GridSearchCV"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define parameter grid\n",
                "param_grid = {\n",
                "    'n_neighbors': [3, 5, 7, 9, 11, 15],\n",
                "    'weights': ['uniform', 'distance'],\n",
                "    'metric': ['euclidean', 'manhattan']\n",
                "}\n",
                "\n",
                "# Create KNN classifier\n",
                "knn_grid = KNeighborsClassifier()\n",
                "\n",
                "# Grid search\n",
                "grid_search = GridSearchCV(\n",
                "    knn_grid, \n",
                "    param_grid, \n",
                "    cv=5, \n",
                "    scoring='accuracy',\n",
                "    n_jobs=-1\n",
                ")\n",
                "\n",
                "print(\"üîç Performing Grid Search...\")\n",
                "grid_search.fit(X_train_scaled, y_train)\n",
                "\n",
                "print(f\"\\nüèÜ Best Parameters: {grid_search.best_params_}\")\n",
                "print(f\"Best Cross-Validation Score: {grid_search.best_score_:.4f}\")\n",
                "print(f\"Test Set Score: {grid_search.score(X_test_scaled, y_test):.4f}\")\n",
                "\n",
                "# Show top 10 configurations\n",
                "results_df = pd.DataFrame(grid_search.cv_results_)\n",
                "top_results = results_df.nsmallest(10, 'rank_test_score')[[\n",
                "    'param_n_neighbors', 'param_weights', 'param_metric', 'mean_test_score'\n",
                "]]\n",
                "print(\"\\nüìä Top 10 Configurations:\")\n",
                "print(top_results.to_string(index=False))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìä Key Takeaways\n",
                "\n",
                "### Algorithm Characteristics:\n",
                "1. **Non-parametric**: No assumptions about data distribution\n",
                "2. **Lazy learning**: No training phase, all computation at prediction time\n",
                "3. **Instance-based**: Uses entire training set for predictions\n",
                "\n",
                "### Critical Requirements:\n",
                "‚úÖ **Feature Scaling**: MANDATORY for KNN\n",
                "‚úÖ **Optimal K**: Use cross-validation to find best K\n",
                "‚úÖ **Distance Metric**: Choose based on data characteristics\n",
                "‚úÖ **Computational Cost**: Slow for large datasets\n",
                "\n",
                "### Choosing K:\n",
                "- **Small K** (1-3): More complex decision boundary, prone to overfitting\n",
                "- **Large K**: Smoother decision boundary, may underfit\n",
                "- **Rule of thumb**: K = ‚àön (where n = number of samples)\n",
                "- **Best practice**: Use cross-validation\n",
                "\n",
                "### Distance Metrics:\n",
                "- **Euclidean**: Most common, works well for continuous features\n",
                "- **Manhattan**: Better for high-dimensional data\n",
                "- **Minkowski**: Generalization of both (p=1: Manhattan, p=2: Euclidean)\n",
                "\n",
                "### Weights:\n",
                "- **Uniform**: All neighbors contribute equally\n",
                "- **Distance**: Closer neighbors have more influence\n",
                "\n",
                "### Pros:\n",
                "‚úÖ Simple to understand and implement\n",
                "‚úÖ No training required\n",
                "‚úÖ Works for both classification and regression\n",
                "‚úÖ Naturally handles multi-class problems\n",
                "\n",
                "### Cons:\n",
                "‚ùå Slow prediction for large datasets\n",
                "‚ùå Sensitive to irrelevant features\n",
                "‚ùå Requires feature scaling\n",
                "‚ùå Curse of dimensionality\n",
                "‚ùå Memory intensive\n",
                "\n",
                "### When to Use KNN:\n",
                "- ‚úÖ Small to medium datasets\n",
                "- ‚úÖ Low-dimensional data\n",
                "- ‚úÖ Non-linear decision boundaries\n",
                "- ‚ùå Large datasets (use approximate methods)\n",
                "- ‚ùå High-dimensional data (use dimensionality reduction first)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "ml_env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.19"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}