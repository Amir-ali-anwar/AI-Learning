{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Advanced Linear Regression with Sklearn\n",
                "\n",
                "## ðŸ“š Learning Objectives\n",
                "- Implement Linear Regression using proper pipelines\n",
                "- Perform comprehensive model evaluation\n",
                "- Analyze residuals and assumptions\n",
                "- Compare multiple regression techniques\n",
                "- Implement feature selection and engineering\n",
                "\n",
                "## ðŸŽ¯ What You'll Learn\n",
                "1. Proper train-test splitting\n",
                "2. Feature preprocessing with pipelines\n",
                "3. Model training and evaluation\n",
                "4. Residual analysis\n",
                "5. Feature importance\n",
                "6. Model comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.impute import SimpleImputer\n",
                "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
                "from sklearn.compose import ColumnTransformer\n",
                "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
                "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
                "from scipy import stats\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Set visualization style\n",
                "sns.set_style('whitegrid')\n",
                "plt.rcParams['figure.figsize'] = (12, 6)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1ï¸âƒ£ Load and Explore Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load dataset\n",
                "df = pd.read_csv('data/dataset.csv')\n",
                "\n",
                "print(f\"Dataset Shape: {df.shape}\")\n",
                "print(f\"\\nColumn Names and Types:\")\n",
                "print(df.dtypes)\n",
                "print(f\"\\nMissing Values:\")\n",
                "print(df.isnull().sum())\n",
                "print(f\"\\nBasic Statistics:\")\n",
                "df.describe()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Separate features and target\n",
                "target_col = 'median_house_value'\n",
                "X = df.drop(columns=[target_col])\n",
                "y = df[target_col]\n",
                "\n",
                "print(f\"Features shape: {X.shape}\")\n",
                "print(f\"Target shape: {y.shape}\")\n",
                "print(f\"\\nTarget statistics:\")\n",
                "print(y.describe())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2ï¸âƒ£ Train-Test Split\n",
                "**CRITICAL**: Always split BEFORE any preprocessing to prevent data leakage!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Split data (80-20 split)\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, \n",
                "    test_size=0.2, \n",
                "    random_state=42\n",
                ")\n",
                "\n",
                "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
                "print(f\"Test set size: {X_test.shape[0]} samples\")\n",
                "print(f\"\\nTraining set percentage: {X_train.shape[0]/df.shape[0]*100:.1f}%\")\n",
                "print(f\"Test set percentage: {X_test.shape[0]/df.shape[0]*100:.1f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3ï¸âƒ£ Build Preprocessing Pipeline\n",
                "Using pipelines ensures:\n",
                "- No data leakage\n",
                "- Reproducible preprocessing\n",
                "- Easy deployment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Identify column types\n",
                "num_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
                "cat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
                "\n",
                "print(f\"Numerical columns ({len(num_cols)}): {num_cols}\")\n",
                "print(f\"Categorical columns ({len(cat_cols)}): {cat_cols}\")\n",
                "\n",
                "# Numerical pipeline: Impute median -> Scale\n",
                "numeric_transformer = Pipeline(steps=[\n",
                "    ('imputer', SimpleImputer(strategy='median')),\n",
                "    ('scaler', StandardScaler())\n",
                "])\n",
                "\n",
                "# Categorical pipeline: Impute 'missing' -> OneHotEncode\n",
                "categorical_transformer = Pipeline(steps=[\n",
                "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
                "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
                "])\n",
                "\n",
                "# Combine transformers\n",
                "preprocessor = ColumnTransformer(\n",
                "    transformers=[\n",
                "        ('num', numeric_transformer, num_cols),\n",
                "        ('cat', categorical_transformer, cat_cols)\n",
                "    ])\n",
                "\n",
                "print(\"\\nâœ… Preprocessing pipeline created successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4ï¸âƒ£ Train Multiple Models\n",
                "Let's compare different regression techniques"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define models to compare\n",
                "models = {\n",
                "    'Linear Regression': LinearRegression(),\n",
                "    'Ridge (L2)': Ridge(alpha=1.0),\n",
                "    'Lasso (L1)': Lasso(alpha=1.0),\n",
                "    'ElasticNet': ElasticNet(alpha=1.0, l1_ratio=0.5)\n",
                "}\n",
                "\n",
                "# Store results\n",
                "results = {}\n",
                "trained_pipelines = {}\n",
                "\n",
                "# Train and evaluate each model\n",
                "for name, model in models.items():\n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"Training: {name}\")\n",
                "    print(f\"{'='*60}\")\n",
                "    \n",
                "    # Create pipeline\n",
                "    pipeline = Pipeline(steps=[\n",
                "        ('preprocessor', preprocessor),\n",
                "        ('regressor', model)\n",
                "    ])\n",
                "    \n",
                "    # Train model\n",
                "    pipeline.fit(X_train, y_train)\n",
                "    \n",
                "    # Make predictions\n",
                "    y_train_pred = pipeline.predict(X_train)\n",
                "    y_test_pred = pipeline.predict(X_test)\n",
                "    \n",
                "    # Calculate metrics\n",
                "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
                "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
                "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
                "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
                "    train_r2 = r2_score(y_train, y_train_pred)\n",
                "    test_r2 = r2_score(y_test, y_test_pred)\n",
                "    \n",
                "    # Store results\n",
                "    results[name] = {\n",
                "        'train_rmse': train_rmse,\n",
                "        'test_rmse': test_rmse,\n",
                "        'train_mae': train_mae,\n",
                "        'test_mae': test_mae,\n",
                "        'train_r2': train_r2,\n",
                "        'test_r2': test_r2,\n",
                "        'predictions': y_test_pred\n",
                "    }\n",
                "    trained_pipelines[name] = pipeline\n",
                "    \n",
                "    # Print results\n",
                "    print(f\"\\nTraining Metrics:\")\n",
                "    print(f\"  RMSE: ${train_rmse:,.2f}\")\n",
                "    print(f\"  MAE:  ${train_mae:,.2f}\")\n",
                "    print(f\"  RÂ²:   {train_r2:.4f}\")\n",
                "    \n",
                "    print(f\"\\nTest Metrics:\")\n",
                "    print(f\"  RMSE: ${test_rmse:,.2f}\")\n",
                "    print(f\"  MAE:  ${test_mae:,.2f}\")\n",
                "    print(f\"  RÂ²:   {test_r2:.4f}\")\n",
                "    \n",
                "    # Check for overfitting\n",
                "    overfit_score = train_r2 - test_r2\n",
                "    if overfit_score > 0.1:\n",
                "        print(f\"\\nâš ï¸  Warning: Possible overfitting detected (gap: {overfit_score:.4f})\")\n",
                "    else:\n",
                "        print(f\"\\nâœ… Good generalization (gap: {overfit_score:.4f})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5ï¸âƒ£ Model Comparison Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create comparison dataframe\n",
                "comparison_df = pd.DataFrame(results).T\n",
                "comparison_df = comparison_df.drop('predictions', axis=1)\n",
                "\n",
                "print(\"\\nðŸ“Š Model Comparison Summary:\")\n",
                "print(comparison_df.round(2))\n",
                "\n",
                "# Visualize comparison\n",
                "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
                "\n",
                "# RMSE comparison\n",
                "comparison_df[['train_rmse', 'test_rmse']].plot(kind='bar', ax=axes[0], color=['skyblue', 'coral'])\n",
                "axes[0].set_title('RMSE Comparison', fontsize=14, fontweight='bold')\n",
                "axes[0].set_ylabel('RMSE ($)')\n",
                "axes[0].set_xlabel('Model')\n",
                "axes[0].legend(['Training', 'Test'])\n",
                "axes[0].tick_params(axis='x', rotation=45)\n",
                "\n",
                "# MAE comparison\n",
                "comparison_df[['train_mae', 'test_mae']].plot(kind='bar', ax=axes[1], color=['lightgreen', 'salmon'])\n",
                "axes[1].set_title('MAE Comparison', fontsize=14, fontweight='bold')\n",
                "axes[1].set_ylabel('MAE ($)')\n",
                "axes[1].set_xlabel('Model')\n",
                "axes[1].legend(['Training', 'Test'])\n",
                "axes[1].tick_params(axis='x', rotation=45)\n",
                "\n",
                "# RÂ² comparison\n",
                "comparison_df[['train_r2', 'test_r2']].plot(kind='bar', ax=axes[2], color=['gold', 'purple'])\n",
                "axes[2].set_title('RÂ² Score Comparison', fontsize=14, fontweight='bold')\n",
                "axes[2].set_ylabel('RÂ² Score')\n",
                "axes[2].set_xlabel('Model')\n",
                "axes[2].legend(['Training', 'Test'])\n",
                "axes[2].tick_params(axis='x', rotation=45)\n",
                "axes[2].axhline(y=0, color='r', linestyle='--', alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6ï¸âƒ£ Residual Analysis\n",
                "Analyzing residuals helps us validate regression assumptions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Select best model for detailed analysis\n",
                "best_model_name = max(results, key=lambda x: results[x]['test_r2'])\n",
                "best_pipeline = trained_pipelines[best_model_name]\n",
                "y_pred = results[best_model_name]['predictions']\n",
                "\n",
                "print(f\"\\nðŸ† Best Model: {best_model_name}\")\n",
                "print(f\"Test RÂ² Score: {results[best_model_name]['test_r2']:.4f}\")\n",
                "\n",
                "# Calculate residuals\n",
                "residuals = y_test - y_pred\n",
                "\n",
                "# Create residual plots\n",
                "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
                "\n",
                "# 1. Residuals vs Predicted Values\n",
                "axes[0, 0].scatter(y_pred, residuals, alpha=0.5, edgecolors='k')\n",
                "axes[0, 0].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
                "axes[0, 0].set_xlabel('Predicted Values', fontsize=12)\n",
                "axes[0, 0].set_ylabel('Residuals', fontsize=12)\n",
                "axes[0, 0].set_title('Residual Plot', fontsize=14, fontweight='bold')\n",
                "axes[0, 0].grid(True, alpha=0.3)\n",
                "\n",
                "# 2. Histogram of Residuals\n",
                "axes[0, 1].hist(residuals, bins=50, edgecolor='black', alpha=0.7)\n",
                "axes[0, 1].set_xlabel('Residuals', fontsize=12)\n",
                "axes[0, 1].set_ylabel('Frequency', fontsize=12)\n",
                "axes[0, 1].set_title('Distribution of Residuals', fontsize=14, fontweight='bold')\n",
                "axes[0, 1].axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
                "axes[0, 1].grid(True, alpha=0.3)\n",
                "\n",
                "# 3. Q-Q Plot\n",
                "stats.probplot(residuals, dist=\"norm\", plot=axes[1, 0])\n",
                "axes[1, 0].set_title('Q-Q Plot', fontsize=14, fontweight='bold')\n",
                "axes[1, 0].grid(True, alpha=0.3)\n",
                "\n",
                "# 4. Actual vs Predicted\n",
                "axes[1, 1].scatter(y_test, y_pred, alpha=0.5, edgecolors='k')\n",
                "axes[1, 1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
                "                'r--', lw=2, label='Perfect Prediction')\n",
                "axes[1, 1].set_xlabel('Actual Values', fontsize=12)\n",
                "axes[1, 1].set_ylabel('Predicted Values', fontsize=12)\n",
                "axes[1, 1].set_title('Actual vs Predicted', fontsize=14, fontweight='bold')\n",
                "axes[1, 1].legend()\n",
                "axes[1, 1].grid(True, alpha=0.3)\n",
                "\n",
                "plt.suptitle(f'Residual Analysis - {best_model_name}', fontsize=16, fontweight='bold', y=1.02)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Statistical tests\n",
                "print(\"\\nðŸ“ˆ Residual Statistics:\")\n",
                "print(f\"Mean of residuals: {residuals.mean():.2f} (should be close to 0)\")\n",
                "print(f\"Std of residuals: {residuals.std():.2f}\")\n",
                "print(f\"Min residual: ${residuals.min():,.2f}\")\n",
                "print(f\"Max residual: ${residuals.max():,.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7ï¸âƒ£ Cross-Validation\n",
                "More robust evaluation using k-fold cross-validation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Perform cross-validation for best model\n",
                "cv_results = cross_validate(\n",
                "    best_pipeline,\n",
                "    X_train, y_train,\n",
                "    cv=5,\n",
                "    scoring=['neg_mean_squared_error', 'neg_mean_absolute_error', 'r2'],\n",
                "    return_train_score=True\n",
                ")\n",
                "\n",
                "# Convert to RMSE\n",
                "cv_rmse = np.sqrt(-cv_results['test_neg_mean_squared_error'])\n",
                "cv_mae = -cv_results['test_neg_mean_absolute_error']\n",
                "cv_r2 = cv_results['test_r2']\n",
                "\n",
                "print(f\"\\nðŸ”„ 5-Fold Cross-Validation Results for {best_model_name}:\")\n",
                "print(f\"\\nRMSE per fold: {cv_rmse}\")\n",
                "print(f\"Average RMSE: ${cv_rmse.mean():,.2f} (+/- ${cv_rmse.std() * 2:,.2f})\")\n",
                "print(f\"\\nMAE per fold: {cv_mae}\")\n",
                "print(f\"Average MAE: ${cv_mae.mean():,.2f} (+/- ${cv_mae.std() * 2:,.2f})\")\n",
                "print(f\"\\nRÂ² per fold: {cv_r2}\")\n",
                "print(f\"Average RÂ²: {cv_r2.mean():.4f} (+/- {cv_r2.std() * 2:.4f})\")\n",
                "\n",
                "# Visualize CV results\n",
                "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
                "\n",
                "axes[0].boxplot(cv_rmse)\n",
                "axes[0].set_title('Cross-Validation RMSE', fontsize=14, fontweight='bold')\n",
                "axes[0].set_ylabel('RMSE ($)')\n",
                "axes[0].grid(True, alpha=0.3)\n",
                "\n",
                "axes[1].boxplot(cv_mae)\n",
                "axes[1].set_title('Cross-Validation MAE', fontsize=14, fontweight='bold')\n",
                "axes[1].set_ylabel('MAE ($)')\n",
                "axes[1].grid(True, alpha=0.3)\n",
                "\n",
                "axes[2].boxplot(cv_r2)\n",
                "axes[2].set_title('Cross-Validation RÂ²', fontsize=14, fontweight='bold')\n",
                "axes[2].set_ylabel('RÂ² Score')\n",
                "axes[2].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8ï¸âƒ£ Feature Importance Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract feature names after preprocessing\n",
                "feature_names = num_cols.copy()\n",
                "\n",
                "# Add one-hot encoded feature names\n",
                "if len(cat_cols) > 0:\n",
                "    ohe = best_pipeline.named_steps['preprocessor'].named_transformers_['cat']['onehot']\n",
                "    cat_feature_names = ohe.get_feature_names_out(cat_cols)\n",
                "    feature_names.extend(cat_feature_names)\n",
                "\n",
                "# Get coefficients\n",
                "coefficients = best_pipeline.named_steps['regressor'].coef_\n",
                "\n",
                "# Create feature importance dataframe\n",
                "feature_importance = pd.DataFrame({\n",
                "    'feature': feature_names,\n",
                "    'coefficient': coefficients,\n",
                "    'abs_coefficient': np.abs(coefficients)\n",
                "}).sort_values('abs_coefficient', ascending=False)\n",
                "\n",
                "print(\"\\nðŸ” Top 15 Most Important Features:\")\n",
                "print(feature_importance.head(15))\n",
                "\n",
                "# Visualize top features\n",
                "top_n = 15\n",
                "top_features = feature_importance.head(top_n)\n",
                "\n",
                "plt.figure(figsize=(12, 8))\n",
                "colors = ['green' if x > 0 else 'red' for x in top_features['coefficient']]\n",
                "plt.barh(range(top_n), top_features['coefficient'], color=colors, edgecolor='black')\n",
                "plt.yticks(range(top_n), top_features['feature'])\n",
                "plt.xlabel('Coefficient Value', fontsize=12)\n",
                "plt.title(f'Top {top_n} Feature Importances - {best_model_name}', fontsize=14, fontweight='bold')\n",
                "plt.axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
                "plt.grid(True, alpha=0.3, axis='x')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ“Š Key Takeaways\n",
                "\n",
                "### Model Performance:\n",
                "1. **Best Model**: Check the comparison above\n",
                "2. **Generalization**: Look at train vs test scores\n",
                "3. **Stability**: Cross-validation shows consistency\n",
                "\n",
                "### Assumptions Check:\n",
                "1. **Linearity**: Residual plot should show random scatter\n",
                "2. **Normality**: Q-Q plot should be roughly linear\n",
                "3. **Homoscedasticity**: Residuals should have constant variance\n",
                "4. **Independence**: No patterns in residual plot\n",
                "\n",
                "### Best Practices Applied:\n",
                "âœ… Proper train-test split BEFORE preprocessing\n",
                "âœ… Used pipelines to prevent data leakage\n",
                "âœ… Multiple model comparison\n",
                "âœ… Cross-validation for robust evaluation\n",
                "âœ… Residual analysis for assumption validation\n",
                "âœ… Feature importance analysis\n",
                "\n",
                "### Next Steps:\n",
                "1. Try polynomial features for non-linear relationships\n",
                "2. Implement feature selection techniques\n",
                "3. Tune hyperparameters (alpha for Ridge/Lasso)\n",
                "4. Handle outliers if detected\n",
                "5. Try ensemble methods for better performance"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "ml_env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.19"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}