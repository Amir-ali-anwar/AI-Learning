{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "intro",
            "metadata": {},
            "source": [
                "# Advanced K-Means Clustering\n",
                "\n",
                "## Learning Objectives\n",
                "- Understand K-Means algorithm in depth\n",
                "- Learn how to determine optimal number of clusters\n",
                "- Implement Elbow Method and Silhouette Analysis\n",
                "- Apply K-Means to real-world datasets\n",
                "- Visualize clustering results effectively\n",
                "\n",
                "## What is K-Means?\n",
                "K-Means is an unsupervised learning algorithm that groups similar data points into K clusters.\n",
                "\n",
                "### How it works:\n",
                "1. Initialize K centroids randomly\n",
                "2. Assign each point to nearest centroid\n",
                "3. Update centroids as mean of assigned points\n",
                "4. Repeat steps 2-3 until convergence\n",
                "\n",
                "### Key Parameters:\n",
                "- `n_clusters`: Number of clusters to form\n",
                "- `init`: Method for initialization ('k-means++' recommended)\n",
                "- `n_init`: Number of times algorithm runs with different centroid seeds\n",
                "- `max_iter`: Maximum number of iterations\n",
                "- `random_state`: For reproducibility"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "imports",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import required libraries\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.cluster import KMeans\n",
                "from sklearn.datasets import make_blobs\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.metrics import silhouette_score, silhouette_samples\n",
                "from sklearn.decomposition import PCA\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Set style for better visualizations\n",
                "sns.set_style('whitegrid')\n",
                "plt.rcParams['figure.figsize'] = (12, 8)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "synthetic_data",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£ Basic K-Means on Synthetic Data\n",
                "Let's start with a simple example using synthetic data to understand the algorithm."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "create_synthetic",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create synthetic dataset with clear clusters\n",
                "X, y_true = make_blobs(\n",
                "    n_samples=500,\n",
                "    centers=4,\n",
                "    cluster_std=0.8,\n",
                "    random_state=42\n",
                ")\n",
                "\n",
                "print(f\"Dataset shape: {X.shape}\")\n",
                "print(f\"Number of true clusters: {len(np.unique(y_true))}\")\n",
                "\n",
                "# Visualize the data\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.scatter(X[:, 0], X[:, 1], c=y_true, cmap='viridis', alpha=0.6, edgecolors='k')\n",
                "plt.title('Synthetic Dataset with True Labels', fontsize=14, fontweight='bold')\n",
                "plt.xlabel('Feature 1')\n",
                "plt.ylabel('Feature 2')\n",
                "plt.colorbar(label='True Cluster')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "basic_kmeans",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Apply K-Means clustering\n",
                "kmeans = KMeans(\n",
                "    n_clusters=4,\n",
                "    init='k-means++',  # Smart initialization\n",
                "    n_init=10,         # Run 10 times with different initializations\n",
                "    max_iter=300,\n",
                "    random_state=42\n",
                ")\n",
                "\n",
                "# Fit and predict\n",
                "y_pred = kmeans.fit_predict(X)\n",
                "\n",
                "# Get cluster centers\n",
                "centers = kmeans.cluster_centers_\n",
                "\n",
                "# Visualize results\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='viridis', alpha=0.6, edgecolors='k')\n",
                "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=300, alpha=0.8, \n",
                "            marker='X', edgecolors='black', linewidths=2, label='Centroids')\n",
                "plt.title('K-Means Clustering Results', fontsize=14, fontweight='bold')\n",
                "plt.xlabel('Feature 1')\n",
                "plt.ylabel('Feature 2')\n",
                "plt.legend()\n",
                "plt.colorbar(label='Predicted Cluster')\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nInertia (Sum of squared distances): {kmeans.inertia_:.2f}\")\n",
                "print(f\"Number of iterations: {kmeans.n_iter_}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "elbow_method",
            "metadata": {},
            "source": [
                "## 2Ô∏è‚É£ Determining Optimal Number of Clusters\n",
                "\n",
                "### Elbow Method\n",
                "The Elbow Method helps us find the optimal K by plotting inertia vs number of clusters.\n",
                "We look for the \"elbow\" point where adding more clusters doesn't significantly reduce inertia."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "elbow_implementation",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate inertia for different values of K\n",
                "inertias = []\n",
                "K_range = range(2, 11)\n",
                "\n",
                "for k in K_range:\n",
                "    kmeans_temp = KMeans(n_clusters=k, init='k-means++', n_init=10, random_state=42)\n",
                "    kmeans_temp.fit(X)\n",
                "    inertias.append(kmeans_temp.inertia_)\n",
                "\n",
                "# Plot Elbow Curve\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
                "plt.xlabel('Number of Clusters (K)', fontsize=12)\n",
                "plt.ylabel('Inertia (Within-Cluster Sum of Squares)', fontsize=12)\n",
                "plt.title('Elbow Method for Optimal K', fontsize=14, fontweight='bold')\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.xticks(K_range)\n",
                "\n",
                "# Highlight the elbow point (K=4 in this case)\n",
                "plt.axvline(x=4, color='r', linestyle='--', linewidth=2, label='Optimal K=4')\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "silhouette_analysis",
            "metadata": {},
            "source": [
                "### Silhouette Analysis\n",
                "Silhouette Score measures how similar a point is to its own cluster compared to other clusters.\n",
                "- Range: [-1, 1]\n",
                "- Higher values indicate better-defined clusters\n",
                "- Values near 0 indicate overlapping clusters\n",
                "- Negative values indicate misclassified points"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "silhouette_scores",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate silhouette scores for different K values\n",
                "silhouette_scores = []\n",
                "\n",
                "for k in K_range:\n",
                "    kmeans_temp = KMeans(n_clusters=k, init='k-means++', n_init=10, random_state=42)\n",
                "    labels = kmeans_temp.fit_predict(X)\n",
                "    score = silhouette_score(X, labels)\n",
                "    silhouette_scores.append(score)\n",
                "    print(f\"K={k}: Silhouette Score = {score:.3f}\")\n",
                "\n",
                "# Plot Silhouette Scores\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.plot(K_range, silhouette_scores, 'go-', linewidth=2, markersize=8)\n",
                "plt.xlabel('Number of Clusters (K)', fontsize=12)\n",
                "plt.ylabel('Silhouette Score', fontsize=12)\n",
                "plt.title('Silhouette Analysis for Optimal K', fontsize=14, fontweight='bold')\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.xticks(K_range)\n",
                "\n",
                "# Highlight the best K\n",
                "best_k = K_range[np.argmax(silhouette_scores)]\n",
                "plt.axvline(x=best_k, color='r', linestyle='--', linewidth=2, \n",
                "            label=f'Best K={best_k}')\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "real_world",
            "metadata": {},
            "source": [
                "## 3Ô∏è‚É£ Real-World Application: Customer Segmentation\n",
                "Let's apply K-Means to segment customers based on their purchasing behavior."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "load_diamonds",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load diamonds dataset\n",
                "df = sns.load_dataset('diamonds')\n",
                "print(f\"Dataset shape: {df.shape}\")\n",
                "print(f\"\\nFirst few rows:\")\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "preprocess_diamonds",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Select numerical features for clustering\n",
                "features = ['carat', 'depth', 'table', 'price', 'x', 'y', 'z']\n",
                "X_diamonds = df[features].copy()\n",
                "\n",
                "# Check for missing values\n",
                "print(\"Missing values:\")\n",
                "print(X_diamonds.isnull().sum())\n",
                "\n",
                "# Remove any rows with zero dimensions (data quality issue)\n",
                "X_diamonds = X_diamonds[(X_diamonds['x'] > 0) & (X_diamonds['y'] > 0) & (X_diamonds['z'] > 0)]\n",
                "print(f\"\\nDataset shape after cleaning: {X_diamonds.shape}\")\n",
                "\n",
                "# Feature scaling is CRUCIAL for K-Means!\n",
                "scaler = StandardScaler()\n",
                "X_scaled = scaler.fit_transform(X_diamonds)\n",
                "\n",
                "print(\"\\nFeatures scaled successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "find_optimal_k_diamonds",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Find optimal K for diamonds dataset\n",
                "# Note: Using a sample for computational efficiency\n",
                "sample_size = 5000\n",
                "X_sample = X_scaled[np.random.choice(X_scaled.shape[0], sample_size, replace=False)]\n",
                "\n",
                "inertias_diamonds = []\n",
                "silhouette_diamonds = []\n",
                "K_range_diamonds = range(2, 9)\n",
                "\n",
                "for k in K_range_diamonds:\n",
                "    kmeans_temp = KMeans(n_clusters=k, init='k-means++', n_init=10, random_state=42)\n",
                "    labels = kmeans_temp.fit_predict(X_sample)\n",
                "    inertias_diamonds.append(kmeans_temp.inertia_)\n",
                "    silhouette_diamonds.append(silhouette_score(X_sample, labels))\n",
                "\n",
                "# Plot both metrics\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
                "\n",
                "# Elbow plot\n",
                "ax1.plot(K_range_diamonds, inertias_diamonds, 'bo-', linewidth=2, markersize=8)\n",
                "ax1.set_xlabel('Number of Clusters (K)', fontsize=12)\n",
                "ax1.set_ylabel('Inertia', fontsize=12)\n",
                "ax1.set_title('Elbow Method - Diamonds Dataset', fontsize=14, fontweight='bold')\n",
                "ax1.grid(True, alpha=0.3)\n",
                "\n",
                "# Silhouette plot\n",
                "ax2.plot(K_range_diamonds, silhouette_diamonds, 'go-', linewidth=2, markersize=8)\n",
                "ax2.set_xlabel('Number of Clusters (K)', fontsize=12)\n",
                "ax2.set_ylabel('Silhouette Score', fontsize=12)\n",
                "ax2.set_title('Silhouette Analysis - Diamonds Dataset', fontsize=14, fontweight='bold')\n",
                "ax2.grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Print scores\n",
                "for k, sil in zip(K_range_diamonds, silhouette_diamonds):\n",
                "    print(f\"K={k}: Silhouette Score = {sil:.3f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "final_clustering",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Apply K-Means with optimal K (let's use K=3 based on analysis)\n",
                "optimal_k = 3\n",
                "kmeans_final = KMeans(n_clusters=optimal_k, init='k-means++', n_init=10, random_state=42)\n",
                "df['cluster'] = kmeans_final.fit_predict(X_scaled)\n",
                "\n",
                "print(f\"\\nCluster distribution:\")\n",
                "print(df['cluster'].value_counts().sort_index())\n",
                "\n",
                "# Analyze cluster characteristics\n",
                "print(\"\\nCluster Characteristics (Mean Values):\")\n",
                "cluster_summary = df.groupby('cluster')[features].mean()\n",
                "print(cluster_summary)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "visualize_clusters",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize clusters using PCA for dimensionality reduction\n",
                "pca = PCA(n_components=2)\n",
                "X_pca = pca.fit_transform(X_scaled)\n",
                "\n",
                "plt.figure(figsize=(12, 8))\n",
                "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=df['cluster'], \n",
                "                     cmap='viridis', alpha=0.6, edgecolors='k', s=50)\n",
                "plt.xlabel(f'First Principal Component ({pca.explained_variance_ratio_[0]:.2%} variance)', fontsize=12)\n",
                "plt.ylabel(f'Second Principal Component ({pca.explained_variance_ratio_[1]:.2%} variance)', fontsize=12)\n",
                "plt.title('Diamond Clusters Visualized with PCA', fontsize=14, fontweight='bold')\n",
                "plt.colorbar(scatter, label='Cluster')\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nTotal variance explained by 2 components: {pca.explained_variance_ratio_.sum():.2%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "insights",
            "metadata": {},
            "source": [
                "## 4Ô∏è‚É£ Business Insights\n",
                "Let's interpret the clusters and derive actionable insights."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cluster_profiling",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create comprehensive cluster profiles\n",
                "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
                "\n",
                "# Price distribution by cluster\n",
                "df.boxplot(column='price', by='cluster', ax=axes[0, 0])\n",
                "axes[0, 0].set_title('Price Distribution by Cluster')\n",
                "axes[0, 0].set_xlabel('Cluster')\n",
                "axes[0, 0].set_ylabel('Price ($)')\n",
                "\n",
                "# Carat distribution by cluster\n",
                "df.boxplot(column='carat', by='cluster', ax=axes[0, 1])\n",
                "axes[0, 1].set_title('Carat Distribution by Cluster')\n",
                "axes[0, 1].set_xlabel('Cluster')\n",
                "axes[0, 1].set_ylabel('Carat')\n",
                "\n",
                "# Cluster sizes\n",
                "cluster_counts = df['cluster'].value_counts().sort_index()\n",
                "axes[1, 0].bar(cluster_counts.index, cluster_counts.values, color='skyblue', edgecolor='black')\n",
                "axes[1, 0].set_title('Cluster Sizes')\n",
                "axes[1, 0].set_xlabel('Cluster')\n",
                "axes[1, 0].set_ylabel('Number of Diamonds')\n",
                "\n",
                "# Average price by cluster\n",
                "avg_price = df.groupby('cluster')['price'].mean()\n",
                "axes[1, 1].bar(avg_price.index, avg_price.values, color='lightcoral', edgecolor='black')\n",
                "axes[1, 1].set_title('Average Price by Cluster')\n",
                "axes[1, 1].set_xlabel('Cluster')\n",
                "axes[1, 1].set_ylabel('Average Price ($)')\n",
                "\n",
                "plt.suptitle('Comprehensive Cluster Analysis', fontsize=16, fontweight='bold', y=1.02)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "conclusions",
            "metadata": {},
            "source": [
                "## üìä Key Takeaways\n",
                "\n",
                "### Algorithm Understanding:\n",
                "1. K-Means is sensitive to initialization (use k-means++)\n",
                "2. Feature scaling is essential\n",
                "3. Works best with spherical clusters\n",
                "\n",
                "### Determining Optimal K:\n",
                "1. **Elbow Method**: Look for the \"elbow\" in the inertia plot\n",
                "2. **Silhouette Score**: Higher is better (closer to 1)\n",
                "3. **Domain Knowledge**: Consider business context\n",
                "\n",
                "### Best Practices:\n",
                "1. Always scale features before clustering\n",
                "2. Try multiple values of K\n",
                "3. Use multiple evaluation metrics\n",
                "4. Visualize results when possible\n",
                "5. Interpret clusters in business context\n",
                "\n",
                "### Limitations:\n",
                "1. Assumes spherical clusters\n",
                "2. Sensitive to outliers\n",
                "3. Requires specifying K beforehand\n",
                "4. May converge to local optima\n",
                "\n",
                "### When to Use K-Means:\n",
                "- ‚úÖ Customer segmentation\n",
                "- ‚úÖ Image compression\n",
                "- ‚úÖ Document clustering\n",
                "- ‚úÖ Anomaly detection (preprocessing)\n",
                "- ‚ùå Non-spherical clusters (use DBSCAN)\n",
                "- ‚ùå Varying cluster densities (use hierarchical clustering)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "exercises",
            "metadata": {},
            "source": [
                "## üéØ Practice Exercises\n",
                "\n",
                "1. **Try different datasets**: Apply K-Means to Iris or Wine datasets\n",
                "2. **Feature engineering**: Create new features and see how they affect clustering\n",
                "3. **Compare algorithms**: Try DBSCAN or Hierarchical Clustering on the same data\n",
                "4. **Outlier handling**: Remove outliers and observe the impact\n",
                "5. **Mini-batch K-Means**: For large datasets, try MiniBatchKMeans for faster computation"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "ml_env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.19"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}