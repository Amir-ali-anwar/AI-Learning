{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# PCA - Complete Guide\n",
                "## Principal Component Analysis for Dimensionality Reduction\n",
                "\n",
                "## üìö Learning Objectives\n",
                "- Understand PCA and dimensionality reduction\n",
                "- Learn how PCA finds principal components\n",
                "- Determine optimal number of components\n",
                "- Apply PCA for visualization and feature reduction\n",
                "- Understand variance explained and reconstruction error\n",
                "- Use PCA for noise reduction and data compression\n",
                "\n",
                "## üéØ What is PCA?\n",
                "\n",
                "**Principal Component Analysis (PCA)** is an unsupervised dimensionality reduction technique that transforms data into a new coordinate system where the greatest variances lie on the first coordinates (principal components).\n",
                "\n",
                "### Key Concepts:\n",
                "\n",
                "1. **Principal Components**: New orthogonal axes that capture maximum variance\n",
                "2. **Variance Explained**: How much information each component retains\n",
                "3. **Dimensionality Reduction**: Reduce features while preserving information\n",
                "4. **Linear Transformation**: PCA finds linear combinations of original features\n",
                "\n",
                "### How PCA Works:\n",
                "1. **Standardize** the data (mean=0, std=1)\n",
                "2. **Compute covariance matrix** of features\n",
                "3. **Find eigenvectors and eigenvalues** of covariance matrix\n",
                "4. **Sort eigenvectors** by eigenvalues (descending)\n",
                "5. **Project data** onto top k eigenvectors\n",
                "\n",
                "### When to Use PCA:\n",
                "‚úÖ **High-dimensional data** (many features)  \n",
                "‚úÖ **Visualization** (reduce to 2D/3D)  \n",
                "‚úÖ **Feature reduction** (remove redundancy)  \n",
                "‚úÖ **Noise reduction** (keep signal, remove noise)  \n",
                "‚úÖ **Speed up algorithms** (fewer features = faster)  \n",
                "‚úÖ **Multicollinearity** (correlated features)  \n",
                "\n",
                "### Advantages:\n",
                "‚úÖ Removes correlated features  \n",
                "‚úÖ Improves algorithm performance  \n",
                "‚úÖ Reduces overfitting  \n",
                "‚úÖ Enables visualization  \n",
                "‚úÖ No parameters to tune (just n_components)  \n",
                "\n",
                "### Disadvantages:\n",
                "‚ùå Loss of interpretability (new features are combinations)  \n",
                "‚ùå Assumes linear relationships  \n",
                "‚ùå Sensitive to feature scaling  \n",
                "‚ùå May not work well for non-linear data  "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.decomposition import PCA\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.datasets import load_iris, load_wine, load_breast_cancer, load_digits\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.metrics import accuracy_score, classification_report\n",
                "from mpl_toolkits.mplot3d import Axes3D\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Set style\n",
                "sns.set_style('whitegrid')\n",
                "plt.rcParams['figure.figsize'] = (12, 6)\n",
                "\n",
                "print(\"‚úÖ Libraries imported successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 1: Understanding PCA with Iris Dataset\n",
                "### 1Ô∏è‚É£ Load and Explore Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load iris dataset\n",
                "iris = load_iris()\n",
                "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
                "y = pd.Series(iris.target, name='species')\n",
                "\n",
                "print(f\"Dataset shape: {X.shape}\")\n",
                "print(f\"Features: {list(X.columns)}\")\n",
                "print(f\"\\nClasses: {iris.target_names}\")\n",
                "\n",
                "# Check correlations\n",
                "print(f\"\\nüìä Feature Correlations:\")\n",
                "corr_matrix = X.corr()\n",
                "print(corr_matrix)\n",
                "\n",
                "# Visualize correlations\n",
                "plt.figure(figsize=(10, 8))\n",
                "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
                "           square=True, linewidths=1, cbar_kws={'label': 'Correlation'})\n",
                "plt.title('Feature Correlation Matrix', fontsize=14, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nüí° High correlations indicate redundancy - PCA can help!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2Ô∏è‚É£ Apply PCA - Step by Step"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 1: Standardize the data (CRITICAL for PCA!)\n",
                "scaler = StandardScaler()\n",
                "X_scaled = scaler.fit_transform(X)\n",
                "\n",
                "print(\"Step 1: Data Standardized ‚úÖ\")\n",
                "print(f\"Mean: {X_scaled.mean(axis=0).round(10)}\")\n",
                "print(f\"Std: {X_scaled.std(axis=0).round(2)}\")\n",
                "\n",
                "# Step 2: Apply PCA\n",
                "pca = PCA()\n",
                "X_pca = pca.fit_transform(X_scaled)\n",
                "\n",
                "print(f\"\\nStep 2: PCA Applied ‚úÖ\")\n",
                "print(f\"Original dimensions: {X.shape[1]}\")\n",
                "print(f\"Principal components: {pca.n_components_}\")\n",
                "\n",
                "# Step 3: Analyze variance explained\n",
                "variance_explained = pca.explained_variance_ratio_\n",
                "cumulative_variance = np.cumsum(variance_explained)\n",
                "\n",
                "print(f\"\\nüìä Variance Explained by Each Component:\")\n",
                "for i, (var, cum_var) in enumerate(zip(variance_explained, cumulative_variance)):\n",
                "    print(f\"PC{i+1}: {var:.4f} ({var*100:.2f}%) | Cumulative: {cum_var:.4f} ({cum_var*100:.2f}%)\")\n",
                "\n",
                "# Visualize variance explained\n",
                "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
                "\n",
                "# Individual variance\n",
                "axes[0].bar(range(1, len(variance_explained)+1), variance_explained, \n",
                "           color='skyblue', edgecolor='black', alpha=0.7)\n",
                "axes[0].set_xlabel('Principal Component', fontsize=12)\n",
                "axes[0].set_ylabel('Variance Explained', fontsize=12)\n",
                "axes[0].set_title('Variance Explained by Each Component', fontsize=14, fontweight='bold')\n",
                "axes[0].set_xticks(range(1, len(variance_explained)+1))\n",
                "axes[0].grid(True, alpha=0.3, axis='y')\n",
                "\n",
                "# Cumulative variance\n",
                "axes[1].plot(range(1, len(cumulative_variance)+1), cumulative_variance, \n",
                "            marker='o', linewidth=2, markersize=8, color='green')\n",
                "axes[1].axhline(y=0.95, color='red', linestyle='--', linewidth=2, label='95% Variance')\n",
                "axes[1].axhline(y=0.90, color='orange', linestyle='--', linewidth=2, label='90% Variance')\n",
                "axes[1].set_xlabel('Number of Components', fontsize=12)\n",
                "axes[1].set_ylabel('Cumulative Variance Explained', fontsize=12)\n",
                "axes[1].set_title('Cumulative Variance Explained', fontsize=14, fontweight='bold')\n",
                "axes[1].set_xticks(range(1, len(cumulative_variance)+1))\n",
                "axes[1].legend(fontsize=11)\n",
                "axes[1].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nüí° First 2 components explain {cumulative_variance[1]*100:.2f}% of variance!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3Ô∏è‚É£ Visualize Data in Principal Component Space"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Reduce to 2D for visualization\n",
                "pca_2d = PCA(n_components=2)\n",
                "X_pca_2d = pca_2d.fit_transform(X_scaled)\n",
                "\n",
                "# Create DataFrame for easy plotting\n",
                "df_pca = pd.DataFrame({\n",
                "    'PC1': X_pca_2d[:, 0],\n",
                "    'PC2': X_pca_2d[:, 1],\n",
                "    'Species': [iris.target_names[i] for i in y]\n",
                "})\n",
                "\n",
                "# Visualize\n",
                "plt.figure(figsize=(12, 8))\n",
                "\n",
                "for species in iris.target_names:\n",
                "    mask = df_pca['Species'] == species\n",
                "    plt.scatter(df_pca.loc[mask, 'PC1'], \n",
                "               df_pca.loc[mask, 'PC2'],\n",
                "               label=species, s=100, alpha=0.7, edgecolors='black')\n",
                "\n",
                "plt.xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]*100:.1f}% variance)', fontsize=12)\n",
                "plt.ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]*100:.1f}% variance)', fontsize=12)\n",
                "plt.title('Iris Dataset in Principal Component Space (2D)', fontsize=14, fontweight='bold')\n",
                "plt.legend(fontsize=11)\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.axhline(y=0, color='k', linestyle='--', linewidth=0.5)\n",
                "plt.axvline(x=0, color='k', linestyle='--', linewidth=0.5)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nüí° Classes are well-separated in PC space!\")\n",
                "print(f\"üí° We reduced from {X.shape[1]} to 2 dimensions while keeping {pca_2d.explained_variance_ratio_.sum()*100:.1f}% of information\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4Ô∏è‚É£ Understanding Principal Components (Loadings)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get component loadings (how original features contribute to PCs)\n",
                "loadings = pd.DataFrame(\n",
                "    pca_2d.components_.T,\n",
                "    columns=['PC1', 'PC2'],\n",
                "    index=X.columns\n",
                ")\n",
                "\n",
                "print(\"üìä Principal Component Loadings:\")\n",
                "print(loadings)\n",
                "print(\"\\nüí° Loadings show how much each original feature contributes to each PC\")\n",
                "\n",
                "# Visualize loadings\n",
                "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
                "\n",
                "# Heatmap\n",
                "sns.heatmap(loadings.T, annot=True, fmt='.3f', cmap='RdBu_r', \n",
                "           center=0, ax=axes[0], cbar_kws={'label': 'Loading'})\n",
                "axes[0].set_title('Component Loadings Heatmap', fontsize=14, fontweight='bold')\n",
                "axes[0].set_xlabel('Original Features', fontsize=12)\n",
                "axes[0].set_ylabel('Principal Components', fontsize=12)\n",
                "\n",
                "# Bar plot for PC1\n",
                "loadings['PC1'].plot(kind='barh', ax=axes[1], color='steelblue', edgecolor='black')\n",
                "axes[1].set_title('PC1 Loadings (Most Important Component)', fontsize=14, fontweight='bold')\n",
                "axes[1].set_xlabel('Loading Value', fontsize=12)\n",
                "axes[1].axvline(x=0, color='black', linestyle='-', linewidth=1)\n",
                "axes[1].grid(True, alpha=0.3, axis='x')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nüí° Interpretation:\")\n",
                "print(\"- Positive loading: Feature increases with PC\")\n",
                "print(\"- Negative loading: Feature decreases with PC\")\n",
                "print(\"- Large absolute value: Feature is important for this PC\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 2: PCA for High-Dimensional Data\n",
                "### 5Ô∏è‚É£ Handwritten Digits (64 dimensions ‚Üí 2D)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load digits dataset (8x8 images = 64 features)\n",
                "digits = load_digits()\n",
                "X_digits = digits.data\n",
                "y_digits = digits.target\n",
                "\n",
                "print(f\"Digits dataset shape: {X_digits.shape}\")\n",
                "print(f\"Number of features: {X_digits.shape[1]}\")\n",
                "print(f\"Number of classes: {len(np.unique(y_digits))}\")\n",
                "\n",
                "# Show sample images\n",
                "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
                "axes = axes.ravel()\n",
                "\n",
                "for i in range(10):\n",
                "    axes[i].imshow(digits.images[i], cmap='gray')\n",
                "    axes[i].set_title(f'Digit: {digits.target[i]}', fontsize=11)\n",
                "    axes[i].axis('off')\n",
                "\n",
                "plt.suptitle('Sample Handwritten Digits (8x8 pixels)', fontsize=14, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Apply PCA\n",
                "scaler_digits = StandardScaler()\n",
                "X_digits_scaled = scaler_digits.fit_transform(X_digits)\n",
                "\n",
                "pca_digits = PCA()\n",
                "X_digits_pca = pca_digits.fit_transform(X_digits_scaled)\n",
                "\n",
                "# Analyze variance\n",
                "cumvar_digits = np.cumsum(pca_digits.explained_variance_ratio_)\n",
                "\n",
                "# Find number of components for 95% variance\n",
                "n_components_95 = np.argmax(cumvar_digits >= 0.95) + 1\n",
                "n_components_90 = np.argmax(cumvar_digits >= 0.90) + 1\n",
                "\n",
                "print(f\"\\nüìä Dimensionality Reduction Results:\")\n",
                "print(f\"Original dimensions: {X_digits.shape[1]}\")\n",
                "print(f\"Components for 90% variance: {n_components_90} ({n_components_90/X_digits.shape[1]*100:.1f}% reduction)\")\n",
                "print(f\"Components for 95% variance: {n_components_95} ({n_components_95/X_digits.shape[1]*100:.1f}% reduction)\")\n",
                "\n",
                "# Plot cumulative variance\n",
                "plt.figure(figsize=(12, 6))\n",
                "plt.plot(range(1, len(cumvar_digits)+1), cumvar_digits, linewidth=2, color='blue')\n",
                "plt.axhline(y=0.95, color='red', linestyle='--', linewidth=2, label='95% Variance')\n",
                "plt.axhline(y=0.90, color='orange', linestyle='--', linewidth=2, label='90% Variance')\n",
                "plt.axvline(x=n_components_95, color='red', linestyle=':', linewidth=2, alpha=0.5)\n",
                "plt.axvline(x=n_components_90, color='orange', linestyle=':', linewidth=2, alpha=0.5)\n",
                "plt.xlabel('Number of Components', fontsize=12)\n",
                "plt.ylabel('Cumulative Variance Explained', fontsize=12)\n",
                "plt.title('Cumulative Variance Explained - Digits Dataset', fontsize=14, fontweight='bold')\n",
                "plt.legend(fontsize=11)\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6Ô∏è‚É£ Visualize High-Dimensional Data in 2D"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Reduce to 2D\n",
                "pca_2d_digits = PCA(n_components=2)\n",
                "X_digits_2d = pca_2d_digits.fit_transform(X_digits_scaled)\n",
                "\n",
                "# Visualize\n",
                "plt.figure(figsize=(14, 10))\n",
                "\n",
                "scatter = plt.scatter(X_digits_2d[:, 0], X_digits_2d[:, 1], \n",
                "                     c=y_digits, cmap='tab10', \n",
                "                     s=50, alpha=0.6, edgecolors='black', linewidths=0.5)\n",
                "\n",
                "plt.xlabel(f'PC1 ({pca_2d_digits.explained_variance_ratio_[0]*100:.1f}% variance)', fontsize=12)\n",
                "plt.ylabel(f'PC2 ({pca_2d_digits.explained_variance_ratio_[1]*100:.1f}% variance)', fontsize=12)\n",
                "plt.title('Handwritten Digits in 2D Principal Component Space\\n(64D ‚Üí 2D)', \n",
                "         fontsize=14, fontweight='bold')\n",
                "plt.colorbar(scatter, label='Digit', ticks=range(10))\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nüí° Reduced from {X_digits.shape[1]} to 2 dimensions!\")\n",
                "print(f\"üí° Retained {pca_2d_digits.explained_variance_ratio_.sum()*100:.1f}% of variance\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 3: PCA for Machine Learning\n",
                "### 7Ô∏è‚É£ Impact on Model Performance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load breast cancer dataset (30 features)\n",
                "cancer = load_breast_cancer()\n",
                "X_cancer = cancer.data\n",
                "y_cancer = cancer.target\n",
                "\n",
                "print(f\"Breast Cancer dataset: {X_cancer.shape}\")\n",
                "print(f\"Features: {X_cancer.shape[1]}\")\n",
                "\n",
                "# Split data\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X_cancer, y_cancer, test_size=0.2, random_state=42, stratify=y_cancer\n",
                ")\n",
                "\n",
                "# Scale data\n",
                "scaler_cancer = StandardScaler()\n",
                "X_train_scaled = scaler_cancer.fit_transform(X_train)\n",
                "X_test_scaled = scaler_cancer.transform(X_test)\n",
                "\n",
                "# Test different numbers of components\n",
                "n_components_list = [2, 5, 10, 15, 20, 25, 30]\n",
                "results = []\n",
                "\n",
                "for n_comp in n_components_list:\n",
                "    # Apply PCA\n",
                "    pca_temp = PCA(n_components=n_comp)\n",
                "    X_train_pca = pca_temp.fit_transform(X_train_scaled)\n",
                "    X_test_pca = pca_temp.transform(X_test_scaled)\n",
                "    \n",
                "    # Train Logistic Regression\n",
                "    lr = LogisticRegression(random_state=42, max_iter=10000)\n",
                "    lr.fit(X_train_pca, y_train)\n",
                "    lr_acc = accuracy_score(y_test, lr.predict(X_test_pca))\n",
                "    \n",
                "    # Train Random Forest\n",
                "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
                "    rf.fit(X_train_pca, y_train)\n",
                "    rf_acc = accuracy_score(y_test, rf.predict(X_test_pca))\n",
                "    \n",
                "    # Variance explained\n",
                "    var_explained = pca_temp.explained_variance_ratio_.sum()\n",
                "    \n",
                "    results.append({\n",
                "        'n_components': n_comp,\n",
                "        'variance_explained': var_explained,\n",
                "        'logistic_regression': lr_acc,\n",
                "        'random_forest': rf_acc\n",
                "    })\n",
                "\n",
                "results_df = pd.DataFrame(results)\n",
                "print(\"\\nüìä Model Performance vs Number of Components:\")\n",
                "print(results_df.to_string(index=False))\n",
                "\n",
                "# Visualize\n",
                "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
                "\n",
                "# Accuracy vs components\n",
                "axes[0].plot(results_df['n_components'], results_df['logistic_regression'], \n",
                "            marker='o', linewidth=2, label='Logistic Regression', color='blue')\n",
                "axes[0].plot(results_df['n_components'], results_df['random_forest'],\n",
                "            marker='s', linewidth=2, label='Random Forest', color='green')\n",
                "axes[0].set_xlabel('Number of Components', fontsize=12)\n",
                "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
                "axes[0].set_title('Model Accuracy vs Number of PCA Components', fontsize=14, fontweight='bold')\n",
                "axes[0].legend(fontsize=11)\n",
                "axes[0].grid(True, alpha=0.3)\n",
                "\n",
                "# Variance vs components\n",
                "axes[1].plot(results_df['n_components'], results_df['variance_explained'],\n",
                "            marker='o', linewidth=2, color='purple')\n",
                "axes[1].axhline(y=0.95, color='red', linestyle='--', linewidth=2, label='95% Variance')\n",
                "axes[1].set_xlabel('Number of Components', fontsize=12)\n",
                "axes[1].set_ylabel('Cumulative Variance Explained', fontsize=12)\n",
                "axes[1].set_title('Variance Explained vs Number of Components', fontsize=14, fontweight='bold')\n",
                "axes[1].legend(fontsize=11)\n",
                "axes[1].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nüí° Key Insights:\")\n",
                "print(f\"- With just {results_df.iloc[2]['n_components']} components ({results_df.iloc[2]['variance_explained']*100:.1f}% variance):\")\n",
                "print(f\"  Logistic Regression: {results_df.iloc[2]['logistic_regression']:.4f}\")\n",
                "print(f\"  Random Forest: {results_df.iloc[2]['random_forest']:.4f}\")\n",
                "print(f\"- Original features: {X_cancer.shape[1]}\")\n",
                "print(f\"- Dimensionality reduction: {(1 - results_df.iloc[2]['n_components']/X_cancer.shape[1])*100:.1f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 8Ô∏è‚É£ Image Reconstruction and Compression"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Select a digit image\n",
                "sample_idx = 0\n",
                "original_image = digits.images[sample_idx]\n",
                "original_flat = digits.data[sample_idx]\n",
                "\n",
                "# Standardize\n",
                "original_scaled = scaler_digits.transform([original_flat])\n",
                "\n",
                "# Reconstruct with different numbers of components\n",
                "n_components_test = [2, 5, 10, 20, 40, 64]\n",
                "\n",
                "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
                "axes = axes.ravel()\n",
                "\n",
                "# Original\n",
                "axes[0].imshow(original_image, cmap='gray')\n",
                "axes[0].set_title('Original\\n(64 features)', fontsize=11, fontweight='bold')\n",
                "axes[0].axis('off')\n",
                "\n",
                "# Reconstructions\n",
                "for idx, n_comp in enumerate(n_components_test[:-1], 1):\n",
                "    # Apply PCA\n",
                "    pca_temp = PCA(n_components=n_comp)\n",
                "    pca_temp.fit(X_digits_scaled)\n",
                "    \n",
                "    # Transform and inverse transform\n",
                "    transformed = pca_temp.transform(original_scaled)\n",
                "    reconstructed = pca_temp.inverse_transform(transformed)\n",
                "    \n",
                "    # Reshape to image\n",
                "    reconstructed_image = reconstructed.reshape(8, 8)\n",
                "    \n",
                "    # Calculate reconstruction error\n",
                "    mse = np.mean((original_scaled - reconstructed)**2)\n",
                "    var_explained = pca_temp.explained_variance_ratio_.sum()\n",
                "    \n",
                "    # Plot\n",
                "    axes[idx].imshow(reconstructed_image, cmap='gray')\n",
                "    axes[idx].set_title(f'{n_comp} components\\n({var_explained*100:.1f}% var, MSE={mse:.4f})', \n",
                "                       fontsize=10)\n",
                "    axes[idx].axis('off')\n",
                "\n",
                "# Hide last subplot\n",
                "axes[-1].axis('off')\n",
                "\n",
                "plt.suptitle('Image Reconstruction with Different Numbers of PCA Components', \n",
                "            fontsize=14, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nüí° PCA can be used for image compression!\")\n",
                "print(\"üí° Trade-off: Fewer components = more compression but lower quality\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìä Key Takeaways\n",
                "\n",
                "### PCA Advantages:\n",
                "‚úÖ **Reduces dimensionality** - Fewer features, faster algorithms  \n",
                "‚úÖ **Removes multicollinearity** - Decorrelates features  \n",
                "‚úÖ **Noise reduction** - Keeps signal, removes noise  \n",
                "‚úÖ **Visualization** - Enables 2D/3D plotting of high-D data  \n",
                "‚úÖ **No parameters** - Only need to choose n_components  \n",
                "‚úÖ **Interpretable variance** - Clear metric of information retained  \n",
                "\n",
                "### PCA Disadvantages:\n",
                "‚ùå **Loss of interpretability** - PCs are combinations of original features  \n",
                "‚ùå **Linear only** - Assumes linear relationships  \n",
                "‚ùå **Sensitive to scaling** - Must standardize data first  \n",
                "‚ùå **Information loss** - Discarding components loses information  \n",
                "‚ùå **Not for categorical data** - Works best with continuous features  \n",
                "\n",
                "### How to Choose Number of Components:\n",
                "\n",
                "**Method 1: Variance Threshold**\n",
                "- Keep components that explain 90-95% of variance\n",
                "- Common in practice\n",
                "\n",
                "**Method 2: Elbow Method**\n",
                "- Plot cumulative variance\n",
                "- Look for \"elbow\" where curve flattens\n",
                "\n",
                "**Method 3: Cross-Validation**\n",
                "- Test different n_components\n",
                "- Choose based on downstream task performance\n",
                "\n",
                "**Method 4: Kaiser Criterion**\n",
                "- Keep components with eigenvalue > 1\n",
                "- Less common in practice\n",
                "\n",
                "### Best Practices:\n",
                "\n",
                "1. **Always standardize** - PCA is sensitive to feature scales\n",
                "   ```python\n",
                "   scaler = StandardScaler()\n",
                "   X_scaled = scaler.fit_transform(X)\n",
                "   ```\n",
                "\n",
                "2. **Check variance explained** - Ensure you keep enough information\n",
                "   ```python\n",
                "   cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
                "   n_components = np.argmax(cumsum >= 0.95) + 1\n",
                "   ```\n",
                "\n",
                "3. **Visualize results** - Plot data in PC space\n",
                "\n",
                "4. **Validate with downstream task** - Check if PCA improves model performance\n",
                "\n",
                "5. **Consider alternatives** - For non-linear data, try t-SNE, UMAP, or kernel PCA\n",
                "\n",
                "### When to Use PCA:\n",
                "‚úÖ High-dimensional data (many features)  \n",
                "‚úÖ Correlated features (multicollinearity)  \n",
                "‚úÖ Need visualization (reduce to 2D/3D)  \n",
                "‚úÖ Speed up algorithms (fewer features)  \n",
                "‚úÖ Noise reduction  \n",
                "‚úÖ Data compression  \n",
                "\n",
                "### When NOT to Use PCA:\n",
                "‚ùå Features are already uncorrelated  \n",
                "‚ùå Interpretability is critical  \n",
                "‚ùå Non-linear relationships (use kernel PCA, t-SNE)  \n",
                "‚ùå Categorical data  \n",
                "‚ùå Small number of features  \n",
                "\n",
                "### Real-World Applications:\n",
                "1. **Image Compression** - Reduce storage size\n",
                "2. **Face Recognition** - Eigenfaces\n",
                "3. **Genomics** - Gene expression analysis\n",
                "4. **Finance** - Portfolio optimization\n",
                "5. **Recommender Systems** - Latent factor models\n",
                "6. **Anomaly Detection** - Reconstruction error\n",
                "\n",
                "### PCA Workflow:\n",
                "```python\n",
                "# 1. Standardize\n",
                "scaler = StandardScaler()\n",
                "X_scaled = scaler.fit_transform(X)\n",
                "\n",
                "# 2. Fit PCA\n",
                "pca = PCA(n_components=0.95)  # Keep 95% variance\n",
                "X_pca = pca.fit_transform(X_scaled)\n",
                "\n",
                "# 3. Check variance\n",
                "print(f\"Components: {pca.n_components_}\")\n",
                "print(f\"Variance: {pca.explained_variance_ratio_.sum()}\")\n",
                "\n",
                "# 4. Use transformed data\n",
                "model.fit(X_pca, y)\n",
                "```\n",
                "\n",
                "### Comparison with Other Methods:\n",
                "\n",
                "| Method | Type | Linear | Preserves Distance | Speed |\n",
                "|--------|------|--------|-------------------|-------|\n",
                "| **PCA** | Unsupervised | Yes | Global | Fast |\n",
                "| **t-SNE** | Unsupervised | No | Local | Slow |\n",
                "| **UMAP** | Unsupervised | No | Local | Medium |\n",
                "| **LDA** | Supervised | Yes | Global | Fast |\n",
                "| **Autoencoder** | Unsupervised | No | Learned | Medium |\n",
                "\n",
                "### Next Steps:\n",
                "1. Try kernel PCA for non-linear data\n",
                "2. Compare with t-SNE for visualization\n",
                "3. Use PCA as preprocessing for ML models\n",
                "4. Apply to your own high-dimensional datasets\n",
                "5. Experiment with incremental PCA for large datasets"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "ml_env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.19"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}