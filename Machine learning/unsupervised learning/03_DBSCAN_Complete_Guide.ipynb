{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# DBSCAN - Complete Guide\n",
                "## Density-Based Spatial Clustering of Applications with Noise\n",
                "\n",
                "## ðŸ“š Learning Objectives\n",
                "- Understand DBSCAN algorithm and density-based clustering\n",
                "- Learn to tune eps and min_samples parameters\n",
                "- Detect and handle noise/outliers\n",
                "- Find clusters of arbitrary shapes\n",
                "- Compare DBSCAN with K-Means\n",
                "- Apply to real-world geospatial and anomaly detection problems\n",
                "\n",
                "## ðŸŽ¯ What is DBSCAN?\n",
                "\n",
                "**DBSCAN** is a density-based clustering algorithm that groups together points that are closely packed together, marking points in low-density regions as outliers.\n",
                "\n",
                "### Key Concepts:\n",
                "\n",
                "1. **Core Point**: A point with at least `min_samples` neighbors within `eps` distance\n",
                "2. **Border Point**: A point within `eps` distance of a core point, but has fewer than `min_samples` neighbors\n",
                "3. **Noise Point**: A point that is neither core nor border (outlier)\n",
                "\n",
                "### Parameters:\n",
                "- **eps (Îµ)**: Maximum distance between two points to be considered neighbors\n",
                "- **min_samples**: Minimum number of points required to form a dense region (core point)\n",
                "\n",
                "### Advantages over K-Means:\n",
                "âœ… **No need to specify number of clusters** (K-Means requires K)  \n",
                "âœ… **Finds arbitrary-shaped clusters** (K-Means only finds spherical)  \n",
                "âœ… **Detects outliers/noise** (K-Means assigns all points to clusters)  \n",
                "âœ… **Robust to outliers** (K-Means sensitive to outliers)  \n",
                "\n",
                "### When to Use:\n",
                "âœ… Unknown number of clusters  \n",
                "âœ… Non-spherical cluster shapes  \n",
                "âœ… Presence of noise/outliers  \n",
                "âœ… Geospatial data  \n",
                "âœ… Anomaly detection  "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.cluster import DBSCAN, KMeans\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.datasets import make_moons, make_circles, make_blobs\n",
                "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
                "from sklearn.neighbors import NearestNeighbors\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Set style\n",
                "sns.set_style('whitegrid')\n",
                "plt.rcParams['figure.figsize'] = (12, 6)\n",
                "\n",
                "print(\"âœ… Libraries imported successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 1: Understanding DBSCAN with Synthetic Data\n",
                "### 1ï¸âƒ£ Create Non-Spherical Clusters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create three different datasets with different shapes\n",
                "np.random.seed(42)\n",
                "\n",
                "# Dataset 1: Moons (non-linear, crescent shapes)\n",
                "X_moons, y_moons = make_moons(n_samples=300, noise=0.05, random_state=42)\n",
                "\n",
                "# Dataset 2: Circles (concentric circles)\n",
                "X_circles, y_circles = make_circles(n_samples=300, noise=0.05, factor=0.5, random_state=42)\n",
                "\n",
                "# Dataset 3: Blobs with noise (spherical + outliers)\n",
                "X_blobs, y_blobs = make_blobs(n_samples=300, centers=3, cluster_std=0.5, random_state=42)\n",
                "# Add noise points\n",
                "noise = np.random.uniform(X_blobs.min(), X_blobs.max(), size=(30, 2))\n",
                "X_blobs = np.vstack([X_blobs, noise])\n",
                "y_blobs = np.hstack([y_blobs, -1 * np.ones(30)])  # -1 for noise\n",
                "\n",
                "# Visualize datasets\n",
                "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
                "\n",
                "datasets = [\n",
                "    (X_moons, 'Moons (Crescent Shapes)'),\n",
                "    (X_circles, 'Circles (Concentric)'),\n",
                "    (X_blobs, 'Blobs with Noise')\n",
                "]\n",
                "\n",
                "for idx, (X, title) in enumerate(datasets):\n",
                "    axes[idx].scatter(X[:, 0], X[:, 1], c='steelblue', alpha=0.6, edgecolors='black', s=50)\n",
                "    axes[idx].set_title(title, fontsize=14, fontweight='bold')\n",
                "    axes[idx].set_xlabel('Feature 1', fontsize=11)\n",
                "    axes[idx].set_ylabel('Feature 2', fontsize=11)\n",
                "    axes[idx].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nðŸ’¡ These datasets have non-spherical shapes that K-Means struggles with\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2ï¸âƒ£ Compare K-Means vs DBSCAN"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Apply K-Means and DBSCAN to moons dataset\n",
                "X = X_moons\n",
                "\n",
                "# K-Means\n",
                "kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
                "kmeans_labels = kmeans.fit_predict(X)\n",
                "\n",
                "# DBSCAN\n",
                "dbscan = DBSCAN(eps=0.3, min_samples=5)\n",
                "dbscan_labels = dbscan.fit_predict(X)\n",
                "\n",
                "# Visualize comparison\n",
                "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
                "\n",
                "# Original data\n",
                "axes[0].scatter(X[:, 0], X[:, 1], c='gray', alpha=0.6, edgecolors='black', s=50)\n",
                "axes[0].set_title('Original Data', fontsize=14, fontweight='bold')\n",
                "axes[0].set_xlabel('Feature 1', fontsize=11)\n",
                "axes[0].set_ylabel('Feature 2', fontsize=11)\n",
                "axes[0].grid(True, alpha=0.3)\n",
                "\n",
                "# K-Means clustering\n",
                "scatter1 = axes[1].scatter(X[:, 0], X[:, 1], c=kmeans_labels, cmap='viridis', \n",
                "                           alpha=0.6, edgecolors='black', s=50)\n",
                "axes[1].scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
                "               c='red', marker='X', s=200, edgecolors='black', linewidths=2, label='Centroids')\n",
                "axes[1].set_title('K-Means Clustering (K=2)', fontsize=14, fontweight='bold')\n",
                "axes[1].set_xlabel('Feature 1', fontsize=11)\n",
                "axes[1].set_ylabel('Feature 2', fontsize=11)\n",
                "axes[1].legend()\n",
                "axes[1].grid(True, alpha=0.3)\n",
                "\n",
                "# DBSCAN clustering\n",
                "scatter2 = axes[2].scatter(X[:, 0], X[:, 1], c=dbscan_labels, cmap='viridis',\n",
                "                           alpha=0.6, edgecolors='black', s=50)\n",
                "# Highlight noise points\n",
                "noise_mask = dbscan_labels == -1\n",
                "if noise_mask.any():\n",
                "    axes[2].scatter(X[noise_mask, 0], X[noise_mask, 1], c='red', marker='x',\n",
                "                   s=100, linewidths=2, label='Noise')\n",
                "axes[2].set_title(f'DBSCAN (eps=0.3, min_samples=5)', fontsize=14, fontweight='bold')\n",
                "axes[2].set_xlabel('Feature 1', fontsize=11)\n",
                "axes[2].set_ylabel('Feature 2', fontsize=11)\n",
                "axes[2].legend()\n",
                "axes[2].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Print statistics\n",
                "n_clusters_dbscan = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
                "n_noise = list(dbscan_labels).count(-1)\n",
                "\n",
                "print(f\"\\nðŸ“Š Clustering Results:\")\n",
                "print(f\"\\nK-Means:\")\n",
                "print(f\"  - Clusters found: {len(set(kmeans_labels))}\")\n",
                "print(f\"  - Silhouette Score: {silhouette_score(X, kmeans_labels):.4f}\")\n",
                "\n",
                "print(f\"\\nDBSCAN:\")\n",
                "print(f\"  - Clusters found: {n_clusters_dbscan}\")\n",
                "print(f\"  - Noise points: {n_noise}\")\n",
                "if n_clusters_dbscan > 1:\n",
                "    # Calculate silhouette score only for non-noise points\n",
                "    mask = dbscan_labels != -1\n",
                "    if mask.sum() > 0:\n",
                "        print(f\"  - Silhouette Score: {silhouette_score(X[mask], dbscan_labels[mask]):.4f}\")\n",
                "\n",
                "print(\"\\nðŸ’¡ DBSCAN correctly identifies the crescent shapes, while K-Means fails!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3ï¸âƒ£ Finding Optimal eps Parameter\n",
                "#### Using K-distance Graph (Elbow Method for DBSCAN)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate k-distance for each point\n",
                "# k should be min_samples - 1\n",
                "min_samples = 5\n",
                "k = min_samples - 1\n",
                "\n",
                "# Fit nearest neighbors\n",
                "neighbors = NearestNeighbors(n_neighbors=k)\n",
                "neighbors_fit = neighbors.fit(X)\n",
                "distances, indices = neighbors_fit.kneighbors(X)\n",
                "\n",
                "# Sort distances\n",
                "distances = np.sort(distances[:, k-1], axis=0)\n",
                "\n",
                "# Plot k-distance graph\n",
                "plt.figure(figsize=(12, 6))\n",
                "plt.plot(distances, linewidth=2, color='steelblue')\n",
                "plt.xlabel('Points sorted by distance', fontsize=12)\n",
                "plt.ylabel(f'{k}-NN Distance', fontsize=12)\n",
                "plt.title(f'K-Distance Graph (k={k})\\nLook for \"elbow\" to determine optimal eps', \n",
                "         fontsize=14, fontweight='bold')\n",
                "plt.grid(True, alpha=0.3)\n",
                "\n",
                "# Add suggested eps line\n",
                "suggested_eps = 0.3\n",
                "plt.axhline(y=suggested_eps, color='red', linestyle='--', linewidth=2, \n",
                "           label=f'Suggested eps = {suggested_eps}')\n",
                "plt.legend(fontsize=11)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nðŸ’¡ The 'elbow' in the graph suggests a good eps value\")\n",
                "print(\"ðŸ’¡ Points after the elbow are likely outliers\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4ï¸âƒ£ Parameter Tuning: eps and min_samples"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test different parameter combinations\n",
                "eps_values = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
                "min_samples_values = [3, 5, 10, 15]\n",
                "\n",
                "results = []\n",
                "\n",
                "for eps in eps_values:\n",
                "    for min_samp in min_samples_values:\n",
                "        dbscan = DBSCAN(eps=eps, min_samples=min_samp)\n",
                "        labels = dbscan.fit_predict(X)\n",
                "        \n",
                "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
                "        n_noise = list(labels).count(-1)\n",
                "        \n",
                "        # Calculate silhouette score if we have clusters\n",
                "        if n_clusters > 1:\n",
                "            mask = labels != -1\n",
                "            if mask.sum() > 0 and len(set(labels[mask])) > 1:\n",
                "                sil_score = silhouette_score(X[mask], labels[mask])\n",
                "            else:\n",
                "                sil_score = -1\n",
                "        else:\n",
                "            sil_score = -1\n",
                "        \n",
                "        results.append({\n",
                "            'eps': eps,\n",
                "            'min_samples': min_samp,\n",
                "            'n_clusters': n_clusters,\n",
                "            'n_noise': n_noise,\n",
                "            'silhouette': sil_score\n",
                "        })\n",
                "\n",
                "results_df = pd.DataFrame(results)\n",
                "print(\"\\nðŸ“Š Parameter Tuning Results:\")\n",
                "print(results_df.to_string(index=False))\n",
                "\n",
                "# Visualize results\n",
                "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
                "\n",
                "# Heatmap of number of clusters\n",
                "pivot_clusters = results_df.pivot(index='min_samples', columns='eps', values='n_clusters')\n",
                "sns.heatmap(pivot_clusters, annot=True, fmt='g', cmap='YlOrRd', ax=axes[0], \n",
                "           cbar_kws={'label': 'Number of Clusters'})\n",
                "axes[0].set_title('Number of Clusters Found', fontsize=14, fontweight='bold')\n",
                "axes[0].set_xlabel('eps', fontsize=12)\n",
                "axes[0].set_ylabel('min_samples', fontsize=12)\n",
                "\n",
                "# Heatmap of silhouette scores\n",
                "pivot_sil = results_df.pivot(index='min_samples', columns='eps', values='silhouette')\n",
                "sns.heatmap(pivot_sil, annot=True, fmt='.3f', cmap='RdYlGn', ax=axes[1],\n",
                "           cbar_kws={'label': 'Silhouette Score'})\n",
                "axes[1].set_title('Silhouette Scores', fontsize=14, fontweight='bold')\n",
                "axes[1].set_xlabel('eps', fontsize=12)\n",
                "axes[1].set_ylabel('min_samples', fontsize=12)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Find best parameters\n",
                "best_params = results_df.loc[results_df['silhouette'].idxmax()]\n",
                "print(f\"\\nðŸŽ¯ Best Parameters:\")\n",
                "print(f\"eps = {best_params['eps']}\")\n",
                "print(f\"min_samples = {int(best_params['min_samples'])}\")\n",
                "print(f\"Silhouette Score = {best_params['silhouette']:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 2: Real-World Application - Customer Segmentation\n",
                "### 5ï¸âƒ£ Load and Prepare Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create synthetic customer data\n",
                "np.random.seed(42)\n",
                "\n",
                "# Generate customer segments\n",
                "n_customers = 500\n",
                "\n",
                "# Segment 1: High value customers (high spending, high frequency)\n",
                "high_value = np.random.multivariate_normal(\n",
                "    mean=[80, 15], \n",
                "    cov=[[100, 20], [20, 10]], \n",
                "    size=100\n",
                ")\n",
                "\n",
                "# Segment 2: Regular customers (medium spending, medium frequency)\n",
                "regular = np.random.multivariate_normal(\n",
                "    mean=[50, 8],\n",
                "    cov=[[80, 15], [15, 8]],\n",
                "    size=200\n",
                ")\n",
                "\n",
                "# Segment 3: Occasional customers (low spending, low frequency)\n",
                "occasional = np.random.multivariate_normal(\n",
                "    mean=[20, 3],\n",
                "    cov=[[50, 10], [10, 5]],\n",
                "    size=150\n",
                ")\n",
                "\n",
                "# Add some outliers (one-time big purchases, fraudulent transactions, etc.)\n",
                "outliers = np.random.uniform(low=[0, 0], high=[100, 20], size=(50, 2))\n",
                "\n",
                "# Combine all data\n",
                "X_customers = np.vstack([high_value, regular, occasional, outliers])\n",
                "\n",
                "# Create DataFrame\n",
                "df_customers = pd.DataFrame(X_customers, columns=['Total_Spending', 'Purchase_Frequency'])\n",
                "df_customers['Total_Spending'] = df_customers['Total_Spending'].clip(lower=0)\n",
                "df_customers['Purchase_Frequency'] = df_customers['Purchase_Frequency'].clip(lower=0)\n",
                "\n",
                "print(f\"Customer dataset shape: {df_customers.shape}\")\n",
                "print(f\"\\nðŸ“Š Dataset Statistics:\")\n",
                "print(df_customers.describe())\n",
                "\n",
                "# Visualize\n",
                "plt.figure(figsize=(12, 6))\n",
                "plt.scatter(df_customers['Total_Spending'], df_customers['Purchase_Frequency'],\n",
                "           alpha=0.6, edgecolors='black', s=50, c='steelblue')\n",
                "plt.xlabel('Total Spending ($)', fontsize=12)\n",
                "plt.ylabel('Purchase Frequency (per month)', fontsize=12)\n",
                "plt.title('Customer Data Distribution', fontsize=14, fontweight='bold')\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6ï¸âƒ£ Apply DBSCAN to Customer Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Scale the data\n",
                "scaler = StandardScaler()\n",
                "X_scaled = scaler.fit_transform(df_customers)\n",
                "\n",
                "# Apply DBSCAN\n",
                "dbscan_customers = DBSCAN(eps=0.5, min_samples=10)\n",
                "clusters = dbscan_customers.fit_predict(X_scaled)\n",
                "\n",
                "# Add cluster labels to dataframe\n",
                "df_customers['Cluster'] = clusters\n",
                "\n",
                "# Statistics\n",
                "n_clusters = len(set(clusters)) - (1 if -1 in clusters else 0)\n",
                "n_noise = list(clusters).count(-1)\n",
                "\n",
                "print(f\"\\nðŸ“Š DBSCAN Results:\")\n",
                "print(f\"Number of clusters: {n_clusters}\")\n",
                "print(f\"Number of noise points (outliers): {n_noise}\")\n",
                "print(f\"\\nCluster sizes:\")\n",
                "print(df_customers['Cluster'].value_counts().sort_index())\n",
                "\n",
                "# Visualize clusters\n",
                "plt.figure(figsize=(14, 7))\n",
                "\n",
                "# Plot each cluster\n",
                "unique_clusters = set(clusters)\n",
                "colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_clusters)))\n",
                "\n",
                "for cluster, color in zip(unique_clusters, colors):\n",
                "    if cluster == -1:\n",
                "        # Noise points in black\n",
                "        mask = clusters == cluster\n",
                "        plt.scatter(df_customers.loc[mask, 'Total_Spending'],\n",
                "                   df_customers.loc[mask, 'Purchase_Frequency'],\n",
                "                   c='black', marker='x', s=100, linewidths=2,\n",
                "                   label='Outliers', alpha=0.8)\n",
                "    else:\n",
                "        mask = clusters == cluster\n",
                "        plt.scatter(df_customers.loc[mask, 'Total_Spending'],\n",
                "                   df_customers.loc[mask, 'Purchase_Frequency'],\n",
                "                   c=[color], s=80, alpha=0.7, edgecolors='black',\n",
                "                   label=f'Cluster {cluster}')\n",
                "\n",
                "plt.xlabel('Total Spending ($)', fontsize=12)\n",
                "plt.ylabel('Purchase Frequency (per month)', fontsize=12)\n",
                "plt.title('Customer Segmentation with DBSCAN', fontsize=14, fontweight='bold')\n",
                "plt.legend(fontsize=10)\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 7ï¸âƒ£ Cluster Profiling and Business Insights"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate cluster statistics\n",
                "cluster_profiles = df_customers.groupby('Cluster').agg({\n",
                "    'Total_Spending': ['mean', 'median', 'std', 'count'],\n",
                "    'Purchase_Frequency': ['mean', 'median', 'std']\n",
                "}).round(2)\n",
                "\n",
                "print(\"\\nðŸ“Š Cluster Profiles:\")\n",
                "print(cluster_profiles)\n",
                "\n",
                "# Assign business labels\n",
                "def assign_segment_label(row):\n",
                "    if row['Cluster'] == -1:\n",
                "        return 'Outlier/Anomaly'\n",
                "    elif row['Total_Spending'] > 70 and row['Purchase_Frequency'] > 12:\n",
                "        return 'VIP Customers'\n",
                "    elif row['Total_Spending'] > 40 and row['Purchase_Frequency'] > 6:\n",
                "        return 'Regular Customers'\n",
                "    elif row['Total_Spending'] < 40:\n",
                "        return 'Occasional Customers'\n",
                "    else:\n",
                "        return 'Other'\n",
                "\n",
                "df_customers['Segment'] = df_customers.apply(assign_segment_label, axis=1)\n",
                "\n",
                "print(\"\\nðŸ“Š Business Segments:\")\n",
                "print(df_customers['Segment'].value_counts())\n",
                "\n",
                "# Visualize segments\n",
                "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
                "\n",
                "# Segment distribution\n",
                "segment_counts = df_customers['Segment'].value_counts()\n",
                "axes[0].pie(segment_counts.values, labels=segment_counts.index, autopct='%1.1f%%',\n",
                "           startangle=90, colors=sns.color_palette('Set3'))\n",
                "axes[0].set_title('Customer Segment Distribution', fontsize=14, fontweight='bold')\n",
                "\n",
                "# Box plots\n",
                "df_customers.boxplot(column='Total_Spending', by='Segment', ax=axes[1])\n",
                "axes[1].set_title('Spending Distribution by Segment', fontsize=14, fontweight='bold')\n",
                "axes[1].set_xlabel('Segment', fontsize=12)\n",
                "axes[1].set_ylabel('Total Spending ($)', fontsize=12)\n",
                "plt.suptitle('')  # Remove default title\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Business recommendations\n",
                "print(\"\\nðŸ’¼ Business Insights & Recommendations:\")\n",
                "print(\"\\n1. VIP Customers:\")\n",
                "print(\"   - High value, high frequency\")\n",
                "print(\"   - Action: Loyalty programs, exclusive offers, personal account manager\")\n",
                "print(\"\\n2. Regular Customers:\")\n",
                "print(\"   - Medium value, medium frequency\")\n",
                "print(\"   - Action: Upselling campaigns, rewards program\")\n",
                "print(\"\\n3. Occasional Customers:\")\n",
                "print(\"   - Low value, low frequency\")\n",
                "print(\"   - Action: Re-engagement campaigns, special discounts\")\n",
                "print(\"\\n4. Outliers/Anomalies:\")\n",
                "print(\"   - Unusual patterns\")\n",
                "print(\"   - Action: Investigate for fraud, one-time events, or data errors\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 3: Geospatial Application\n",
                "### 8ï¸âƒ£ Crime Hotspot Detection"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate synthetic crime location data\n",
                "np.random.seed(42)\n",
                "\n",
                "# Hotspot 1: Downtown area\n",
                "downtown = np.random.multivariate_normal(\n",
                "    mean=[40.7589, -73.9851],  # NYC coordinates\n",
                "    cov=[[0.0001, 0], [0, 0.0001]],\n",
                "    size=150\n",
                ")\n",
                "\n",
                "# Hotspot 2: Another high-crime area\n",
                "area2 = np.random.multivariate_normal(\n",
                "    mean=[40.7489, -73.9651],\n",
                "    cov=[[0.00008, 0], [0, 0.00008]],\n",
                "    size=100\n",
                ")\n",
                "\n",
                "# Scattered incidents\n",
                "scattered = np.random.uniform(\n",
                "    low=[40.72, -74.01],\n",
                "    high=[40.78, -73.95],\n",
                "    size=(50, 2)\n",
                ")\n",
                "\n",
                "# Combine\n",
                "crime_locations = np.vstack([downtown, area2, scattered])\n",
                "df_crime = pd.DataFrame(crime_locations, columns=['Latitude', 'Longitude'])\n",
                "\n",
                "print(f\"Crime incidents: {len(df_crime)}\")\n",
                "print(f\"\\nðŸ“Š Location Statistics:\")\n",
                "print(df_crime.describe())\n",
                "\n",
                "# Apply DBSCAN (eps in degrees, approximately 100 meters = 0.001 degrees)\n",
                "dbscan_crime = DBSCAN(eps=0.003, min_samples=10)\n",
                "crime_clusters = dbscan_crime.fit_predict(df_crime)\n",
                "\n",
                "df_crime['Hotspot'] = crime_clusters\n",
                "\n",
                "n_hotspots = len(set(crime_clusters)) - (1 if -1 in crime_clusters else 0)\n",
                "n_isolated = list(crime_clusters).count(-1)\n",
                "\n",
                "print(f\"\\nðŸ“Š Crime Hotspot Analysis:\")\n",
                "print(f\"Number of hotspots detected: {n_hotspots}\")\n",
                "print(f\"Isolated incidents: {n_isolated}\")\n",
                "print(f\"\\nHotspot sizes:\")\n",
                "print(df_crime['Hotspot'].value_counts().sort_index())\n",
                "\n",
                "# Visualize\n",
                "plt.figure(figsize=(14, 10))\n",
                "\n",
                "unique_hotspots = set(crime_clusters)\n",
                "colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_hotspots)))\n",
                "\n",
                "for hotspot, color in zip(unique_hotspots, colors):\n",
                "    if hotspot == -1:\n",
                "        mask = crime_clusters == hotspot\n",
                "        plt.scatter(df_crime.loc[mask, 'Longitude'],\n",
                "                   df_crime.loc[mask, 'Latitude'],\n",
                "                   c='gray', marker='o', s=30, alpha=0.5,\n",
                "                   label='Isolated Incidents')\n",
                "    else:\n",
                "        mask = crime_clusters == hotspot\n",
                "        plt.scatter(df_crime.loc[mask, 'Longitude'],\n",
                "                   df_crime.loc[mask, 'Latitude'],\n",
                "                   c=[color], s=100, alpha=0.7, edgecolors='black',\n",
                "                   label=f'Hotspot {hotspot} ({mask.sum()} incidents)')\n",
                "\n",
                "plt.xlabel('Longitude', fontsize=12)\n",
                "plt.ylabel('Latitude', fontsize=12)\n",
                "plt.title('Crime Hotspot Detection using DBSCAN', fontsize=14, fontweight='bold')\n",
                "plt.legend(fontsize=10, loc='best')\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nðŸš” Law Enforcement Recommendations:\")\n",
                "print(\"1. Deploy additional patrols to identified hotspots\")\n",
                "print(\"2. Investigate common factors in hotspot areas\")\n",
                "print(\"3. Implement preventive measures in high-density zones\")\n",
                "print(\"4. Monitor isolated incidents for emerging patterns\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ“Š Key Takeaways\n",
                "\n",
                "### DBSCAN Advantages:\n",
                "âœ… **No need to specify K** - Automatically determines number of clusters  \n",
                "âœ… **Arbitrary shapes** - Finds non-spherical clusters  \n",
                "âœ… **Noise detection** - Identifies outliers automatically  \n",
                "âœ… **Robust** - Not affected by outliers  \n",
                "âœ… **Intuitive parameters** - eps and min_samples have clear meaning  \n",
                "\n",
                "### DBSCAN Disadvantages:\n",
                "âŒ **Sensitive to parameters** - Requires careful tuning of eps and min_samples  \n",
                "âŒ **Varying densities** - Struggles with clusters of different densities  \n",
                "âŒ **High dimensions** - Performance degrades in high-dimensional spaces  \n",
                "âŒ **No cluster centers** - Unlike K-Means, doesn't provide centroids  \n",
                "\n",
                "### Parameter Guidelines:\n",
                "\n",
                "**eps (Îµ):**\n",
                "- Use k-distance graph to find elbow\n",
                "- Start with average distance to k-nearest neighbors\n",
                "- Too small: Many small clusters and noise\n",
                "- Too large: All points in one cluster\n",
                "\n",
                "**min_samples:**\n",
                "- Rule of thumb: min_samples â‰¥ dimensions + 1\n",
                "- Larger values: More robust to noise, but may miss small clusters\n",
                "- Smaller values: More sensitive, may create many small clusters\n",
                "- Typical range: 3-10 for 2D data\n",
                "\n",
                "### When to Use DBSCAN:\n",
                "âœ… Unknown number of clusters  \n",
                "âœ… Non-spherical cluster shapes  \n",
                "âœ… Presence of noise/outliers  \n",
                "âœ… Geospatial data (hotspot detection)  \n",
                "âœ… Anomaly detection  \n",
                "âœ… Varying cluster sizes  \n",
                "\n",
                "### When NOT to Use DBSCAN:\n",
                "âŒ Clusters with very different densities  \n",
                "âŒ High-dimensional data (curse of dimensionality)  \n",
                "âŒ Need cluster centers/centroids  \n",
                "âŒ All points must be assigned to clusters  \n",
                "\n",
                "### DBSCAN vs K-Means:\n",
                "\n",
                "| Feature | DBSCAN | K-Means |\n",
                "|---------|--------|----------|\n",
                "| **Number of clusters** | Automatic | Must specify K |\n",
                "| **Cluster shape** | Arbitrary | Spherical only |\n",
                "| **Outlier handling** | Detects noise | Assigns all points |\n",
                "| **Scalability** | O(n log n) with index | O(nÂ·kÂ·i) |\n",
                "| **Parameters** | eps, min_samples | K, max_iter |\n",
                "| **Deterministic** | Yes | No (random init) |\n",
                "\n",
                "### Real-World Applications:\n",
                "1. **Geospatial Analysis** - Crime hotspots, disease outbreaks\n",
                "2. **Customer Segmentation** - Identify customer groups and outliers\n",
                "3. **Anomaly Detection** - Fraud detection, network intrusion\n",
                "4. **Image Segmentation** - Object detection in images\n",
                "5. **Astronomy** - Galaxy cluster detection\n",
                "6. **Biology** - Gene expression analysis\n",
                "\n",
                "### Best Practices:\n",
                "1. **Scale your data** - DBSCAN is sensitive to feature scales\n",
                "2. **Use k-distance graph** - To find optimal eps\n",
                "3. **Try different parameters** - Grid search over eps and min_samples\n",
                "4. **Validate results** - Use silhouette score, domain knowledge\n",
                "5. **Investigate noise** - Outliers may contain valuable insights\n",
                "6. **Consider HDBSCAN** - For varying density clusters\n",
                "\n",
                "### Next Steps:\n",
                "1. Try HDBSCAN (Hierarchical DBSCAN) for varying densities\n",
                "2. Combine with PCA for high-dimensional data\n",
                "3. Use for anomaly detection in your domain\n",
                "4. Apply to geospatial problems\n",
                "5. Compare with other clustering algorithms"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "ml_env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.19"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}