{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Isolation Forest - Complete Guide\n",
                "## Anomaly Detection and Outlier Identification\n",
                "\n",
                "## üìö Learning Objectives\n",
                "- Understand Isolation Forest algorithm for anomaly detection\n",
                "- Learn how isolation works for outlier detection\n",
                "- Tune contamination parameter\n",
                "- Apply to fraud detection and outlier identification\n",
                "- Compare with other anomaly detection methods\n",
                "- Handle imbalanced anomaly detection problems\n",
                "\n",
                "## üéØ What is Isolation Forest?\n",
                "\n",
                "**Isolation Forest** is an unsupervised machine learning algorithm for anomaly detection that isolates anomalies instead of profiling normal data points.\n",
                "\n",
                "### Key Insight:\n",
                "**Anomalies are few and different** - they are easier to isolate than normal points!\n",
                "\n",
                "### How It Works:\n",
                "1. **Randomly select a feature** and a split value\n",
                "2. **Recursively partition** the data\n",
                "3. **Anomalies require fewer splits** to isolate (shorter path length)\n",
                "4. **Normal points require more splits** (longer path length)\n",
                "\n",
                "### Anomaly Score:\n",
                "- Based on **path length** in isolation trees\n",
                "- **Short path** ‚Üí High anomaly score ‚Üí Likely anomaly\n",
                "- **Long path** ‚Üí Low anomaly score ‚Üí Likely normal\n",
                "\n",
                "### Advantages:\n",
                "‚úÖ **Fast** - Linear time complexity O(n)  \n",
                "‚úÖ **Scalable** - Works with large datasets  \n",
                "‚úÖ **No distance metrics** - Unlike LOF, DBSCAN  \n",
                "‚úÖ **Handles high dimensions** - Better than distance-based methods  \n",
                "‚úÖ **Few parameters** - Only contamination and n_estimators  \n",
                "‚úÖ **Unsupervised** - No labels needed  \n",
                "\n",
                "### When to Use:\n",
                "‚úÖ Fraud detection  \n",
                "‚úÖ Network intrusion detection  \n",
                "‚úÖ Manufacturing defect detection  \n",
                "‚úÖ Medical anomaly detection  \n",
                "‚úÖ Outlier removal before modeling  \n",
                "‚úÖ High-dimensional data  "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.ensemble import IsolationForest\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.datasets import make_classification, make_blobs\n",
                "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
                "from sklearn.model_selection import train_test_split\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Set style\n",
                "sns.set_style('whitegrid')\n",
                "plt.rcParams['figure.figsize'] = (12, 6)\n",
                "\n",
                "print(\"‚úÖ Libraries imported successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 1: Understanding Isolation Forest\n",
                "### 1Ô∏è‚É£ Create Synthetic Data with Anomalies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create normal data\n",
                "np.random.seed(42)\n",
                "n_samples = 300\n",
                "n_outliers = 30\n",
                "\n",
                "# Normal points (clustered)\n",
                "X_normal = np.random.randn(n_samples, 2) * 0.5 + np.array([0, 0])\n",
                "\n",
                "# Outliers (scattered far from normal points)\n",
                "X_outliers = np.random.uniform(low=-4, high=4, size=(n_outliers, 2))\n",
                "\n",
                "# Combine\n",
                "X = np.vstack([X_normal, X_outliers])\n",
                "y_true = np.hstack([np.zeros(n_samples), np.ones(n_outliers)])  # 0=normal, 1=anomaly\n",
                "\n",
                "print(f\"Dataset shape: {X.shape}\")\n",
                "print(f\"Normal points: {n_samples} ({n_samples/(n_samples+n_outliers)*100:.1f}%)\")\n",
                "print(f\"Anomalies: {n_outliers} ({n_outliers/(n_samples+n_outliers)*100:.1f}%)\")\n",
                "\n",
                "# Visualize\n",
                "plt.figure(figsize=(12, 8))\n",
                "plt.scatter(X_normal[:, 0], X_normal[:, 1], \n",
                "           c='blue', s=50, alpha=0.6, edgecolors='black', label='Normal')\n",
                "plt.scatter(X_outliers[:, 0], X_outliers[:, 1],\n",
                "           c='red', s=100, alpha=0.8, edgecolors='black', marker='x', \n",
                "           linewidths=2, label='Anomalies')\n",
                "plt.xlabel('Feature 1', fontsize=12)\n",
                "plt.ylabel('Feature 2', fontsize=12)\n",
                "plt.title('Synthetic Dataset with Anomalies', fontsize=14, fontweight='bold')\n",
                "plt.legend(fontsize=11)\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2Ô∏è‚É£ Apply Isolation Forest"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train Isolation Forest\n",
                "# contamination = expected proportion of outliers\n",
                "contamination_rate = n_outliers / (n_samples + n_outliers)\n",
                "\n",
                "iso_forest = IsolationForest(\n",
                "    contamination=contamination_rate,\n",
                "    random_state=42,\n",
                "    n_estimators=100\n",
                ")\n",
                "\n",
                "# Fit and predict\n",
                "y_pred = iso_forest.fit_predict(X)\n",
                "# Convert to 0/1 (IsolationForest returns 1 for normal, -1 for anomaly)\n",
                "y_pred_binary = (y_pred == -1).astype(int)\n",
                "\n",
                "# Get anomaly scores\n",
                "anomaly_scores = iso_forest.score_samples(X)\n",
                "# More negative = more anomalous\n",
                "\n",
                "print(f\"\\nüìä Isolation Forest Results:\")\n",
                "print(f\"Detected anomalies: {(y_pred == -1).sum()}\")\n",
                "print(f\"True anomalies: {y_true.sum():.0f}\")\n",
                "print(f\"\\nAnomaly score range: [{anomaly_scores.min():.3f}, {anomaly_scores.max():.3f}]\")\n",
                "print(f\"More negative = more anomalous\")\n",
                "\n",
                "# Evaluate\n",
                "print(f\"\\nüìã Classification Report:\")\n",
                "print(classification_report(y_true, y_pred_binary, target_names=['Normal', 'Anomaly']))\n",
                "\n",
                "# Confusion matrix\n",
                "cm = confusion_matrix(y_true, y_pred_binary)\n",
                "plt.figure(figsize=(8, 6))\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
                "           xticklabels=['Normal', 'Anomaly'],\n",
                "           yticklabels=['Normal', 'Anomaly'],\n",
                "           cbar_kws={'label': 'Count'})\n",
                "plt.xlabel('Predicted Label', fontsize=12)\n",
                "plt.ylabel('True Label', fontsize=12)\n",
                "plt.title('Confusion Matrix - Isolation Forest', fontsize=14, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3Ô∏è‚É£ Visualize Anomaly Scores"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create visualization\n",
                "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
                "\n",
                "# Plot 1: Predictions\n",
                "normal_mask = y_pred == 1\n",
                "anomaly_mask = y_pred == -1\n",
                "\n",
                "axes[0].scatter(X[normal_mask, 0], X[normal_mask, 1],\n",
                "               c='blue', s=50, alpha=0.6, edgecolors='black', label='Predicted Normal')\n",
                "axes[0].scatter(X[anomaly_mask, 0], X[anomaly_mask, 1],\n",
                "               c='red', s=100, alpha=0.8, edgecolors='black', marker='x',\n",
                "               linewidths=2, label='Predicted Anomaly')\n",
                "axes[0].set_xlabel('Feature 1', fontsize=12)\n",
                "axes[0].set_ylabel('Feature 2', fontsize=12)\n",
                "axes[0].set_title('Isolation Forest Predictions', fontsize=14, fontweight='bold')\n",
                "axes[0].legend(fontsize=11)\n",
                "axes[0].grid(True, alpha=0.3)\n",
                "\n",
                "# Plot 2: Anomaly scores (heatmap)\n",
                "scatter = axes[1].scatter(X[:, 0], X[:, 1], c=anomaly_scores,\n",
                "                         cmap='RdYlGn', s=80, alpha=0.7, edgecolors='black')\n",
                "axes[1].set_xlabel('Feature 1', fontsize=12)\n",
                "axes[1].set_ylabel('Feature 2', fontsize=12)\n",
                "axes[1].set_title('Anomaly Scores\\n(Red = Anomalous, Green = Normal)', \n",
                "                 fontsize=14, fontweight='bold')\n",
                "plt.colorbar(scatter, ax=axes[1], label='Anomaly Score')\n",
                "axes[1].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Distribution of anomaly scores\n",
                "plt.figure(figsize=(12, 6))\n",
                "plt.hist(anomaly_scores[y_true == 0], bins=30, alpha=0.6, \n",
                "        label='Normal Points', color='blue', edgecolor='black')\n",
                "plt.hist(anomaly_scores[y_true == 1], bins=30, alpha=0.6,\n",
                "        label='True Anomalies', color='red', edgecolor='black')\n",
                "plt.xlabel('Anomaly Score', fontsize=12)\n",
                "plt.ylabel('Frequency', fontsize=12)\n",
                "plt.title('Distribution of Anomaly Scores', fontsize=14, fontweight='bold')\n",
                "plt.legend(fontsize=11)\n",
                "plt.grid(True, alpha=0.3, axis='y')\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nüí° Anomalies have more negative scores!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4Ô∏è‚É£ Parameter Tuning: Contamination"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test different contamination values\n",
                "contamination_values = [0.01, 0.05, 0.1, 0.15, 0.2, 0.25]\n",
                "results = []\n",
                "\n",
                "for contam in contamination_values:\n",
                "    iso = IsolationForest(contamination=contam, random_state=42, n_estimators=100)\n",
                "    y_pred_temp = iso.fit_predict(X)\n",
                "    y_pred_temp_binary = (y_pred_temp == -1).astype(int)\n",
                "    \n",
                "    # Calculate metrics\n",
                "    from sklearn.metrics import precision_score, recall_score, f1_score\n",
                "    \n",
                "    precision = precision_score(y_true, y_pred_temp_binary, zero_division=0)\n",
                "    recall = recall_score(y_true, y_pred_temp_binary, zero_division=0)\n",
                "    f1 = f1_score(y_true, y_pred_temp_binary, zero_division=0)\n",
                "    n_detected = (y_pred_temp == -1).sum()\n",
                "    \n",
                "    results.append({\n",
                "        'Contamination': contam,\n",
                "        'Detected': n_detected,\n",
                "        'Precision': precision,\n",
                "        'Recall': recall,\n",
                "        'F1-Score': f1\n",
                "    })\n",
                "\n",
                "results_df = pd.DataFrame(results)\n",
                "print(\"\\nüìä Impact of Contamination Parameter:\")\n",
                "print(results_df.to_string(index=False))\n",
                "\n",
                "# Visualize\n",
                "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
                "\n",
                "# Metrics vs contamination\n",
                "axes[0].plot(results_df['Contamination'], results_df['Precision'], \n",
                "            marker='o', linewidth=2, label='Precision', color='blue')\n",
                "axes[0].plot(results_df['Contamination'], results_df['Recall'],\n",
                "            marker='s', linewidth=2, label='Recall', color='green')\n",
                "axes[0].plot(results_df['Contamination'], results_df['F1-Score'],\n",
                "            marker='^', linewidth=2, label='F1-Score', color='red')\n",
                "axes[0].axvline(x=contamination_rate, color='black', linestyle='--', \n",
                "               linewidth=2, label=f'True Rate ({contamination_rate:.2f})')\n",
                "axes[0].set_xlabel('Contamination Parameter', fontsize=12)\n",
                "axes[0].set_ylabel('Score', fontsize=12)\n",
                "axes[0].set_title('Performance vs Contamination Parameter', fontsize=14, fontweight='bold')\n",
                "axes[0].legend(fontsize=11)\n",
                "axes[0].grid(True, alpha=0.3)\n",
                "\n",
                "# Number detected\n",
                "axes[1].bar(results_df['Contamination'].astype(str), results_df['Detected'],\n",
                "           color='skyblue', edgecolor='black', alpha=0.7)\n",
                "axes[1].axhline(y=n_outliers, color='red', linestyle='--', linewidth=2,\n",
                "               label=f'True Anomalies ({n_outliers})')\n",
                "axes[1].set_xlabel('Contamination Parameter', fontsize=12)\n",
                "axes[1].set_ylabel('Number of Anomalies Detected', fontsize=12)\n",
                "axes[1].set_title('Detected Anomalies vs Contamination', fontsize=14, fontweight='bold')\n",
                "axes[1].legend(fontsize=11)\n",
                "axes[1].grid(True, alpha=0.3, axis='y')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nüí° Best F1-Score: {results_df.loc[results_df['F1-Score'].idxmax(), 'F1-Score']:.4f}\")\n",
                "print(f\"üí° At contamination: {results_df.loc[results_df['F1-Score'].idxmax(), 'Contamination']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 2: Real-World Application - Credit Card Fraud Detection\n",
                "### 5Ô∏è‚É£ Simulate Credit Card Transactions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create synthetic credit card transaction data\n",
                "np.random.seed(42)\n",
                "n_transactions = 10000\n",
                "n_fraud = 200  # 2% fraud rate (realistic)\n",
                "\n",
                "# Normal transactions\n",
                "normal_amount = np.random.gamma(shape=2, scale=50, size=n_transactions)\n",
                "normal_time = np.random.uniform(0, 24, size=n_transactions)\n",
                "normal_merchant_cat = np.random.choice([1, 2, 3, 4, 5], size=n_transactions, \n",
                "                                      p=[0.3, 0.25, 0.2, 0.15, 0.1])\n",
                "normal_location = np.random.normal(0, 1, size=n_transactions)\n",
                "\n",
                "# Fraudulent transactions (different patterns)\n",
                "fraud_amount = np.random.uniform(500, 2000, size=n_fraud)  # Large amounts\n",
                "fraud_time = np.random.uniform(0, 6, size=n_fraud)  # Late night\n",
                "fraud_merchant_cat = np.random.choice([4, 5], size=n_fraud)  # Unusual categories\n",
                "fraud_location = np.random.normal(5, 2, size=n_fraud)  # Unusual locations\n",
                "\n",
                "# Combine\n",
                "df_transactions = pd.DataFrame({\n",
                "    'Amount': np.concatenate([normal_amount, fraud_amount]),\n",
                "    'Time_of_Day': np.concatenate([normal_time, fraud_time]),\n",
                "    'Merchant_Category': np.concatenate([normal_merchant_cat, fraud_merchant_cat]),\n",
                "    'Location_Score': np.concatenate([normal_location, fraud_location]),\n",
                "    'Is_Fraud': np.concatenate([np.zeros(n_transactions), np.ones(n_fraud)])\n",
                "})\n",
                "\n",
                "# Shuffle\n",
                "df_transactions = df_transactions.sample(frac=1, random_state=42).reset_index(drop=True)\n",
                "\n",
                "print(f\"Total transactions: {len(df_transactions)}\")\n",
                "print(f\"Fraud rate: {df_transactions['Is_Fraud'].mean()*100:.2f}%\")\n",
                "print(f\"\\nüìä Transaction Statistics:\")\n",
                "print(df_transactions.groupby('Is_Fraud').describe().T)\n",
                "\n",
                "# Visualize\n",
                "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
                "\n",
                "# Amount distribution\n",
                "axes[0, 0].hist(df_transactions[df_transactions['Is_Fraud']==0]['Amount'], \n",
                "               bins=50, alpha=0.6, label='Normal', color='blue', edgecolor='black')\n",
                "axes[0, 0].hist(df_transactions[df_transactions['Is_Fraud']==1]['Amount'],\n",
                "               bins=50, alpha=0.6, label='Fraud', color='red', edgecolor='black')\n",
                "axes[0, 0].set_xlabel('Transaction Amount ($)', fontsize=11)\n",
                "axes[0, 0].set_ylabel('Frequency', fontsize=11)\n",
                "axes[0, 0].set_title('Transaction Amount Distribution', fontsize=12, fontweight='bold')\n",
                "axes[0, 0].legend()\n",
                "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
                "\n",
                "# Time distribution\n",
                "axes[0, 1].hist(df_transactions[df_transactions['Is_Fraud']==0]['Time_of_Day'],\n",
                "               bins=24, alpha=0.6, label='Normal', color='blue', edgecolor='black')\n",
                "axes[0, 1].hist(df_transactions[df_transactions['Is_Fraud']==1]['Time_of_Day'],\n",
                "               bins=24, alpha=0.6, label='Fraud', color='red', edgecolor='black')\n",
                "axes[0, 1].set_xlabel('Time of Day (hour)', fontsize=11)\n",
                "axes[0, 1].set_ylabel('Frequency', fontsize=11)\n",
                "axes[0, 1].set_title('Transaction Time Distribution', fontsize=12, fontweight='bold')\n",
                "axes[0, 1].legend()\n",
                "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
                "\n",
                "# Scatter: Amount vs Time\n",
                "axes[1, 0].scatter(df_transactions[df_transactions['Is_Fraud']==0]['Amount'],\n",
                "                  df_transactions[df_transactions['Is_Fraud']==0]['Time_of_Day'],\n",
                "                  alpha=0.3, s=20, label='Normal', color='blue')\n",
                "axes[1, 0].scatter(df_transactions[df_transactions['Is_Fraud']==1]['Amount'],\n",
                "                  df_transactions[df_transactions['Is_Fraud']==1]['Time_of_Day'],\n",
                "                  alpha=0.8, s=50, label='Fraud', color='red', marker='x', linewidths=2)\n",
                "axes[1, 0].set_xlabel('Transaction Amount ($)', fontsize=11)\n",
                "axes[1, 0].set_ylabel('Time of Day (hour)', fontsize=11)\n",
                "axes[1, 0].set_title('Amount vs Time', fontsize=12, fontweight='bold')\n",
                "axes[1, 0].legend()\n",
                "axes[1, 0].grid(True, alpha=0.3)\n",
                "\n",
                "# Fraud rate by merchant category\n",
                "fraud_by_cat = df_transactions.groupby('Merchant_Category')['Is_Fraud'].mean()\n",
                "axes[1, 1].bar(fraud_by_cat.index, fraud_by_cat.values * 100,\n",
                "              color='coral', edgecolor='black', alpha=0.7)\n",
                "axes[1, 1].set_xlabel('Merchant Category', fontsize=11)\n",
                "axes[1, 1].set_ylabel('Fraud Rate (%)', fontsize=11)\n",
                "axes[1, 1].set_title('Fraud Rate by Merchant Category', fontsize=12, fontweight='bold')\n",
                "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6Ô∏è‚É£ Apply Isolation Forest for Fraud Detection"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare features\n",
                "X_fraud = df_transactions[['Amount', 'Time_of_Day', 'Merchant_Category', 'Location_Score']].values\n",
                "y_fraud_true = df_transactions['Is_Fraud'].values\n",
                "\n",
                "# Scale features\n",
                "scaler = StandardScaler()\n",
                "X_fraud_scaled = scaler.fit_transform(X_fraud)\n",
                "\n",
                "# Train Isolation Forest\n",
                "fraud_rate = y_fraud_true.mean()\n",
                "iso_fraud = IsolationForest(\n",
                "    contamination=fraud_rate,\n",
                "    random_state=42,\n",
                "    n_estimators=100\n",
                ")\n",
                "\n",
                "# Predict\n",
                "y_fraud_pred = iso_fraud.fit_predict(X_fraud_scaled)\n",
                "y_fraud_pred_binary = (y_fraud_pred == -1).astype(int)\n",
                "\n",
                "# Get fraud scores\n",
                "fraud_scores = iso_fraud.score_samples(X_fraud_scaled)\n",
                "df_transactions['Fraud_Score'] = fraud_scores\n",
                "df_transactions['Predicted_Fraud'] = y_fraud_pred_binary\n",
                "\n",
                "print(f\"\\nüìä Fraud Detection Results:\")\n",
                "print(f\"Total transactions: {len(df_transactions)}\")\n",
                "print(f\"True fraud cases: {y_fraud_true.sum():.0f}\")\n",
                "print(f\"Detected fraud cases: {y_fraud_pred_binary.sum()}\")\n",
                "print(f\"\\nüìã Classification Report:\")\n",
                "print(classification_report(y_fraud_true, y_fraud_pred_binary, \n",
                "                          target_names=['Legitimate', 'Fraud']))\n",
                "\n",
                "# Confusion matrix\n",
                "cm_fraud = confusion_matrix(y_fraud_true, y_fraud_pred_binary)\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
                "\n",
                "# Confusion matrix\n",
                "sns.heatmap(cm_fraud, annot=True, fmt='d', cmap='Reds', ax=axes[0],\n",
                "           xticklabels=['Legitimate', 'Fraud'],\n",
                "           yticklabels=['Legitimate', 'Fraud'],\n",
                "           cbar_kws={'label': 'Count'})\n",
                "axes[0].set_xlabel('Predicted Label', fontsize=12)\n",
                "axes[0].set_ylabel('True Label', fontsize=12)\n",
                "axes[0].set_title('Confusion Matrix - Fraud Detection', fontsize=14, fontweight='bold')\n",
                "\n",
                "# ROC Curve\n",
                "# Convert scores to probabilities (more negative = higher fraud probability)\n",
                "fraud_proba = -fraud_scores  # Invert so higher = more fraudulent\n",
                "fpr, tpr, thresholds = roc_curve(y_fraud_true, fraud_proba)\n",
                "roc_auc = roc_auc_score(y_fraud_true, fraud_proba)\n",
                "\n",
                "axes[1].plot(fpr, tpr, linewidth=2, label=f'Isolation Forest (AUC = {roc_auc:.3f})', color='red')\n",
                "axes[1].plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier')\n",
                "axes[1].set_xlabel('False Positive Rate', fontsize=12)\n",
                "axes[1].set_ylabel('True Positive Rate', fontsize=12)\n",
                "axes[1].set_title('ROC Curve - Fraud Detection', fontsize=14, fontweight='bold')\n",
                "axes[1].legend(fontsize=11)\n",
                "axes[1].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nüéØ ROC-AUC Score: {roc_auc:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 7Ô∏è‚É£ Analyze Detected Fraud Cases"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Top suspicious transactions\n",
                "top_suspicious = df_transactions.nsmallest(20, 'Fraud_Score')\n",
                "\n",
                "print(\"\\nüö® Top 20 Most Suspicious Transactions:\")\n",
                "print(top_suspicious[['Amount', 'Time_of_Day', 'Merchant_Category', \n",
                "                      'Location_Score', 'Fraud_Score', 'Is_Fraud', 'Predicted_Fraud']])\n",
                "\n",
                "# Analyze false positives and false negatives\n",
                "false_positives = df_transactions[(df_transactions['Is_Fraud'] == 0) & \n",
                "                                 (df_transactions['Predicted_Fraud'] == 1)]\n",
                "false_negatives = df_transactions[(df_transactions['Is_Fraud'] == 1) & \n",
                "                                 (df_transactions['Predicted_Fraud'] == 0)]\n",
                "\n",
                "print(f\"\\n‚ùå False Positives: {len(false_positives)}\")\n",
                "print(f\"‚ùå False Negatives: {len(false_negatives)}\")\n",
                "\n",
                "if len(false_positives) > 0:\n",
                "    print(f\"\\nüìä False Positive Characteristics:\")\n",
                "    print(false_positives[['Amount', 'Time_of_Day', 'Merchant_Category', 'Location_Score']].describe())\n",
                "\n",
                "if len(false_negatives) > 0:\n",
                "    print(f\"\\nüìä False Negative Characteristics:\")\n",
                "    print(false_negatives[['Amount', 'Time_of_Day', 'Merchant_Category', 'Location_Score']].describe())\n",
                "\n",
                "# Visualize fraud score distribution\n",
                "plt.figure(figsize=(14, 6))\n",
                "plt.hist(df_transactions[df_transactions['Is_Fraud']==0]['Fraud_Score'],\n",
                "        bins=50, alpha=0.6, label='Legitimate', color='blue', edgecolor='black')\n",
                "plt.hist(df_transactions[df_transactions['Is_Fraud']==1]['Fraud_Score'],\n",
                "        bins=50, alpha=0.6, label='Fraud', color='red', edgecolor='black')\n",
                "plt.xlabel('Fraud Score (more negative = more suspicious)', fontsize=12)\n",
                "plt.ylabel('Frequency', fontsize=12)\n",
                "plt.title('Distribution of Fraud Scores', fontsize=14, fontweight='bold')\n",
                "plt.legend(fontsize=11)\n",
                "plt.grid(True, alpha=0.3, axis='y')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìä Key Takeaways\n",
                "\n",
                "### Isolation Forest Advantages:\n",
                "‚úÖ **Fast and scalable** - O(n) time complexity  \n",
                "‚úÖ **No distance metrics** - Works well in high dimensions  \n",
                "‚úÖ **Few parameters** - Only contamination and n_estimators  \n",
                "‚úÖ **Unsupervised** - No labels needed for training  \n",
                "‚úÖ **Interpretable scores** - Anomaly scores are intuitive  \n",
                "‚úÖ **Handles outliers well** - Designed for anomaly detection  \n",
                "\n",
                "### Isolation Forest Disadvantages:\n",
                "‚ùå **Requires contamination estimate** - Need to know approximate outlier rate  \n",
                "‚ùå **Not for clustered anomalies** - Works best for scattered outliers  \n",
                "‚ùå **Sensitive to contamination** - Wrong value affects performance  \n",
                "‚ùå **No probability estimates** - Only anomaly scores  \n",
                "\n",
                "### Parameter Guidelines:\n",
                "\n",
                "**contamination:**\n",
                "- Expected proportion of outliers in dataset\n",
                "- Default: 0.1 (10%)\n",
                "- Too low: Misses anomalies\n",
                "- Too high: Too many false positives\n",
                "- **Recommendation:** Use domain knowledge or cross-validation\n",
                "\n",
                "**n_estimators:**\n",
                "- Number of isolation trees\n",
                "- Default: 100\n",
                "- More trees: More stable, slower\n",
                "- **Recommendation:** 100-200 for most cases\n",
                "\n",
                "**max_samples:**\n",
                "- Number of samples to draw for each tree\n",
                "- Default: 'auto' (min(256, n_samples))\n",
                "- Smaller: Faster, less accurate\n",
                "- **Recommendation:** Keep default\n",
                "\n",
                "### When to Use Isolation Forest:\n",
                "‚úÖ Fraud detection (credit cards, insurance)  \n",
                "‚úÖ Network intrusion detection  \n",
                "‚úÖ Manufacturing defect detection  \n",
                "‚úÖ Medical anomaly detection  \n",
                "‚úÖ Outlier removal before modeling  \n",
                "‚úÖ High-dimensional data  \n",
                "‚úÖ Large datasets (scalable)  \n",
                "\n",
                "### When NOT to Use:\n",
                "‚ùå Anomalies form clusters (use DBSCAN)  \n",
                "‚ùå Need probability estimates (use One-Class SVM)  \n",
                "‚ùå Very small datasets (< 100 samples)  \n",
                "‚ùå All data points are important (no outliers)  \n",
                "\n",
                "### Real-World Applications:\n",
                "1. **Credit Card Fraud** - Detect unusual transactions\n",
                "2. **Network Security** - Identify intrusions\n",
                "3. **Manufacturing** - Find defective products\n",
                "4. **Healthcare** - Detect unusual patient vitals\n",
                "5. **E-commerce** - Identify fake reviews\n",
                "6. **IoT** - Detect sensor anomalies\n",
                "\n",
                "### Best Practices:\n",
                "\n",
                "1. **Scale your data** - Isolation Forest benefits from scaling\n",
                "   ```python\n",
                "   scaler = StandardScaler()\n",
                "   X_scaled = scaler.fit_transform(X)\n",
                "   ```\n",
                "\n",
                "2. **Tune contamination** - Use domain knowledge or validation\n",
                "   ```python\n",
                "   # If you know ~2% are anomalies\n",
                "   iso = IsolationForest(contamination=0.02)\n",
                "   ```\n",
                "\n",
                "3. **Use anomaly scores** - Don't just use binary predictions\n",
                "   ```python\n",
                "   scores = iso.score_samples(X)\n",
                "   # More negative = more anomalous\n",
                "   ```\n",
                "\n",
                "4. **Validate results** - Check detected anomalies make sense\n",
                "\n",
                "5. **Combine with other methods** - Ensemble for better results\n",
                "\n",
                "### Comparison with Other Methods:\n",
                "\n",
                "| Method | Speed | High-D | Scalability | Interpretability |\n",
                "|--------|-------|--------|-------------|------------------|\n",
                "| **Isolation Forest** | Fast | Good | Excellent | Medium |\n",
                "| **One-Class SVM** | Slow | Poor | Poor | Low |\n",
                "| **LOF** | Slow | Poor | Poor | Medium |\n",
                "| **DBSCAN** | Medium | Poor | Medium | High |\n",
                "| **Autoencoder** | Medium | Excellent | Good | Low |\n",
                "\n",
                "### Isolation Forest Workflow:\n",
                "```python\n",
                "# 1. Prepare data\n",
                "scaler = StandardScaler()\n",
                "X_scaled = scaler.fit_transform(X)\n",
                "\n",
                "# 2. Estimate contamination\n",
                "contamination = 0.05  # 5% outliers expected\n",
                "\n",
                "# 3. Train model\n",
                "iso = IsolationForest(\n",
                "    contamination=contamination,\n",
                "    n_estimators=100,\n",
                "    random_state=42\n",
                ")\n",
                "predictions = iso.fit_predict(X_scaled)\n",
                "\n",
                "# 4. Get anomaly scores\n",
                "scores = iso.score_samples(X_scaled)\n",
                "\n",
                "# 5. Analyze results\n",
                "anomalies = X[predictions == -1]\n",
                "```\n",
                "\n",
                "### Performance Metrics:\n",
                "- **Precision**: Of detected anomalies, how many are true?\n",
                "- **Recall**: Of true anomalies, how many detected?\n",
                "- **F1-Score**: Balance between precision and recall\n",
                "- **ROC-AUC**: Overall discrimination ability\n",
                "\n",
                "### Next Steps:\n",
                "1. Apply to your own anomaly detection problem\n",
                "2. Try ensemble with other anomaly detectors\n",
                "3. Experiment with different contamination values\n",
                "4. Compare with autoencoders for complex data\n",
                "5. Deploy for real-time fraud detection"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "ml_env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.19"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}