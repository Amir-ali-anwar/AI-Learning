{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Complete ML Pipeline: Used Cars Price Prediction\n",
                "\n",
                "This notebook demonstrates the complete end-to-end machine learning pipeline:\n",
                "\n",
                "1. ‚úÖ **Data Preprocessing** - Clean and prepare data\n",
                "2. ‚úÖ **Feature Engineering** - Create new features\n",
                "3. ‚úÖ **Feature Selection** - Select most important features\n",
                "4. ‚úÖ **Model Training** - Train multiple models\n",
                "5. ‚úÖ **Model Evaluation** - Evaluate and compare models\n",
                "6. ‚úÖ **Hyperparameter Tuning** - Optimize best models\n",
                "7. ‚úÖ **Final Model Selection** - Choose and save best model\n",
                "\n",
                "## üéØ Goal\n",
                "Build a production-ready model to predict used car prices with high accuracy."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup and Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import libraries\n",
                "import sys\n",
                "sys.path.append('../src')\n",
                "\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import logging\n",
                "from pathlib import Path\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Import custom modules\n",
                "from config import *\n",
                "from data_validator import DataValidator\n",
                "from preprocessing_pipeline import DataPreprocessor, split_data\n",
                "from feature_engineering import FeatureEngineer\n",
                "from feature_selection import FeatureSelector\n",
                "from model_training import ModelTrainer\n",
                "from model_evaluation import ModelEvaluator\n",
                "from hyperparameter_tuning import HyperparameterTuner\n",
                "\n",
                "# Set up plotting\n",
                "%matplotlib inline\n",
                "sns.set_style('whitegrid')\n",
                "plt.rcParams['figure.figsize'] = (12, 6)\n",
                "\n",
                "print(\"‚úÖ All modules imported successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Configure Logging"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create logs directory\n",
                "log_dir = Path('../logs')\n",
                "log_dir.mkdir(exist_ok=True)\n",
                "\n",
                "# Configure logging\n",
                "logging.basicConfig(\n",
                "    level=logging.INFO,\n",
                "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
                "    handlers=[\n",
                "        logging.FileHandler(log_dir / 'complete_pipeline.log'),\n",
                "        logging.StreamHandler()\n",
                "    ]\n",
                ")\n",
                "\n",
                "logger = logging.getLogger(__name__)\n",
                "logger.info(\"=\"*80)\n",
                "logger.info(\"COMPLETE ML PIPELINE - USED CARS PRICE PREDICTION\")\n",
                "logger.info(\"=\"*80)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Load and Preprocess Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load raw data\n",
                "logger.info(\"Loading raw data...\")\n",
                "df_raw = pd.read_csv(RAW_DATA_PATH)\n",
                "print(f\"Raw data shape: {df_raw.shape}\")\n",
                "\n",
                "# Create configuration\n",
                "config = {\n",
                "    'COLUMNS_TO_DROP': COLUMNS_TO_DROP,\n",
                "    'PRICE_FILTER': PRICE_FILTER,\n",
                "    'YEAR_FILTER': YEAR_FILTER,\n",
                "    'ODOMETER_FILTER': ODOMETER_FILTER,\n",
                "    'IMPUTATION_STRATEGY': IMPUTATION_STRATEGY,\n",
                "    'CONSTANT_VALUES': CONSTANT_VALUES,\n",
                "    'TARGET_COLUMN': TARGET_COLUMN,\n",
                "    'OUTLIER_CONFIG': OUTLIER_CONFIG,\n",
                "    'NUMERICAL_COLUMNS': NUMERICAL_COLUMNS,\n",
                "    'ENCODING_CONFIG': ENCODING_CONFIG,\n",
                "    'SCALING_CONFIG': SCALING_CONFIG,\n",
                "    'TRAIN_TEST_SPLIT': TRAIN_TEST_SPLIT\n",
                "}\n",
                "\n",
                "# Preprocess data\n",
                "preprocessor = DataPreprocessor(config)\n",
                "df_processed = preprocessor.fit_transform(df_raw.copy())\n",
                "print(f\"\\nProcessed data shape: {df_processed.shape}\")\n",
                "print(f\"Rows retained: {len(df_processed) / len(df_raw) * 100:.1f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Feature Engineering"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize feature engineer\n",
                "fe_config = {\n",
                "    'create_polynomial': False,  # Set to True for polynomial features\n",
                "    'create_statistical': False  # Set to True for statistical features (slower)\n",
                "}\n",
                "\n",
                "feature_engineer = FeatureEngineer(fe_config)\n",
                "\n",
                "# Create engineered features\n",
                "df_engineered = feature_engineer.fit_transform(df_processed.copy())\n",
                "print(f\"\\nEngineered data shape: {df_engineered.shape}\")\n",
                "print(f\"New features created: {len(feature_engineer.get_feature_names())}\")\n",
                "print(f\"\\nNew features: {feature_engineer.get_feature_names()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Split Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Split into train and test sets\n",
                "X_train, X_test, y_train, y_test = split_data(df_engineered, config)\n",
                "\n",
                "print(f\"\\nTrain set: {X_train.shape}\")\n",
                "print(f\"Test set: {X_test.shape}\")\n",
                "print(f\"\\nTarget statistics:\")\n",
                "print(f\"  Train - Mean: ${y_train.mean():,.2f}, Median: ${y_train.median():,.2f}\")\n",
                "print(f\"  Test - Mean: ${y_test.mean():,.2f}, Median: ${y_test.median():,.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Feature Selection"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize feature selector\n",
                "fs_config = {\n",
                "    'variance_threshold': 0.01,\n",
                "    'correlation_threshold': 0.95\n",
                "}\n",
                "\n",
                "feature_selector = FeatureSelector(fs_config)\n",
                "\n",
                "# Select features using ensemble method\n",
                "n_features_to_select = min(50, X_train.shape[1])  # Select top 50 or all if less\n",
                "X_train_selected = feature_selector.fit_transform(\n",
                "    X_train, y_train, \n",
                "    method='ensemble',  # Use ensemble of multiple methods\n",
                "    n_features=n_features_to_select\n",
                ")\n",
                "\n",
                "X_test_selected = feature_selector.transform(X_test)\n",
                "\n",
                "print(f\"\\nSelected features: {X_train_selected.shape[1]}\")\n",
                "print(f\"\\nTop 10 features:\")\n",
                "print(feature_selector.get_feature_scores().head(10)[['feature', 'avg_score']])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot feature importance\n",
                "feature_selector.plot_feature_importance(\n",
                "    feature_selector.get_feature_scores(),\n",
                "    top_n=20,\n",
                "    save_path='../data/processed/feature_importance.png'\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Train Baseline Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize model trainer\n",
                "trainer = ModelTrainer()\n",
                "\n",
                "# Train baseline models (subset for speed)\n",
                "baseline_models = [\n",
                "    'linear_regression',\n",
                "    'ridge',\n",
                "    'random_forest',\n",
                "    'gradient_boosting',\n",
                "    'xgboost',\n",
                "    'lightgbm'\n",
                "]\n",
                "\n",
                "trained_models = trainer.train_all_models(\n",
                "    X_train_selected, y_train,\n",
                "    model_subset=baseline_models,\n",
                "    use_optimized=False\n",
                ")\n",
                "\n",
                "print(f\"\\n‚úÖ Trained {len(trained_models)} baseline models\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Evaluate Baseline Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize evaluator\n",
                "evaluator = ModelEvaluator()\n",
                "\n",
                "# Get predictions from all models\n",
                "predictions_train = trainer.predict_all(X_train_selected)\n",
                "predictions_test = trainer.predict_all(X_test_selected)\n",
                "\n",
                "# Evaluate on train set\n",
                "train_results = evaluator.evaluate_all_models(predictions_train, y_train, dataset='train')\n",
                "\n",
                "# Evaluate on test set\n",
                "test_results = evaluator.evaluate_all_models(predictions_test, y_test, dataset='test')\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"BASELINE MODEL RESULTS (TEST SET)\")\n",
                "print(\"=\"*80)\n",
                "print(test_results[['model', 'r2', 'rmse', 'mae', 'mape']].to_string(index=False))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot model comparison\n",
                "evaluator.plot_model_comparison(\n",
                "    test_results,\n",
                "    metric='r2',\n",
                "    save_path='../data/processed/baseline_model_comparison.png'\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get best baseline model\n",
                "best_baseline_name, best_baseline_metrics = evaluator.get_best_model(metric='r2', dataset='test')\n",
                "print(f\"\\nüèÜ Best baseline model: {best_baseline_name}\")\n",
                "print(f\"   R¬≤ Score: {best_baseline_metrics['r2']:.4f}\")\n",
                "print(f\"   RMSE: ${best_baseline_metrics['rmse']:,.2f}\")\n",
                "print(f\"   MAE: ${best_baseline_metrics['mae']:,.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Visualize Best Baseline Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get predictions from best model\n",
                "best_predictions = predictions_test[best_baseline_name]\n",
                "\n",
                "# Plot predictions vs actual\n",
                "evaluator.plot_predictions_vs_actual(\n",
                "    y_test, best_predictions,\n",
                "    model_name=best_baseline_name,\n",
                "    save_path=f'../data/processed/{best_baseline_name}_predictions.png'\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot residuals\n",
                "evaluator.plot_residuals(\n",
                "    y_test, best_predictions,\n",
                "    model_name=best_baseline_name,\n",
                "    save_path=f'../data/processed/{best_baseline_name}_residuals.png'\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Hyperparameter Tuning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize tuner\n",
                "tuner = HyperparameterTuner()\n",
                "\n",
                "# Select top models to tune\n",
                "models_to_tune = ['random_forest', 'xgboost', 'lightgbm']\n",
                "\n",
                "# Tune models using random search (faster than grid search)\n",
                "print(\"\\n‚öôÔ∏è Starting hyperparameter tuning...\")\n",
                "print(\"This may take several minutes...\\n\")\n",
                "\n",
                "tuned_results = tuner.tune_all_models(\n",
                "    X_train_selected, y_train,\n",
                "    models=models_to_tune,\n",
                "    method='random',  # Use 'grid' for exhaustive search\n",
                "    n_iter=20,  # Number of random combinations to try\n",
                "    cv=3,  # 3-fold cross-validation for speed\n",
                "    scoring='r2'\n",
                ")\n",
                "\n",
                "print(f\"\\n‚úÖ Tuned {len(tuned_results)} models\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display best parameters\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"BEST HYPERPARAMETERS\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "for model_name in models_to_tune:\n",
                "    params = tuner.get_best_params(model_name)\n",
                "    if params:\n",
                "        print(f\"\\n{model_name.upper()}:\")\n",
                "        for param, value in params.items():\n",
                "            print(f\"  {param}: {value}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Evaluate Tuned Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get predictions from tuned models\n",
                "tuned_predictions_test = {}\n",
                "\n",
                "for model_name, (model, params) in tuned_results.items():\n",
                "    tuned_predictions_test[f\"{model_name}_tuned\"] = model.predict(X_test_selected)\n",
                "\n",
                "# Evaluate tuned models\n",
                "tuned_test_results = evaluator.evaluate_all_models(\n",
                "    tuned_predictions_test, y_test, dataset='test'\n",
                ")\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"TUNED MODEL RESULTS (TEST SET)\")\n",
                "print(\"=\"*80)\n",
                "print(tuned_test_results[['model', 'r2', 'rmse', 'mae', 'mape']].to_string(index=False))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare baseline vs tuned\n",
                "comparison_df = pd.concat([test_results, tuned_test_results])\n",
                "comparison_df = comparison_df.sort_values('r2', ascending=False)\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"ALL MODELS COMPARISON (BASELINE + TUNED)\")\n",
                "print(\"=\"*80)\n",
                "print(comparison_df[['model', 'r2', 'rmse', 'mae']].to_string(index=False))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 12. Select Final Best Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get overall best model\n",
                "best_model_name = comparison_df.iloc[0]['model']\n",
                "best_r2 = comparison_df.iloc[0]['r2']\n",
                "best_rmse = comparison_df.iloc[0]['rmse']\n",
                "best_mae = comparison_df.iloc[0]['mae']\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"üèÜ FINAL BEST MODEL\")\n",
                "print(\"=\"*80)\n",
                "print(f\"\\nModel: {best_model_name}\")\n",
                "print(f\"\\nPerformance Metrics:\")\n",
                "print(f\"  R¬≤ Score: {best_r2:.4f}\")\n",
                "print(f\"  RMSE: ${best_rmse:,.2f}\")\n",
                "print(f\"  MAE: ${best_mae:,.2f}\")\n",
                "print(f\"\\nInterpretation:\")\n",
                "print(f\"  - The model explains {best_r2*100:.2f}% of the variance in car prices\")\n",
                "print(f\"  - Average prediction error: ${best_mae:,.2f}\")\n",
                "print(f\"  - Root mean squared error: ${best_rmse:,.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 13. Save Everything"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create models directory\n",
                "models_dir = Path('../models')\n",
                "models_dir.mkdir(exist_ok=True)\n",
                "\n",
                "# Save preprocessor\n",
                "import pickle\n",
                "with open(models_dir / 'preprocessor.pkl', 'wb') as f:\n",
                "    pickle.dump(preprocessor, f)\n",
                "print(\"‚úÖ Saved: preprocessor.pkl\")\n",
                "\n",
                "# Save feature engineer\n",
                "with open(models_dir / 'feature_engineer.pkl', 'wb') as f:\n",
                "    pickle.dump(feature_engineer, f)\n",
                "print(\"‚úÖ Saved: feature_engineer.pkl\")\n",
                "\n",
                "# Save feature selector\n",
                "with open(models_dir / 'feature_selector.pkl', 'wb') as f:\n",
                "    pickle.dump(feature_selector, f)\n",
                "print(\"‚úÖ Saved: feature_selector.pkl\")\n",
                "\n",
                "# Save all trained models\n",
                "trainer.save_all_models(models_dir)\n",
                "\n",
                "# Save tuned models\n",
                "for model_name, (model, params) in tuned_results.items():\n",
                "    with open(models_dir / f\"{model_name}_tuned.pkl\", 'wb') as f:\n",
                "        pickle.dump(model, f)\n",
                "    print(f\"‚úÖ Saved: {model_name}_tuned.pkl\")\n",
                "\n",
                "# Save best parameters\n",
                "tuner.save_best_params(models_dir / 'best_params.pkl')\n",
                "\n",
                "# Save evaluation results\n",
                "comparison_df.to_csv(models_dir / 'model_comparison.csv', index=False)\n",
                "print(\"‚úÖ Saved: model_comparison.csv\")\n",
                "\n",
                "# Save selected features\n",
                "selected_features = feature_selector.get_selected_features()\n",
                "pd.DataFrame({'feature': selected_features}).to_csv(\n",
                "    models_dir / 'selected_features.csv', index=False\n",
                ")\n",
                "print(\"‚úÖ Saved: selected_features.csv\")\n",
                "\n",
                "print(f\"\\n‚úÖ All artifacts saved to: {models_dir.absolute()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 14. Generate Final Report"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate evaluation report\n",
                "report = evaluator.generate_evaluation_report(\n",
                "    comparison_df,\n",
                "    save_path='../models/evaluation_report.txt'\n",
                ")\n",
                "\n",
                "print(report)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 15. Example: Using the Model for Predictions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example of how to use the saved model for new predictions\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"EXAMPLE: MAKING PREDICTIONS ON NEW DATA\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "# Take a sample from test set\n",
                "sample_data = X_test.head(5)\n",
                "sample_actual = y_test.head(5)\n",
                "\n",
                "# Apply feature engineering\n",
                "sample_engineered = feature_engineer.transform(sample_data)\n",
                "\n",
                "# Apply feature selection\n",
                "sample_selected = feature_selector.transform(sample_engineered)\n",
                "\n",
                "# Make predictions with best model\n",
                "# (In production, load the saved model)\n",
                "if '_tuned' in best_model_name:\n",
                "    base_name = best_model_name.replace('_tuned', '')\n",
                "    best_model = tuned_results[base_name][0]\n",
                "else:\n",
                "    best_model = trainer.get_model(best_model_name)\n",
                "\n",
                "sample_predictions = best_model.predict(sample_selected)\n",
                "\n",
                "# Display results\n",
                "results_df = pd.DataFrame({\n",
                "    'Actual Price': sample_actual.values,\n",
                "    'Predicted Price': sample_predictions,\n",
                "    'Error': sample_predictions - sample_actual.values,\n",
                "    'Error %': ((sample_predictions - sample_actual.values) / sample_actual.values * 100)\n",
                "})\n",
                "\n",
                "print(\"\\nSample Predictions:\")\n",
                "print(results_df.to_string(index=False))\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"‚úÖ PIPELINE COMPLETED SUCCESSFULLY!\")\n",
                "print(\"=\"*80)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "### What We Accomplished:\n",
                "\n",
                "1. ‚úÖ **Preprocessed** 400K+ rows of raw data\n",
                "2. ‚úÖ **Engineered** new features to improve predictions\n",
                "3. ‚úÖ **Selected** the most important features\n",
                "4. ‚úÖ **Trained** multiple baseline models\n",
                "5. ‚úÖ **Evaluated** and compared all models\n",
                "6. ‚úÖ **Tuned** hyperparameters for best models\n",
                "7. ‚úÖ **Selected** the best performing model\n",
                "8. ‚úÖ **Saved** all artifacts for production use\n",
                "\n",
                "### Next Steps:\n",
                "\n",
                "1. **Deploy the model** to a production environment\n",
                "2. **Monitor performance** on new data\n",
                "3. **Retrain periodically** with new data\n",
                "4. **A/B test** different models\n",
                "5. **Collect feedback** and iterate\n",
                "\n",
                "### How to Use in Production:\n",
                "\n",
                "```python\n",
                "import pickle\n",
                "\n",
                "# Load all components\n",
                "with open('../models/preprocessor.pkl', 'rb') as f:\n",
                "    preprocessor = pickle.load(f)\n",
                "\n",
                "with open('../models/feature_engineer.pkl', 'rb') as f:\n",
                "    feature_engineer = pickle.load(f)\n",
                "\n",
                "with open('../models/feature_selector.pkl', 'rb') as f:\n",
                "    feature_selector = pickle.load(f)\n",
                "\n",
                "with open('../models/best_model.pkl', 'rb') as f:\n",
                "    model = pickle.load(f)\n",
                "\n",
                "# Process new data\n",
                "df_new_processed = preprocessor.transform(df_new)\n",
                "df_new_engineered = feature_engineer.transform(df_new_processed)\n",
                "df_new_selected = feature_selector.transform(df_new_engineered)\n",
                "\n",
                "# Make predictions\n",
                "predictions = model.predict(df_new_selected)\n",
                "```"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "ml_env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.19"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}